# FEDAVG CONVERGES TO ZERO TRAINING LOSS LINEARLY FOR OVERPARAMETERIZED MULTI-LAYER NEURAL NETWORKS

**Anonymous authors**
Paper under double-blind review

### ABSTRACT

Federated Learning (FL) is a distributed learning paradigm that allows multiple
clients to learn a joint model by utilizing privately held data at each client. Significant research efforts have been devoted to develop advanced algorithms that
deal with the situation where the data at individual clients have heterogeneous
distributions. In this work, we show that data heterogeneity can be dealt from a
different perspective. That is, by utilizing a certain overparameterized multi-layer
neural network at each client, even the vanilla FedAvg (a.k.a. the Local SGD)
algorithm can accurately optimize the training problem: When each client has a
neural network with one wide layer of size N (where N is the number of total
training samples), followed by layers of smaller widths, FedAvg converges linearly to a solution that achieves (almost) zero training loss, without requiring any
assumptions on the clients’ data distributions. To our knowledge, this is the first
work that demonstrates such resilience to data heterogeneity for FedAvg when
trained on multi-layer neural networks. Our experiments also confirm that, neural networks of large size can achieve better and more stable performance for FL
problems.

### 1 INTRODUCTION

In Federated Learning (FL), multiple clients collaborate with the help of a server to learn a joint
model McMahan et al. (2017). The privacy guarantees of FL has made it a popular distributed
learning paradigm, as each client holds a private data set and aims to learn a global model without
leaking its data to other nodes or the server. The performance of FL algorithms is known to degrade
when training data at individual nodes originates from different distributions, referred to as the
_heterogeneous data setting Yu et al. (2019a); Woodworth et al. (2020a). In the past few years, a_
substantial research effort has been devoted towards developing a large number of algorithms that
can better deal with data heterogeneity, Karimireddy et al. (2020b); Zhang et al. (2021); Li et al.
(2018); Acar et al. (2020); Khanduri et al. (2021). However, in practice it has been observed by a
number of recent works, that in spite of the data heterogeneity, the simple vanilla FedAvg algorithm
(a.k.a. the Local SGD) still offers competitive performance in comparison to the state-of-the-art.
For example, see Table 2 in Karimireddy et al. (2020a), Table 1 in Reddi et al. (2020), and Table 2
in Yang et al. (2021) for performance comparison of FedAvg on popular FL tasks.

Motivated by these observations, we ask: Is it possible to handle the the data heterogeneity issue
from a different perspective, without modifying the vanilla FedAvg algorithm? To answer this question, in this work we show that FedAvg can indeed perform very well regardless of the heterogeneity
conditions, if the models to be learned are nice enough. Specifically, FedAvg finds solutions that
achieve almost zero training loss (or almost global optimal solution) very quickly (i.e., linearly),
when the FL model to be trained is certain overparameterized multi-layer neural network. To the
best of our knowledge, this is the first result that shows (linear) convergence of FedAvg in the overparameterized regime for training multilayer neural networks. The major contributions of our work
are listed below.

- Under certain assumptions on the neural network architecture, we prove some key properties of
the clients’ (stochastic) gradients during the training phase (Lemmas 1 and 2). These results allow
us to establish convergence of FedAvg for training overparameterized neural networks without
imposing restrictive heterogeneity assumptions on the gradients of the local loss functions.


-----

- We design a special initialization strategy for training the network using FedAvg. The initialization
is designed such that the singular values of the model parameters and the outputs of the first
layer of local and aggregated model parameters stay positive definite during the training. This
property combined with overparameterization enables FedAvg to converge linearly to a (near)
optimal solution.

- We conduct experiments on CIFAR-10 and MNIST datasets in both i.i.d. and heterogeneous data
settings to compare the performance of FedAvg on various network architectures of different sizes.
To our knowledge, this is the first work that shows the linear convergence of FedAvg (both SGD and
GD versions) to the optimal solution when training a overparameterized multi-layer neural networks.

**Related Work: Federated Learning (FL). FL algorithms were first proposed in McMahan et al.**
(2017), where within each communication round the clients utilize their private data to update the
model parameters using multiple SGD steps. Earlier works analyzed the performance of FedAvg
for the case of homogeneous data setting Zhou and Cong (2018); Stich (2018); Lin et al. (2020);
Woodworth et al. (2020b); Wang and Joshi (2021), i.e., when the local data at each client follows
the same underlying distribution. Motivated by practical applications, recent works have analyzed
FedAvg for heterogeneous client data distributions Yu et al. (2019b;a); Haddadpour and Mahdavi
(2019); Woodworth et al. (2020a) and it was observed that the performance of FedAvg degrades
as the data heterogeneity increases. To address the data heterogeneity issue among clients, many
works have focused on developing sophisticated algorithms Karimireddy et al. (2020b); Zhang et al.
(2021); Acar et al. (2020); Li et al. (2018); Khanduri et al. (2021); Karimireddy et al. (2020a); Das
et al. (2020).

**Overparameterized Neural Networks. The surprising performance of overparameterized neural**
networks[1] has raised significant research interest in the ML community to analyze the phenomenon
of overparameterization Belkin et al. (2019). Consequently, many works have analyzed the performance of centralized (stochastic) gradient descent (S)GD on overparameterized neural network
architectures under different settings Jacot et al. (2018); Li and Liang (2018); Arora et al. (2019);
Du et al. (2018; 2019); Allen-Zhu et al. (2019); Zou and Gu (2019); Nguyen and Mondelli (2020);
Nguyen (2021).

However, there are only a handful of works that have attempted to analyze the performance of
overparameterized neural networks in the distributed setting Li et al. (2021); Huang et al. (2021);
Deng and Mahdavi (2021). The works most closely related to our work are Huang et al. (2021) and
Deng and Mahdavi (2021). Huang et al. (2021) analyzed the performance of FedAvg on a single
hidden-layer neural network for the case when each client utilizes GD for the local updates. The
authors established linear convergence of FedAvg using the NTK parameterization and showed that
it suffices to design the neural network of width Ω(N [4]) to achieve this performance (where N is
the number of training samples). Similarly, Deng and Mahdavi (2021) analyzed the performance of
FedAvg on a ReLU neural network but when each client utilizes SGD (or GD) for the local updates.
The authors proved convergence of FedAvg under the standard parameterization while requiring
the very large network width of Ω(N [18]). Note that since individual clients can be devices with
limited computational capabilities, in realistic settings it is undesirable to have networks of such
large widths. In contrast to both these works, we focus on the more practical setting of a multi-layer
neural network Nguyen and Mondelli (2020) and establish linear convergence of FedAvg even for
the case when each client utilizes SGD for the local updates. Importantly, we show that with proper
initialization, it only requires a network of width N at each client, which is much smaller compared
to the unrealistic requirements of Huang et al. (2021); Deng and Mahdavi (2021).

### 2 PROBLEM SETUP

In this section, we define the multi-layer neural network and formalize the problem we aim to solve.
We consider a distributed system of K clients with each client having access to a privately held data
set. We assume that each client k ∈{1, . . ., K } has Nk training samples denoted as {(Xk, Yk)},
with Xk R[N][k][×][d][in] and Yk R[N][k][×][d][out] . Note that each row of Xk and Yk represents the feature
_∈_ _∈_
vector and its corresponding label, and din and dout denote the feature (input) and label (output)
dimensions, respectively. We further denote N = [�]k[K]=1 _[N][k][ as the total samples across all clients.]_

1A model is generally referred to as overparameterized if the number of (trainable) parameters are more
than the number of training samples N .


-----

Suppose each client trains a fully-connected neural network with L layers, and with activation
function σ : R → R. We denote the vectorized parameters at each node k ∈{1, . . ., K} as
_θk = [vec (W1,k), . . ., vec (WL,k)] ∈_ R[D], where Wl,k ∈ R[n][l][−][1][×][n][l] represents the weight matrix of
each layer l ∈{1, . . ., L} and nl represents the width of each layer. Note that each layer inputs a
(feature) vector of dimension nl−1 and outputs a (feature) vector of dimension nl. For simplicity,
define n0 = din and nL = dout as the input and the output dimensions of the neural network. We
define Fl,k as the local output of each layer l at client k, then using the above notations, we have


_Fl,k =_


Xk _l = 0_


_σ (Fl−1,kWl,k)_ _l ∈{1, 2, . . ., L}_ _._ (1)

FL−1,kWL,k _l = L_


We further define the vectorized output of each layer and the labels at each client as fl,k =
vec(Fl,k) ∈ R[N][k][n][l] and yk = vec(Yk) ∈ R[N][k][n][L] .

Similar to the above setup, we also define the notations to describe a single network, with the full
data (X, Y ) with X ∈ R[N] _[×][d][in]_ and Y ∈ R[N] _[×][d][out]_ as input. This “centralized" network will be useful
later to perform the analysis. Then given parameter θ = [vec (W1), . . ., vec (WL)], the output at
each layer of the network is defined as


_X_ _l = 0_
_σ (Fl−1Wl)_ _l ∈{1, 2, . . ., L}_ _._ (2)
_FL−1WL_ _l = L_


_Fl =_








Next, we define the local and global loss functions. First, each client k 1, . . ., K has a local
_∈{_ _}_
loss function given by: Φk(θ) := 2N1 _k_ 2[, where][ ∥· ∥][2] [denotes the standard][ ℓ][2][-norm.]

_[∥][f][L,k][(][θ][)][ −]_ _[y][k][∥][2]_
Then the global loss function is the sum of weighted local loss functions, given by:


Φ(θ) :=


_K_
�

_k=1_


_Nk_ 1 _F_ _[.]_ (3)

_N_ [Φ][k][(][θ][) =] 2N

_[∥][F][L][(][θ][)][ −]_ _[y][∥][2]_


Additionally, define the gradient of (3) as g := [vec(∇W1 Φ(θ)), . . ., vec(∇WL Φ(θ))], which is the
stacked gradient of the loss w.r.t. the 1[st] to L[th] layer’s parameters; define the gradient of the losses
at each client k ∈ [K] as: gk := [g1,k, . . ., gL,K] with gl,k := vec(∇Wl,k Φ(θ)) for all l ∈ [L].

Next, we define the optimality criteria to solve (3) using an overparameterized neural network.
**Definition 1 (ϵ-optimal solution). Consider an overparameterized problem minθ Φ(θ), where there**
_exist θ[∗]_ _such that Φ(θ[∗]) = 0. A solution θ is called an ϵ-optimal solution if it satisfies Φ(θ)_ _ϵ._
_≤_
_Moreover, if θ is a random variable, then we use E[Φ(θ)]_ _ϵ to denote an ϵ-optimal solution, where_
_≤_
_the expectation is taken w.r.t. the randomness of x._

### 3 THE FEDAVG ALGORITHM

A classical algorithm to solve problem (3) is the FedAvg McMahan et al. (2017). In FedAvg, each
client performs multiple local updates before sharing their updated parameters with the server. We
refer the algorithm as FedAvg-SGD (resp. FedAvg-GD) if the clients employ SGD (resp. GD) for
the local updates.

The detailed steps to implement FedAvg-SGD are listed in Algorithm 1. We execute the algorithm
for a total of T communication rounds, within each communication round every client performs
_r local updates. In each communication round t the server aggregates the local parameters and_
constructs _θ[¯][rt]_ from each client’s local parameters θk[rt][+][r] and shares it with the clients. The clients
use the aggregated parameter, _θ[¯]k[rt][+][r], as the initial parameter value for computing the next round of_
local updates. For each v 0, 1, . . ., r 1, to update the local parameters the clients compute
_∈{_ _−_ _}_
the (unbiased) stochastic gradient using m-samples drawn form their private data set (Xk, Yk). We
denote the random sample drawn at v[th] local step in the t[th] communication round as ( X[˜]k[rt][+][v], _Y[˜]k[rt][+][v])._
Using the stochastic gradient estimate, the clients update their parameters locally by employing the
SGD step. After r local SGD steps, each client shares its updated parameters with the server and gets
back the aggregated parameters before starting the next round of updates. Note that if we choose the
batch size m = Nk, for all k ∈{1, . . ., K}, FedAvg-SGD becomes FedAvg-GD.


-----

**Algorithm 1 The FedAvg-SGD Algorithm**

**Initialize: Parameters θk[0]** [=][ θ][0][, Step-size][ η][, #]
of communication rounds, local updates T, r
**for t = 0, 1, . . ., T −** 1 do

**for each client k ∈{1, . . ., K} do**

Set θk[rt] [= ¯][θ][rt]

**for v = 0, 1, . . ., r −** 1 do

Sample mini-batch of size _m,_
( X[˜]k[rt][+][v], _Y[˜]k[rt][+][v])_

Compute stochastic gradient ˜gk[rt][+][v] using (5)
Update: θk[rt][+][v][+1] = θk[rt][+][v] _−_ _ηg˜k[rt][+][v]._

_K_
Aggregation: _θ[¯][r][(][t][+1)]_ = � _NNk_ _[θ]k[rt][+][r]_

_k=1_

**Return: Parameters,** _θ[¯][rT]_


Algorithm 1 summarizes the above description. For each communication round t
_∈_
0, 1, . . ., T 1 and local step _v_
_{_ _−_ _}_ _∈_
_{0, 1, . . ., r_ _−1}, we define the vector θk[rt][+][v]_ :=

[vec(W1[rt],k[+][v]), . . ., vec(WL,k[rt][+][v][)]][. For FedAvg-]
SGD, define _F[˜]l,k[rt][+][v]_ and _f[˜]l,k[rt][+][v]_ as the output
and the vectorized output of each hidden layer
_l, respectively, when the input to the client_
_k’s local network is the stochastic (mini-batch)_
samples ( X[˜]k[rt][+][v], _Y[˜]k[rt][+][v]). Using the notation_
_y˜k[rt][+][v]_ = vec( Y[˜]k[rt][+][v]) as the vectorized labels
of the stochastic samples at each local step, we
define the mini-batch stochastic loss as:

1
˜Φk(θk[rt][+][v]) := _L,k_ _−_ _y˜k[rt][+][v]∥2[2][,]_ (4)

2m _[∥][f][˜][ rt][+][v]_


and the stochastic gradient as ˜gk[rt][+][v] := [˜g1[rt],k[+][v][, . . .,][ ˜][g]L,k[rt][+][v][]][, where][ ˜][g]l,k[rt][+][v] is the stochastic gradient
w.r.t. the l[th] layer of the network evaluated at the k[th] client:

� �
_g˜l,k[rt][+][v]_ := vec _∇Wl,k_ Φ[˜] _k(θk[rt][+][v])_ _∈_ R[n][l][−][1][n][l] _._ (5)

For each communication round, let us define the aggregated parameters as:

_K_

_θ¯[rt]_ := �vec( W[¯] 1[rt][)][,][ · · ·][,][ vec( ¯][W][ rt]L [)]� _,_ _W[¯]_ _l[rt]_ = � _Nk_ _l,k[.]_ (6)

_N [W][ rt]_

_k=1_

For FedAvg-GD, we denote gk[rt][+][v] := [g1[rt],k[+][v][, . . ., g]L,k[rt][+][v][]][ as the full gradient of][ k][th][ client’s loss]
function, where similar to (5) g1[rt],k[+][v] defines the gradient of the loss function w.r.t. the l[th] layer’s
parameters. Throughout, we make the following standard assumption Ghadimi and Lan (2013).

**Assumption 1. The stochastic gradients at each client are unbiased, i.e., we have E[˜gk[rt][+][v]] = gk[rt][+][v]**
_k_ [K].
_∀_ _∈_
Next, we analyze the performance of the FedAvg for an overparameterized neural network.

### 4 CONVERGENCE ANALYSIS

We present the convergence guarantees of FedAvg when training an overparameterized neural network. We first present a set of assumptions on the network architecture, and activation functions.

**Assumption 2. The width of each hidden layer satisfies: n1 ≥** _N, n2 ≥_ _n3 ≥_ _. . . ≥_ _nL ≥_ 1.

**Assumption 3. The activation function σ(·) in (1) satisfies the following:** 1) _σ[′](x)_ _∈_

[γ, 1]; 2) |σ(x)| ≤|x|; ∀ _x ∈_ R; 3) σ[′] _is β-Lipschitz, with γ ∈_ (0, 1) and β > 0.

**Remark 1. Assumptions 2 and 3 play an important role in our analysis. They help ensure that**
_the local and global loss functions and their (stochastic) gradients are well behaved. Note that As-_
_sumption 2 only requires the first layer to be wide while the rest of the layers can be of constant_
_width. Assumption 2 is required to establish a PL like property for the global and local loss func-_
_tions Nguyen and Hein (2018); Nguyen and Mondelli (2020). Assumption 3 is also standard in_
_the analysis of overparameterized neural networks. Similar assumptions on the smoothness of the_
_activation functions have been made in the past Jacot et al. (2018); Du et al. (2019); Nguyen and_
_Mondelli (2020); Huang and Yau (2020) and are utilized to manage the behavior of the gradients of_
_the loss functions. Importantly, note that as demonstrated in Nguyen and Mondelli (2020) activa-_
_tion functions satisfying Assumption 3 can be utilized to uniformly approximate the ReLU function_
_to arbitrary accuracy._

**Remark 2. We do not impose any assumptions on the distribution of individual clients’ local data**
_sets. In contrast, a majority of works on FL impose restrictive assumptions on the gradients (and/or_
_the Hessians) of each client’s local loss functions to guarantee algorithm convergence Yu et al._
_(2019b); Li et al. (2018); Yu et al. (2019a); Karimireddy et al. (2020a). Below, we list two most_


-----

_popular heterogeneity assumptions (from Yu et al. (2019a) and Koloskova et al. (2020), respec-_
_tively):_

_∥∇Φk(θ) −∇Φ(θ)∥≤_ _δ, ∀θ, ∈_ R[D], ∀ _k ∈_ [K], for some δ > 0. (7)


1

_K_


_K_
�

_∥∇Φk(θ)∥≤_ _δ1 + δ2∥∇Φ(θ)∥, ∀_ _θ ∈_ R[D], for some δ1, δ2 > 0. (8)
_k=1_


_Both conditions impose strong restrictions on the gradients of the local clients, and they do not hold_
_for even simple quadratic loss Khaled et al. (2019); Zhang et al. (2021). We will see shortly that, our_
_results will indicate that as long as the neural network is large enough, then the local (stochastic)_
_gradients will be well-behaved, thereby eliminating the need to impose any additional assumptions_
_on the data distributions._
In the following, we show the convergence guarantees achieved by FedAvg. Our analysis roughly
follows the four steps presented below:

**[Step 1] We first show a key result, that the ratio of the local stochastic gradients and the local full**
gradients stays bounded (Lemma 1). This result is crucial for the FedAvg-SGD analysis, as it allows
us to work with the full local gradients directly, and it helps to bound the gradient drift across local
updates within each communication round.

**[Step 2] Using the result of Step 1, we bound the summation of (stochastic) gradients and the**
gradient drift during the local updates within each communication round (Lemma 2). This result
ensures that irrespective of the data heterogeneity, the gradients size will not change too much from
their initial values at the beginning of each round.

**[Step 3] We then show that adopted network architecture allows us to derive bounds on the size of**
the gradients and ensure the loss function to be PL during the each communication round (Lemma 3).
Utilizing this and the results derived in Steps 1 and 2, we show that the expected loss (3) converges
linearly to zero (Proposition 1).

**[Step 4] Finally, we find a special initialization strategy so that all the conditions imposed on the**
network properties are satisfied during the entire training process.

Next, let us begin with Step 1. We need the following definition.
**Definition 2. Given parameter θk[rt][+][v], we define the following quantity for each k ∈** [K], t ∈
_{0, 1, . . ., T −_ 1} and v ∈{0, 1, . . ., r − 1}: ρ(θk[rt][+][v]) := ∥g˜k[rt][+][v]∥2/∥gk[rt][+][v]∥2.

Clearly, ρ(θk[rt][+][v]) measures the ratio of the norm of stochastic and full gradients of the local loss
functions. In the following, we show that if the model parameters at each client satisfy certain
conditions, then ρ(θk[rt][+][v]) is uniformly bounded. Define σmax(·) and σmin(·) as the largest and
smallest singular value of a matrix, respectively.
**Lemma 1. Let Assumptions 2 and 3 hold. Suppose in any iteration rt + v, v ∈{0, 1, · · ·, r −** 1},
_for θk[rt][+][v]_ = [vec(W1[rt],k[+][v]), . . ., vec(WL,k[rt][+][v][)]][, there exists constant][ ¯Λ][l][,][ Λ]l[,][ Λ]F _[>][ 0][ such that the]_
_singular values of Wl,k[rt][+][v]_ _and F1[rt],k[+][v]_ _satisfy_
σmax(Wl,k[rt][+][v]) ≤ Λ[¯] _l, l ∈_ [L], k ∈ [K],

_σmin(Wl,k[rt][+][v]) ≥_ Λl, l ∈{3, . . ., L}, k ∈ [K], (9)

σmin(F1[rt],k[+][v]) ≥ ΛF, k ∈ [K].

_where λi→j :=_ _l�=j_ _i_ _λl for given layer-wise parameter λl, then: ρ(θk[rt][+][v]) ≤_ _mγLN[L][−][2]l¯Λ1min∈Λ[3L→→]_ _L¯ΛLlΛF_ _[.]_

As discussed earlier in Step 1, this lemma is crucial to our analysis as it allows us to work with full
gradients of individual clients. Before proceeding to Step 2, we need the following definitions:


_K_
�

_k=1_


_Nk_

_N_ _[g][˜]k[rt][+][v]._


_g¯[rt][+][v]_ :=


_K_
�

_k=1_


_NN k_ _[g]k[rt][+][v]_ and _g[¯]˜[rt][+][v]_ :=


Here ¯g[rt][+][v] and _g[¯]˜[rt][+][v]_ are the weighted averages of the full and stochastic gradients, respectively.
Next, in Step 2 (Lemma 2) we first bound the size of the sum of _g[¯]˜[rt][+][v]_ over the local updates
within each communication round. Then we bound the change in ¯g[rt][+][v] from v = 0 to any v
_∈_
0, 1, . . ., r 1 . Note that this quantity measures the drift in the averaged gradients from the start
_{_ _−_ _}_
of each communication round.


-----

**Lemma 2. For FedAvg-SGD, given step size η > 0, v ∈{0, 1, . . ., r** _−_ 1} and q ∈{0, 1, . . ., v _−_ 1}.
_Suppose there exists constants_ Λ[¯] _l, ρ, and A > 0 such that the following conditions hold:_

¯Λl ≥ sup _σmax_ �Wl,k[rt][+][q]� _,_ _ρ ≥_ sup _ρ_ �θk[rt][+][q]� _,_ Φk(θ[rt][+][q]) ≤ _A[q]_ _· Φk(θ[¯][rt]), k ∈_ [K].
_k∈[K]_ _k∈[K]_

_Then we have_


_v_ _v+1_
� _A_ 2 _−_ 1 ¯Λ1→L
�� _q=0_ _g¯˜[rt][+][q][�]�2_ _[≤]_ _[ρL][∥]N[X][∥][F]_ _√A −_ 1 _lmin∈[L]_ ¯Λl _∥fL(θ[¯][rt]) −_ _y∥2._ (10)


_Further, for all k ∈_ [K], ∃Qk > 0, such that we have


�� _K_
��
� _Q[2]k_ _[∥][X][k][∥]F[2]_ _[∥][f][L][(¯][θ][rt][)][ −]_ _[y][∥][2][.]_ (11)

_k=1_


_v+1_
_A_ 2 1

_−_
_√_

_A_ 1
_−_


��g¯rt+v − _g¯rt��2_ _[≤]_ _[ηρL]N_


¯Λ1→L

min ¯Λl
_l∈[L]_


Next, we show Step 3, that the averaged parameter _θ[¯][rt]_ defined in (6), after tth communication round,
will have good performance. Towards this end, we define the full gradient given parameter _θ[¯][rt]_ as

_g[rt]_ := [vec(∇W1 Φ(θ[¯][rt]), . . ., vec(∇WL Φ(θ[¯][rt])]. (12)

**Lemma 3. Let Assumptions 2 and 3 hold. At each communication round rt, suppose there exists**
_constant_ Ω[¯] _l, Ωl, ΩF, such that_
σmax( W[¯] _l[rt][)][ ≤]_ [¯Ω][l][, l][ ∈] [[][L][]][,]

_σmin( W[¯]_ _l[rt][)][ ≥]_ [Ω]l[, l][ ∈{][3][, . . ., L][}][,] (13)

σmin(F1(θ[¯][rt])) ≥ ΩF,

_where_ _θ[¯][rt]_ _and_ _W[¯]lrt are defined in (6). Then we have_


_∥g(θ[¯][rt])∥2 ≥∥_ vec �∇W2 Φ �θ¯rt�� _∥2 ≥_ _[γ][L]N[−][1]_ Ω3→LΩF ��fL(¯θrt) − _y��2_ _[,]_ (14)

_∥g(θ[¯][rt])∥2 ≤_ _N[L]_ min¯Ω1→¯ΩLl ��fL(¯θrt) − _y��2_ _[.]_ (15)

_l∈[L]_

**Remark 3. Note that (14) is a PL-type inequality Karimi et al. (2016), and requires the special**
_structure of the network that satisfies Assumption 2 Nguyen and Hein (2018); Nguyen and Mondelli_
_(2020). Also, (15) can be proven using Assumption 3._

Now, we utilize the results of Steps 1 - 2 and Lemma 3 to derive the convergence of FedAvg.

**Proposition 1. Use Algorithm 1 to minimize (3). Suppose Assumptions 1, 2 and 3 are satisfied, and**
_for each iteration rt + v, v ∈{0, 1, · · ·, r −_ 1}, θk[rt][+][v] _satisfies the conditions in Lemmas 1 and 2;_
_and for each communication round rt,_ _θ[¯][rt]_ _satisfies conditions in Lemma 3, then_ _η > 0 such that_
_∃_

_E[Φ_ �θ¯rt�] ≤ �1 − _[rη]N [γ][2(][L][−][2)][Ω]3[2]→L[Ω][2]F_ �t Φ �θ[0][�] _._ (16)

**Remark 4. Proposition 1 above shows that, if the conditions in Lemmas 1, 2 and 3 are satisfied,**
_i.e., we have well-behaved gradients (Lemmas 1 and 2) and PL condition (Lemma 3), we achieve_
_linear convergence of expected loss function for solving (3) with FedAvg-SGD._

We outline the major steps in the proof of Proposition 1.
_Proof Sketch. Consider the t[th]_ communication round, and suppose the singular values of the parameters satisfy (13), then it is easy to show that Φ(θ[¯][rt]) is Lipschitz smooth with some constant Q > 0.
Then using the Lipschitz smoothness of Φ(θ[¯][rt]), we get


Φ(θ[¯][r][(][t][+1)]) Φ(θ[¯][rt]) _η_ �g[rt], _g[¯]˜[rt]_ + . . . + g[˜]˜[rt][+][r][−][1][�] + _[Q]_
_≤_ _−_ 2 _[η][2][ ��][g][¯˜][rt][ +][ . . .][ + ¯˜][g][rt][+][r][−][1][��]2[2]_ _[.]_

Taking expectation on both sides and conditioning on _θ[¯][rt]_ and the past, we get the following


-----

�
_E[Φ(θ[¯][r][(][t][+1)])] ≤_ _E_ Φ(θ[¯][rt]) − _η⟨g[rt], ¯g[rt]_ + . . . + ¯g[rt][+][r][−][1]⟩ + _[Q]2_ _[η][2][ ��][g][¯˜][rt][ +][ . . .][ + ¯˜][g][rt][+][r][−][1][��]2[2]_


�


_r−1_
�

_g¯[rt][+][v]_ _−_ _g¯[rt]⟩_ + _[Q]2_ _[η][2][ ��][g][¯˜][rt][ +][ . . .][ + ¯˜][g][rt][+][r][−][1][��]2[2]_
_v=1_


= E�Φ �θ¯rt� _−_ _η_ �g[rt], rg¯[rt][�] _−_ _η⟨g[rt],_

_≤_ _E�Φ_ �θ¯rt� _−_ _ηr∥g[rt]∥2[2]_ [+][ η][∥][g][rt][∥]2[∥]


�


_r−1_
�

_g¯[rt][+][v]_ _−_ _g¯[rt]∥2 +_ _[Q]2_ _[η][2][ ��][g][¯˜][rt][ +][ . . .][ + ¯˜][g][rt][+][r][−][1][��]2[2]_
_v=1_


�
_._ (17)


Now we bound each term in (17) using Lemmas 2 and 3. We first use the upper and lower bounds
in Lemma 3 to bound the gradient norm. First, to bound the second term on the right hand side (rhs)
of (17) we use the PL-inequality in (14) of Lemma 3

_∥g[rt]∥2 ≥_ _[γ][L]N[−][1]_ Ω3→LΩF ��fL(¯θrt) − _y��2_ _[.]_ (18)

We bound gradient norm in the third term using the upper bound of gradient in (15) of Lemma 3


_∥g[rt]∥2 ≤_ _N[L]_


¯Ω1→L

min ¯Ωl
_l∈[L]_


��fL(¯θrt) − _y��2_ [:=][ T][1] (19)


Additionally, we use (11) in Lemma 2 to bound the gradient drift in the third term, we get


_K_
�

_Q[2]k_ _[∥][X][k][∥]F[2]_ _[∥][f][L][(¯][θ][rt][)][ −]_ _[y][∥][2][ :=][ T][2]_ (20)
_k=1_


¯Λ1→L

min ¯Λl
_l∈[L]_


�
�
�
�


_v+1_
_A_ 2 1

_−_
_√_

_A_ 1
_−_


_∥_


_r−1_
�

_g¯[rt][+][v]_ _−_ _g¯[rt]∥2 ≤_ _η [ρL]N_
_v=1_


Next, using (10) in Lemma 2 to bound fourth term on the rhs, the sum of stochastic gradient as


_v+1_

��g¯˜rt + . . . + ¯˜grt+r−1��2 _[≤]_ _[ρL][∥]N[X][∥][F]_ _A√A2_ _−11_

_−_


¯Λ1→L _∥f(θ[¯][rt]) −_ _y∥2 := T3._

min ¯Λl
_l∈[L]_


Finally, plugging the bounds for each term in (17), using the definition of loss functionγ[2(][L][−][1)] Φ(θ[¯][rt]) =

21N _[∥][f][L][(¯][θ][rt][)][ −]_ _[y][∥]2[2]_ [along with the choice of step-size][ η <] 2NT[2]1T2Ω+[2]3QT→L3[2][Ω][2]F, we get

� �
_E[Φ(θ[¯][r][(][t][+1)])] ≤_ 1 − _[rη]N [γ][2(][L][−][1)][Ω]3[2]→L[Ω][2]F_ _E[Φ(θ[¯][rt])]._ (21)

Using the above inequality recursively, we get the statement of Proposition 1.
Now Step 3 is complete and we move on to define the initialization strategy of Step 4. It is important
to note that Proposition 1 utilized Lemmas 1 – 3, all of which impose some conditions on the singular
values of the model parameters and the outputs of the first layer at each client during the entire
training phase. Next, we define the initialization strategy that ensures that the conditions of Lemmas
1 – 3 are satisfied almost surely.

Next, we go to Step 4, and discuss the initialization strategy. Define λl := σmin �Wl[0]� and

_λ¯l :=_ � 32 �1 + σmax(Wl[0][)]� _,_ for l ∈{1, 2}, (22)

_σmax(Wl[0][)][,]_ for l ∈{3, . . ., L} _[.]_

We also define the largest and smallest singular values of the output of the first layer at initialization
� � ��
for each client as α0,k := σmin _σ_ _XkW1[0],k_ _. Similarly, for the centralized setting when all the_

clients share the same parameter and full data, we define α0 := σmin �σ �XW1[0]��.

**Initialization Strategy: Given any ϵ < Φ(θ[0]), we initialize the model weights such that for some**
constants M1, M2, M3 > 0, the following are satisfied
_M1 minl∈[L]_ _λ¯l_ 32 � 12 _[λ, l][ ∈{][3][, . . ., L][}][,]_

_·_ [Φ(][θ][0][)] _≤_ _,_ (23)
_∥X∥F_ _λ[¯]1→L_ _ϵ_ 1, l ∈{1, 2},


_M2 minl∈[L]_ _λ¯l_ 32 �

_λ¯1→L_ _·_ [Φ(][θ]ϵ[0][)] _≤_ min _α0, mink∈[K]_ _[α][0][,k]_


� _,_ _M3λ3→Lα0 ≥_ minλ¯1→λ¯Ll _._ (24)

_l∈[L]_


-----

To satisfy the required initialization, we follow the initialization strategy of Nguyen and Mondelli
(2020). First, randomly initialize �W1[0]�ij in[)][. Broadcast][ [][W][ 0]1 []][ij] [to each client and]

_[∼N]_ [(0][,][ 1][/d][2]
collect F1,k, which is the output of the first layer of each client, as well as the norm of local data
_∥Xk∥F . With F1,k, α0 and α0,k can be computed. For (23), since we have n1 > N_, α0 and α0,k
are strictly positive. Then it is easy to verify that given ϵ > 0, (23) and the second relation in (24)
will be satisfied if we choose large enough _lminλ∈¯1[L→]_ _λL¯l_ [. This can be realized by choosing arbitrarily large]

_λ¯l, l ∈{3, · · ·, L}. In order to satisfy the first relation in (24), we need to make ¯λl and λl close_
to each other. Intuitively, one way is to construct �Wl[0]�Ll=3 [such that][ λ][l][ = ¯][λ][l][ =][ ζ >][ 1][, where][ ζ]
can be chosen to be any large number such that (23) and the second relation in (24) are satisfied.
We also need to upper bound Φ(θ[0]). This can be done by choosing small W2[0][. Randomly initialize]
_W2[0]_ [such that] �W2[0]�ij

_[∼N]_ [(0][, κ][)][. We can set][ κ][ to be arbitrarily small, then][ Φ(][θ][0][)][ is bounded by]

2

_N_ _[∥][y][∥]2[2]_ [with high probability (see (10) in Nguyen and Mondelli (2020)). Note that the desired error]
_ϵ is another key constant in the initialization. When we expect the error to be small, (23) and the_
second relation in (24) will be more strict. But this is not an issue since we can choose a larger ζ
such that the initial conditions are satisfied. The detailed initialization strategy that ensures that the
conditions of Lemmas 1, 2 and 3 are satisfied is given in the Appendix B.2.

Next, let us state our main result, which indicates the linear convergence of local SGD to any ϵoptimal solution (see Definition 1). The proof is attached in Appendix B.3.
**Theorem 1. Using FedAvg-SGD to minimize (3) with Algorithm 1. Suppose Assumptions 1, 2 and**
_3 are satisfied, then there exists an initialization strategy such that for any ϵ < Φ(θ[0]), there exists_
_step-size η > 0 such that we have (where µ[′]_ := 2rN _[γ][2(][L][−][2)][ �]_ 2[1] �2(L−1) λ23→L[α]0[2][, and][ ηµ][′][ <][ 1][)]

_E[Φ(θ[¯][r][(][t][+1)])]_ (1 _µ[′]η)[t]_ Φ(θ[0]), t 0, . . ., T 1 _._
_≤_ _−_ _∈{_ _−_ _}_

Theorem 1 shows that, for any ϵ > 0, we can always find an initialization, such that FedAvgSG achieves an ϵ accuracy within O �log( [1]ϵ [)]� rounds of communication. Notice that there is no

heterogeneity assumption on the data (see Remark 2), and no assumption on the Lipschitz gradient
of the loss function.
**Remark 5. We comment on the key novelties of this work compared to Nguyen and Mondelli (2020).**
_(1) Our work requires a careful analysis to deal with multiple local updates at each client. Note_
_that in contrast to Nguyen and Mondelli (2020), for our algorithm there is no guarantee that the_
_overall objective will always decrease during local updates. In fact, our analysis demonstrates_
_that the overall objective can increase after each local iteration, we show that this increase will be_
_compensated by the descent in the objective value between each communication round._
_(2) Our algorithm and analysis can deal with the stochastic gradients for conducting local updates,_
_while Nguyen and Mondelli (2020) only considered gradient descent in a centralized setting. A_
_key step in our analysis is to characterize the relationship between the stochastic and full gradient_
_updates, which is illustrated in Lemma 1._
**Remark 6. We comment on the choice of parameters and the convergence rate. As will be shown**
_in Appendix B.3, by utilizing our initialization strategy, we can choose η = c/µ[′]_ _for some constant_
_c_ (0, 1) (independent of ϵ). This implies that µ[′]η = c < 1, which further implies that we have
_∈_
(1 _µ[′]η) < 1 in Theorem 1, ensuring linear convergence of FedAvg-SGD._
_−_

Finally, we present the convergence guarantees for the case when FedAvg-GD is utilized.
**Corollary 1. Using FedAvg-GD to minimize (3) with Algorithm 1. Suppose Assumptions 2 and 3**
_are satisfied, then there exists an initialization strategy and step-size η > 0, such that we have_

Φ(θ[¯][r][(][t][+1)]) (1 _µ[′]η)[t]_ Φ(θ[0]), _t_ 0, . . ., T 1 _._ (25)
_≤_ _−_ _∀_ _∈{_ _−_ _}_

**Remark 7. Corollary 1 implies that FedAvg-GD achieves linear convergence when optimizing (3).**
_We note that the result of Corollary 1 is much stronger compared to Theorem 1 as the initialization_
_for FedAvg-GD is independent of ϵ compared to the one for FedAvg-SGD (shown in Appendix B.2)._

### 5 NUMERICAL EXPERIMENTS

In this section, we analyze the effect of increasing the network sizes on popular image classification
tasks with MNIST, Fashion MNIST and CIFAR-10 data sets. We compare the performance of


-----

FedAvg in both homogeneous (i.i.d.) and heterogeneous (non-i.i.d.) data settings. Through our
experiments we establish that larger sized networks uniformly outperform smaller networks under
different settings. Next, we discuss the data and the model setting for our experiments.

Figure 1: CIFAR-10 with CNN: FedAvg-SGD on large and small size CNN.

Figure 2: CIFAR-10 with ResNet: Comparison of FedAvg-SGD on ResNet18 and ResNet50.

Figure 3: Test accuracy for MNIST (left) and Fashion MNIST (right) datasets. We compare the performance with (standard) random initialization and the proposed initialization strategy for both iid
and noniid settings. Legends ‘iid_ini’ and ‘noniid_ini’ represent the proposed initialization strategy.
**Data set: For MNIST, Fashion MNIST and CIFAR-10 data sets, we split the data set among K =**
100 clients. For the homogeneous (i.i.d.) setting, we randomly distribute the complete data set with
60, 000 samples to each client. To model the heterogeneous (non i.i.d.) setting, we split the clients
into two sets. One set of clients receive randomly drawn samples while the second set of clients
receive data from only two out of ten labels McMahan et al. (2017). For our experiments on MNIST
and Fashion MNIST data, 70% of the users receive non-i.i.d samples, while for CIFAR-10 data, the
fraction is 20%.
**Results and Discussion For each setting, we compare the training loss and testing accuracy of Fe-**
dAvg on smaller and larger sized networks. To analyze the effect of network sizes on the stability
of FedAvg, we also plot the performance of FedAvg averaged over 10 iterations for non-i.i.d. client
data setting for all the network architectures. From our experiments, we make a few observations.
First, we observe from Figures 1 and 2 that in all the cases, the i.i.d setting has more stable performance (lower variance) than non-i.i.d setting. Second, we note that the larger network uniformly
outperforms the smaller network under all the settings. Third, we note from the box plots in Figures
1 and 2 that the performance of the larger networks have lower variance, hence more stable performance compared with what can be achieved by the smaller networks. Finally, we compare the
random initialization with special initialization strategy which satisfies (23), (24). We can conclude
from Figure 3 that these two initialization are similar in test performance.


-----

### REFERENCES

D. A. E. Acar, Y. Zhao, R. Matas, M. Mattina, P. Whatmough, and V. Saligrama. Federated learning
based on dynamic regularization. In International Conference on Learning Representations, 2020.

Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization.
In International Conference on Machine Learning, pages 242–252. PMLR, 2019.

S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang. On exact computation with an
infinitely wide neural net. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
Associates, Inc., 2019.

M. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and the
classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32):
15849–15854, 2019.

R. Das, A. Acharya, A. Hashemi, S. Sanghavi, I. S. Dhillon, and U. Topcu. Faster non-convex
federated learning via global and local momentum. arXiv preprint arXiv:2012.04061, 2020.

Y. Deng and M. Mahdavi. Local sgd optimizes overparameterized neural networks in polynomial
time. arXiv preprint arXiv:2107.10868, 2021.

S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural
networks. In International Conference on Machine Learning, pages 1675–1685. PMLR, 2019.

S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
neural networks. arXiv preprint arXiv:1810.02054, 2018.

S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013.

F. Haddadpour and M. Mahdavi. On the convergence of local descent methods in federated learning.
_arXiv preprint arXiv:1910.14425, 2019._

B. Huang, X. Li, Z. Song, and X. Yang. Fl-ntk: A neural tangent kernel-based framework for
federated learning analysis. In International Conference on Machine Learning, pages 4423–4434.
PMLR, 2021.

J. Huang and H.-T. Yau. Dynamics of deep neural networks and neural tangent hierarchy. In
_International Conference on Machine Learning, pages 4542–4551. PMLR, 2020._

A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in
neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.

H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition. In Joint European Conference on Machine Learning
_and Knowledge Discovery in Databases, pages 795–811. Springer, 2016._

S. P. Karimireddy, M. Jaggi, S. Kale, M. Mohri, S. J. Reddi, S. U. Stich, and A. T. Suresh.
Mime: Mimicking centralized stochastic algorithms in federated learning. _arXiv preprint_
_arXiv:2008.03606, 2020a._

S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh. Scaffold: Stochastic
controlled averaging for federated learning. In International Conference on Machine Learning,
pages 5132–5143. PMLR, 2020b.

A. Khaled, K. Mishchenko, and P. Richtárik. First analysis of local gd on heterogeneous data. arXiv
_preprint arXiv:1909.04715, 2019._

P. Khanduri, P. Sharma, H. Yang, M. Hong, J. Liu, K. Rajawat, and P. K. Varshney. Stem: A
stochastic two-sided momentum algorithm achieving near-optimal sample and communication
complexities for federated learning. arXiv preprint arXiv:2106.10435, 2021.


-----

A. Koloskova, N. Loizou, S. Boreiri, M. Jaggi, and S. Stich. A unified theory of decentralized sgd
with changing topology and local updates. In International Conference on Machine Learning,
pages 5381–5393. PMLR, 2020.

T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith. Federated optimization in
heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.

X. Li, M. Jiang, X. Zhang, M. Kamp, and Q. Dou. Fedbn: Federated learning on non-iid features
via local batch normalization. arXiv preprint arXiv:2102.07623, 2021.

Y. Li and Y. Liang. Learning overparameterized neural networks via stochastic gradient descent on
structured data. arXiv preprint arXiv:1808.01204, 2018.

T. Lin, S. U. Stich, K. K. Patel, and M. Jaggi. Don’t use large mini-batches, use local sgd. In
_International Conference on Learning Representations, 2020._

B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient
learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages
1273–1282. PMLR, 2017.

Q. Nguyen. On the proof of global convergence of gradient descent for deep relu networks with
linear widths. arXiv preprint arXiv:2101.09612, 2021.

Q. Nguyen and M. Hein. Optimization landscape and expressivity of deep cnns. In International
_conference on machine learning, pages 3730–3739. PMLR, 2018._

Q. Nguyen and M. Mondelli. Global convergence of deep networks with one wide layer followed
by pyramidal topology. arXiv preprint arXiv:2002.07867, 2020.

S. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Koneˇcn`y, S. Kumar, and H. B. McMahan.
Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020.

S. U. Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767,
2018.

J. Wang and G. Joshi. Cooperative sgd: A unified framework for the design and analysis of localupdate sgd algorithms. Journal of Machine Learning Research, 22(213):1–50, 2021.

B. Woodworth, K. K. Patel, and N. Srebro. Minibatch vs local sgd for heterogeneous distributed
learning. arXiv preprint arXiv:2006.04735, 2020a.

B. Woodworth, K. K. Patel, S. U. Stich, Z. Dai, B. Bullins, H. B. McMahan, O. Shamir, and N. Srebro. Is local sgd better than minibatch sgd? arXiv preprint arXiv:2002.07839, 2020b.

H. Yang, M. Fang, and J. Liu. Achieving linear speedup with partial worker participation in non-iid
federated learning. arXiv preprint arXiv:2101.11203, 2021.

H. Yu, R. Jin, and S. Yang. On the linear speedup analysis of communication efficient momentum
sgd for distributed non-convex optimization. In International Conference on Machine Learning,
pages 7184–7193. PMLR, 2019a.

H. Yu, S. Yang, and S. Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI
_Conference on Artificial Intelligence, volume 33, pages 5693–5700, 2019b._

X. Zhang, M. Hong, S. Dhople, W. Yin, and Y. Liu. Fedpd: A federated learning framework
with adaptivity to non-iid data. IEEE Transactions on Signal Processing, pages 1–1, 2021. doi:
10.1109/TSP.2021.3115952.

F. Zhou and G. Cong. On the convergence properties of a k-step averaging stochastic gradient descent algorithm for nonconvex optimization. In Proceedings of the Twenty-Seventh International
_Joint Conference on Artificial Intelligence, IJCAI-18, pages 3219–3227, 7 2018._

D. Zou and Q. Gu. An improved analysis of training over-parameterized deep neural networks.
_arXiv preprint arXiv:1906.04688, 2019._


-----

### APPENDIX

 A RELATED WORK

**Overparameterized Neural Networks.** The surprising performance of overparameterized neural
networks has raised significant research interest in the ML community to analyze the phenomenon of
overparameterization Belkin et al. (2019). Consequently, a number of works have analyzed the performance of centralized (stochastic) gradient descent (S)GD on overparameterized neural network
architectures under different settings Jacot et al. (2018); Li and Liang (2018); Arora et al. (2019);
Du et al. (2018; 2019); Allen-Zhu et al. (2019); Zou and Gu (2019); Nguyen and Mondelli (2020);
Nguyen (2021). The authors in Jacot et al. (2018), showed that an infinite width neural network
when trained using gradient descent (GD) behaves like a kernel method with the kernel defined as
neural tangent kernel (NTK). Using this NTK parameterization Li and Liang (2018) showed that
deep neural networks trained using GD require Ω(N [4]) width to find the global optimal. This result was later improved to Ω(N [3]) in Huang and Yau (2020). The authors in Du et al. (2018) and
Du et al. (2019) also analyze the performance of GD on overparameterized neural networks under
different settings. Under standard parameterization, the work Allen-Zhu et al. (2019) studied the
convergence of SGD and showed that network width of Ω(N [24]) suffices to guarantee linear convergence. Recently, Nguyen and Mondelli (2020) and Nguyen (2021) have improved the dependence
on the width and have shown that GD requires only Ω(N ) width to achieve linear convergence. All
the works mentioned above focus on the centralized setting, and therefore, do not deal with data
heterogeneity problem.

### B PROOF OF MAIN RESULT

B.1 PROOF OF LEMMAS

We define some additional notations before we state some lemmas which are needed in the
proof. Let ⊗ denote the Kronecker product, and denote Σl := diag [vec (σ[′] (Fl−1Wl))] ∈
R[Nn][l][×][Nn][l], Σl,k := diag [vec (σ[′] (Fl−1,kWl,k))] _∈_ R[N][k][n][l][×][N][k][n][l] and ˜Σl,k :=

� � ���
diag vec _σ[′][ �]F˜l−1,kWl,k_ _∈_ R[mn][l][×][mn][l] . Define fL,k[rt][+][v] := _fL,k(θk[rt][+][v]), FL,k[rt][+][v]_ :=

_FL,k(θ[rt][+][v]); fL[rt]_ [:=][ f][L][(¯][θ][rt][)][, F]L[ rt] [:=][ F][L][(¯][θ][rt][+][v][)][, f][L][(][θ][rt][+][v][) :=][ vec][(][F][ rt]L [+][v]).

**Lemma 4. Nguyen and Mondelli (2020) Suppose Assumptions 2 and Assumption 3 are satisfied.**
_Then for l_ [L] the following holds:
_∈_


1. gl,k = [1]

_Nk_


_L_
�Inl ⊗ _Fl[T]−1,k�_ � Σp−1,k (Wp,k ⊗ _INk_ ) (fL,k − _yk),_

_p=l+1_


(26)

�WL[T]−p,k _[⊗]_ _[I][N]k_ � ΣL−t−1 �Inl,k ⊗ _Fl−1,k�_ _,_ (27)


_∂fL,k_
2.

_∂_ vec(Wl,k) [=]


_L−l−1_
�

_p=0_


3. ∥g2,k∥2 ≥ _N[1]k_ _σmin (F1,k)_


_L_
�

_σmin (Σp−1,k) σmin (Wp,k) ∥fL,k −_ _yk∥2,_ (28)
_p=3_


4. ∥Fl,k∥F ≤∥Xk∥F


_l_
�

_σmax(Wp,k),_ (29)
_p=1_


5. ��∇Wl,k Φk��F _[≤]_ _N[1]k_ _∥Xk∥F_


_L_
�

_p=1_
_p≠_ _l_


_σmax(Wp,k) ∥fL,k −_ _yk∥2,_ (30)


6. ∥gk∥2 ≤ _[L][∥][X]N[k][∥][F]_


_L_
� _σmax(Wl,k)_

_l=1_

min
_l∈[L]_ _[σ][max][(][W][l,k][)]_


_L_
�

_σmax (Σl−1,k) ∥fL,k −_ _yk∥2._ (31)
_l=2_


-----

� � � � ��
_Furthermore, given with θk[a]_ _[and][ θ]k[b]_ _[, if][ ¯Λ][l][ ≥]_ [max] _σmax_ _Wl,k[a]_ _, σmax_ _Wl,k[b]_ _for some scalars_

_L_
¯Λl. Let R = � max �1, Λ[¯] _p�. Then, for l ∈_ [L],

_p=1_


(32)


_√_
7. ��F aL,k _[−]_ _[F][ b]L,k��F_ _[≤]_


_L∥Xk∥F_


_L_
� ¯Λl

_l=1_

min ¯Λl
_l∈[L]_


��θka _[−]_ _[θ]k[b]_ ��2 _[,]_


_√_
_≤_
�����2


8.


_∂fL (θk[a][)]_ _∂fL_ �θk[b] �
����� _∂_ vec (Wl[a][)][ −] _∂_ vec �Wl[b]�


_L∥Xk∥F R (1 + Lβ∥Xk∥F R)_ ��θka _[−]_ _[θ]k[b]_ ��2 _[.e]_ (33)


The above Lemma follows Lemma 4.1 Nguyen and Mondelli (2020): (26) gives the expression of
the vectorized gradient; (27) provides the vectorized Jacobian matrix of the output of the network;
(28) gives a lower bound on the norm of the gradient, which holds under Assumption 2. (29)
provides an upper bound on the norm of output of each layer while (30) gives an upper bound on the
norm of gradient of each layer; (32) derives the Lipschitz constant of the networks and (33) provides
the Lipschitz constant for the Jacobian of each layer. Similar results can be derived in centralized
optimization problem, so we do not include the results here.

**Lemma 5. (Nguyen and Mondelli, 2020, Lemma 4.3) Let f : R[n]** _→_ R be a C [2] _function. Let_
_x, y ∈_ R[n] _be given, and assume that ∥∇f_ (z) −∇f (x)∥2 ≤ _C∥z −_ _x∥2 for every z = x + t(y −_ _x)_
_with t_ [0, 1]. Then,
_∈_

_f_ (y) _f_ (x) + _f_ (x), y _x_ + _[C]_
_≤_ _⟨∇_ _−_ _⟩_

2

_[∥][x][ −]_ _[y][∥][2][.]_

**Lemma 6. For constant C, µ, ρ, if η →** 0, we have


1

log 1−µCη1 = e


3ρ
2µ (34)


lim
_η→0_


�

1 + 3ρCη


� log(Φ(θ[0])/ϵ) �
_Furthermore, given ϵ < Φ(θ[0]), let T =_ log( 1−µCη1 [)][ + 1] _, then there exists constant ξ, such that_


�T
1 + 3ρCη _,_ (35)
_≤_ _[ξ][Φ(][θ][0][)]_

_ϵ_


sup
0<η<min( _ρC[1]_ _[,]_ _µC1_ [)]


��


_where ξ_ _e_
_≥_


3ρ
2µ
_is a constant dependent on ρ and µ._


_Proof. Take logarithm on both sides, we get_


��
log


1 + 3ρCη


log 1−1 _µCη1_ � = = (36)

_−_ [log(][√][1 + 3][ρCη][)] _−_ [1]

log(1 _µCη)_ 2 log(1 _µCη)_
_−_ _[·][ log(1 + 3] −_ _[ρCη][)]_


Now let η 0, by L’Hôpital’s rule, take derivative over η, we have
_→_

1 1 _µCη_

_−_

lim (37)
_η→0_ _[−]_ [1]2 _[·][ log(1 + 3]log(1 −_ _µCη[ρCη]) [)]_ [= lim]η→0 2 _[·][ 3]µC[ρC]_ 1 + 3ρCη [= 3]2µ[ρ] _[.]_


1

Next, if we can show the function of η, which is 1 + 3ρCη log 1−µCη1, has a limit when η

_[√]_ _→_

min( _ρC[1]_ _[,]_ _µC1_ [)][, then by the continuity, it has an upper bound in] �0, min( _ρC[1]_ _[,]_ _µC1_ [)]�, denote it as ξ. It

is easy to derive that


1

log 1

2 1− _[µ]ρ, ρ > µ,_

1, _ρ_ _µ._
_≤_


�


1 + 3ρCη


1

log 1−1µη


lim
_η→min(_ _ρC[1]_ _[,]_ _µC1_ [)]


�


1

log 1−µCη1 = lim

_η→min(_ _ρ[1]_ _[,][ 1]µ_ [)]


�

1 + 3ρη


-----

1
Then by the continuity of the function, �√1 + 3ρCη� log 1−µCη1 is bounded by some constant ξ.

Then we can derive


3ρ
2µ [Φ(][θ][0][)] (38)
_·_

_ϵ_


log(Φ(θ[0] )/ϵ)

log 1−1µη = e


�

sup
_η∈(0,min(_ _ρC[1]_ _[,]_ _µC1_ [)][)]


_T_ �
1 + 3ρCη lim 1 + 3ρη
_≥_ _η→0_


then we have there exists some constant ξ _e_
_≥_


3ρ
2µ
, such that


_T_ _ξΦ_ �θ[0][�]
1 + 3ρCη _._ (39)
_≤_

_ϵ_


sup
_η∈(0,min(_ _ρC[1]_ _[,]_ _µC1_ [)][)]


�


**Lemma 7. Let Assumption 2 and Assumption 3 hold. For θk, suppose there exists constant** Λ[¯] _l, Λl,_
ΛF such that and
σmax(Wl,k) ≤ Λ[¯] _l, l ∈_ [L], k ∈ [K],


_σmin(Wl,k) ≥_ Λl, l ∈{3, . . ., L}, k ∈ [K], (40)

σmin(F1,k) ≥ ΛF, k ∈ [K].


_then we have_

_Proof. By definition, we have_


_LN_ ¯Λ1→L

min ¯Λl

_ρ(θk) ≤_ _l∈[L]_ (41)

_mγ[L][−][2]Λ3→LΛF_


_ρ(θk) =_ _[∥][g][˜][k][∥][2]_ _≤_ _[∥][g][˜][k][∥][2]_ _._ (42)

_∥gk∥2_ _∥g2,k∥2_

Since by (31) and (28) in Lemma 4, we have


_Xk∥F_ ¯Λ1→L
_∥g˜k∥2 ≤_ _[L][∥]_ _m[˜]_ min ¯Λl _∥f[˜]L,k(θ) −_ _y˜∥2,_ (43)

_l∈[L]_

_∥g2,k∥2 ≥_ _N[1]k_ _γ[L][−][2]Λ3→LΛF ∥fL,k(θ) −_ _y∥,_ (44)

where Xk is the sampled data at θk. So we can derive


_ρ(θk) ≤_


_L∥Xm[˜]k∥F_ min¯Λ1→¯ΛLl _L,k[(][θ][)][ −]_ _[y][˜]k[∥]2_ _LN_ _∥X∥F_ min¯Λ1→¯ΛLl

1 _l∈[L]_ _[∥][f][˜]_ _≤_ _l∈[L]_ _,_ (45)

_Nk_ _[γ][L][−][2][Λ][3][→][L][Λ][F]_ _[∥][f][L,k][(][θ][)][ −]_ _[y][k][∥][2]_ _mγ[L][−][2]Λ3→LΛF_


where the last inequality is because ∥X[˜]k∥F ≤∥X∥F and ∥f[˜]L,k(θ) − _y˜k∥2 ≤∥fL,k(θ) −_ _yk∥2._

**Lemma 8. For the FedAvg-SGD algorithm, given step size η > 0, v ∈{0, 1, . . ., r −** 1} and
_q_ 0, 1, . . ., v 1 _. Suppose the following conditions hold:_
_∈{_ _−_ _}_


� �
1.Λ[¯] _l ≥_ sup _σmax_ _Wl,k[rt][+][q]_ _,_
_k∈[K]_


(46)


2.ρ ≥ sup _ρ_ �θk[rt][+][q]� _,_ (47)
_k∈[K]_

3.Φk(θ[rt][+][q]) ≤ _A[q]_ _· Φk(θ[rt]), k ∈_ [K], (48)

_then we have_


_v_ _v_
�� � _g¯˜[rt][+][q][�]�2_ �

_[≤]_
_q=0_ _q=0_


_v+1_

��g¯˜rt+q��2 _[≤]_ _[ρL][∥]N[X][∥][F]_ _A√A2_ _−11_

_−_


min¯Λ1→¯ΛLl _∥fL[rt]_ _[−]_ _[y][∥][2][.]_ (49)
_l∈[L]_


-----

_Further, there exists constant Qk, such that ∀k ∈_ [K] we have
���gkrt+q+1 _−_ _gk[rt][+][q]���2_ ���θkrt+q+1 _−_ _θk[rt][+][q]���2_ _[,]_

_[≤]_ _[ρQ][k]_

_and_


(50)


_K_
�

_Q[2]k_ _[∥][X][k][∥]F[2]_ _[∥][f][ rt]L_ _[−]_ _[y][∥][2][.]_
_k=1_

(51)


��g¯rt+q+1 − _g¯rt+q��2_ _[≤]_ _[ηρL]N_ minΛ1→L

_l∈[L]_ [Λ][l]


�
�
�
�


_v+1_
_A_ 2 1

_−_
_√_

_A_ 1
_−_


��g¯rt+v − _g¯rt��2_

_[≤]_


_v−1_
�

_q=0_


_Proof. First, let us show (49)._


_v_
�� � _g¯˜[rt][+][v][�]�2_

_q=0_


_K_
�

_k=1_


_K_
�

_k=1_


_v_
�

_q=0_


_Nk_ _k_ _∥2_ (52)

_N_

_[∥][g][rt][+][v]_


(i)
_≤_


_v_
� ��g¯˜rt+v��2

_q=0_


(ii)
_≤_


_v_
�

_q=0_


_Nk_

_k_ _∥2_
_N_

_[∥][g][˜][rt][+][v]_


(iii)
_ρ_
_≤_


�K _∥Xk∥F_ ¯Λ1→L _∥fL,k[rt][+][q]_ _−_ _y∥2_ (53)

min ¯Λl

_k=1_ _l∈[L]_


(iv)
_≤_ _[ρL]_

_N_


_v_
�

_q=0_


_v_
� _∥fL,k[rt][+][q]_ _−_ _yk∥2_ (54)

_q=0_


= _[ρL]_ ¯Λ1→L

_N_ min

_l∈[L]_


_K_
�

_∥Xk∥F_
_k=1_


_v_
� _A_ _q2 ∥fL,k[rt][+][q]_ _−_ _yk∥2_ (55)

_q=0_


(v) ¯Λ1→L
_≤_ _[ρL]_

_N_ min ¯Λl

_l∈[L]_


_K_
�

_∥Xk∥F_
_k=1_


_K_
� _∥Xk∥F ∥fL,k[rt][+][q]_ _−_ _yk∥2_ (56)

_k=1_


= _[ρL]_ ¯Λ1→L

_N_ min ¯Λl

_l∈[L]_


_v+1_
_A_ 2 1

_−_
_√_

_A_ 1
_−_


_K_
�

_∥Xk∥F[2]_
_k=1_


�
�
�
�


_K_
� _∥fL,k[rt][+][q]_ _−_ _yk∥F[2]_ (57)

_k=1_


_A_ _v+12_ _−_ 1 ���

_√_ �

_A_ 1
_−_


(vi)
_≤_ _[ρL]_

_N_


¯Λ1→L

min ¯Λl
_l∈[L]_


_v+1_
_A_ 2 1

_√A_ _−1_ _∥fL[rt]_ _[−]_ _[y][∥][2][,]_ (58)

_−_


= _[ρL][∥][X][∥][F]_

_N_


¯Λ1→L

min ¯Λl
_l∈[L]_


So we can derive (49). Next, we show (50). Let us denote _JfL,k[rt][+][q]_ :=
� _∂_ vec(∂fL,k[rt]W[+]1[q],k) _[, . . .,]_ _∂_ vec(∂fL,k[rt]W[+]L,k[q] ) �. By triangle inequality, we have


���gkrt+q+1 _−_ _gk[rt][+][q]���2_ [=] ���Jf rtL,k+q+1 �fL,k[rt][+][q][+1] _−_ _yk�_ _−_ _JfL,k[rt][+][q]_ �fL,k[rt][+][q] _−_ _yk����_

_≤_ ���f rtL,k+q+1 _−_ _fL,k[rt][+][q]���2_ ���Jf rtL,k+q+1���2 [+] ���Jf rtL,k+q+1 _−_ _JfL,k[rt][+][q]���2_


���f rtL,k+q _−_ _yk���2_
(59)


� � � � ��
Now we find the bound for each term in (59). Since max _σmax_ _Wl,k[rt][+][q][+1]_ _, σmax_ _Wl,k[rt][+][q]_ _≤_
¯Λl, by (32) in Lemma 4, we get
���f rtL,k+q+1 _−_ _fL,k[rt][+][q]���2_ _[≤]_ _√L ∥Xk∥F_ min¯Λ1→¯ΛLl ���θkrt+q+1 _−_ _θk[rt][+][q]���2_ (60)

_l∈[L]_


Further, by (27) we have

_Jf rtL,k+q+1_
��� ���2

_[≤]_


_L_
�

_l=1_


_≤_ _L ∥Xk∥F_ ¯Λ1→L _._ (61)

min ¯Λl

�����2 _l∈[L]_


_∂JfL,k[rt][+][q][+1]_

_∂vec (Wl,k)_

�����


-----

Using (33) in Lemma 4, we have

_∥JfL,k[rt][+][q][+1]_ _−_ _JfL,k[rt][+][q][∥][2][ ≤]_ �l=1L ����� _∂∂Jfvec (L,k[rt]W[+][q]l,k[+1])_ _[−]_ _∂_ _∂Jfvec (L,k[rt]W[+]l,k[q]_ ) �����2

_≤_ _L_ 23 ∥Xk∥F R[′] (1 + Lβ ∥Xk∥F R[′]) ���θkrt+q+1 _−_ _θk[rt][+][q]���2_ _[,]_ (62)

where R[′] = [�]p[L]=1 [max] �1, Λ[¯] _l�. So plug the above bounds into (59). Set Lipschitz constant_


_√_ _√_
_Qk =_ _[L]NkL_ _∥Xk∥F[2]_ min¯Λ[2]1→¯ΛL[2]l + _[L]NkL_ _∥Xk∥F (1 + Lβ ∥Xk∥F R[′]) R[′][ ��]fL,k[0]_ _[−]_ _[y][k]��2_ _[,]_ (63)

_l∈[L]_

then we derive


���gkrt+q+1 _−_ _gk[rt][+][q]���2_ _k_ _−_ _θk[rt][+][q]∥2._ (64)

_[≤]_ _[Q][k][∥][θ][rt][+][q][+1]_

Now (50) is proved. Last, we prove (51). We have


_v−1_ _K_
� �

_q=0_ _k=1_


_Nk_

_N_


���gkrt+q+1 _−_ _gk[rt][+][q]���2_


��g¯rt+v − _g¯rt��2_

_[≤]_


_v−1_
� _rt+q+1_ _rt+q_

��g¯ _−_ _g¯_ ��2

_q=0_


(i)
_≤_


_K_
�

_k=1_


_v−1_
�

_q=0_

_v−1_
�

_q=0_


(ii)
_≤_


_K_
�

_k=1_


_NN k_ _[Q][k]_ ���θkrt+q+1 _−_ _θk[rt][+][q]���2_ [=]


_v−1_
�

_q=0_


_NN k_ _[Q][k][ ·][ η]_ ��g˜krt+q��2


_Qk_ ¯Λ1→L

_N [L][ ∥][X][k][∥][F]_ min ¯Λl

_l∈[L]_


(iii)
_≤_


_K_
�

_k=1_


��f rtL +q _−_ _yk��2_


(iv) ¯Λ1→L
_≤_ _[ηL]_

_N_ min ¯Λl

_l∈[L]_


_K_
�

_Qk ∥Xk∥F_
_k=1_


_v�−1_ _A_ _q2_ �[�]fL[rt][+][q] _−_ _yk��22_

_q=0_


_K_
�

_Q[2]k_ _[∥][X][k][∥]F[2]_ _[· ∥][f][ rt]L_ [+][q] _−_ _y∥2,_
_k=1_


¯Λ1→L

min ¯Λl
_l∈[L]_

¯Λ1→L

min ¯Λl
_l∈[L]_

¯Λ1→L

min ¯Λl
_l∈[L]_


_K_
� _Qk ∥Xk∥F_ ��f rtL +q _−_ _yk��22_

_k=1_


_≤_ _[ηL]_

_N_


_K_
�

_k=1_


��f rtL +q _−_ _yk��22_


�
�
�
�


(v)
_≤_ _[ηL]_

_N_


�
�
�
�


_K_
�

_Q[2]k_ _[∥][X][k][∥]F[2]_
_k=1_


= _[ηL]_

_N_


_v+1_
_A_ 2 1

_−_
_√_

_A_ 1
_−_

_v+1_
_A_ 2 1

_−_
_√_

_A_ 1
_−_

_v+1_
_A_ 2 1

_−_
_√_

_A_ 1
_−_


�
�
�
�


where (i) uses triangle inequality; (ii) uses the Lipschitz gradient assumption in condition 2; (iii)
comes from (31) in Lemma 4; (iv) uses condition 3; (v) is from Cauchy-Schwartz inequality.


-----

B.2 INITIALIZATION STRATEGY

**Detailed Initialization for FedAvg-SGD: Denote**


�L−1
(2[r] 1),
_−_


_P :=_ _[L][∥][X][∥][F]_

_N_


� 7

4


_C := PL∥X∥F_ � 32 �L−1 ¯minλ21→λ¯L[2]l _,_ (65)

_l∈[L]_

_LN_ _∥X∥F 7[L][−][1 ¯]min[λ][1][→]λ[L]¯l_

_ρ :=_ _l∈[L]_ _,_ (66)

� �
_mγ[L][−][2]λ3→L min_ _α0, mink∈[K]_ _[α][0][,k]_

_µ :=_ 2Nr [2][ γ][2(][L][−][2)][ �] 2[1] �2(L−1) λ23→L[α][2]0 _._ (67)

_C_

Suppose given any small ϵ such that ϵ < Φ(θ[0]), the initialized weights satisfies the following conditions:


3
2N 2 _[ξ][Φ(][θ][0][)]_

_·_

_L∥X∥F (_ 2[3] [)][L][−][1 ¯]min[λ][1][→]λ[L]¯l _ϵ_

_l∈[L]_


� 1

�2Φ(θ[0]) 2 _[λ][l][, l][ ∈{][3][,][ · · ·][, L][}][,]_ (68)

_≤_ 1, l 1, 2 _._
_∈{_ _}_


3
2N 2 _[ξ][Φ(][θ][0][)]_

_·_

_L(_ [3]2 [)][L][−][1 ¯]min[λ][1][→]λ[L]¯l _ϵ_

_l∈[L]_


�

�

2Φ(θ[0]) ≤ [1] _α0, min_

2 [min] _k∈[K]_ _[α][0][,k]_


�
_._ (69)


where ξ _e_
_≥_


3ρ
2µ
is some constant dependent on ρ and µ.


Now we provide a detailed way to realize the above initialization condition. To satisfy the required
initialization, we follow the initialization strategy of Nguyen and Mondelli (2020). First, randomly
initialize �W1[0]�ij in[)][. Broadcast][ [][W][ 0]1 []][ij] [to each client and collect][ F][1][,k][, which is the]

_[∼N]_ [(0][,][ 1][/d][2]
output of the first layer of each client, as well as the norm of local data ∥Xk∥F and norm of local
label ∥yk∥2. With F1,k, α0 and α0,k can be computed. For (23), since we have n1 > N, α0 and
_α0,k are strictly positive with probability 1. Then it is easy to verify that given ϵ > 0, (23) and_
the second relation in (24) will be satisfied if we choose large enough _lminλ∈¯1[L→]_ _λL¯l_ [. This can be realized]

by choosing arbitrarily large λl, l ∈{3, · · ·, L}. However, notice that by Lemma 6, the constant3ρ
_ξ, which is defined in (39), is only dependent on ρ and µ and ξ_ _e_ 2µ . So if we can fix ρ and
_≥_

_µ as some constants, ξ is a bounded constant. Notice in (66) and (67), for l_ 3, _, L_, if we
_∈{_ _· · ·_ _}_
can make _λ[¯]l and λl close to each other, then ρ and µ are also close, so_ 2[3]µ[ρ] [is not large. This is]

equivalent to the first relation in (24) in main text. In order to satisfy the above conditions, one way
is to construct �Wl[0]�Ll=3 [in such way that][ λ][l][ = ¯][λ][l][ =][ ζ >][ 1][, where][ ζ][ can be chosen to be any large]
number such that (23) and the second relation in (24) are satisfied. Specifically, we can utilize the
following construction: Initialize Wl[0] [such that its top block is a scaled identity matrix and rest of]
entries are zero


_Wl[0]_ [=] � _ζ ·0 Inl_


�
_∈_ R[n][l][×][n][l][−][1] _, l = 3, . . ., L._ (70)


We also need to upper bound Φ(θ[0]). This can be done by choosing small W2[0][. Randomly initialize]
_W2[0]_ [such that] �W2[0]�ij

_[∼N]_ [(0][, κ][)][. We can set][ κ][ to be arbitrarily small, similar to (10) in Nguyen]


-----

and Mondelli (2020),we can find a bound for Φ(θ[0]) with high probability:

�

2N Φ(θ[0]) = ∥FL(θ[0]) − _y∥F_ (71)

_≤∥y∥2 +_ ��FL �θ[0][���]F


_≤∥y∥2 +_


_L_
� _σmax(Wl[0][)][∥][X][∥][F]_

_l=1_


_≤_ 2∥y∥2 (72)


Then the loss function at initialization can be bounded by constant �


2N Φ(θ[0]) ≤ 2∥y∥2.


**Initialization for FedAvg-GD: The initialized weight matrices satisfy the following conditions:**

2N �� 32 �L−1 + 2L−1(r − 1)� _∥X∥F_ _λ¯1→L_ � 12 _[λ][l][, l][ ∈{][3][,][ · · ·][, L][}]_ (73)

_rγ[2(][L][−][2)][ �]_ [1]2 �2(L−1)2 23→L[α]0[2] _·_ _λ¯l_ _≤_ 1, l ∈{1, 2}

2N �� 23 �L−1 + 2L−1(r − 1)� _∥X∥F[2]_

_rγ[2(][L][−][2)][ �]_ 2[1] �2(L−1)2 λ23→L[α][2]0 _·_ _λ[¯]2→L ≤_ [1]2 _[α][0][.]_ (74)


The initialization strategy is similar to FedAvg-SGD, so we omit the discussion here.

B.3 PROOF OF THEOREM 1

**Theorem 1. Using FedAvg-SGD to minimize (3) with Algorithm 1. Suppose Assumptions 1, 2 and**
_3 are satisfied, then there exists an initialization strategy such that for any ϵ < Φ(θ[0]), there exists_
_step-size η > 0 such that we have_

_E[Φ(θ[¯][r][(][t][+1)])]_ (1 _µ[′]η)[t]_ Φ(θ[0]), t 0, . . ., T 1 (75)
_≤_ _−_ _∈{_ _−_ _}_

_where µ[′]_ = _N[r]_ _[γ][2(][L][−][2)][ �]_ 2[1] �2(L−1) λ23→L[α]0[2][.]


_Proof. First, we provide a structure of our proof. We will show the following recursively at each_
communication round: 1) The averaged weights are bounded at each communication round; 2)
The divergence of loss function (3) is bounded at each communication round; 3) The expected loss
function (3) decreases linearly at each communication round. Further, we will show that in each local
epoch within a fixed communication round, we have: 1) The weights of each client are bounded; 2)
The divergence of loss function Φk of each client is bounded.

� log(Φ(θ0)/ϵ) �
Now let us set T = log( 1−µCη1 [) + 1] . If we can show (75) holds for t = 0, . . ., T, then it is easy

to show that

_E[Φ(θ[¯][rT]_ )] (1 _µCη)[T]_ Φ(θ[0]) _ϵ._
_≤_ _−_ _≤_

We prove Theorem 1 by induction. Define

_ρ[rt][+][v]_ := sup _ρ(θk[rt][+][q])_
_k∈[K]_
_q∈{0,1,...,v}_


_ρ :=_


_Lm∥X∥F 7[L][−][1 ¯]min[λ][1][→]λ[L]¯l_

_l∈[L]_

�
_Nγ[L][−][2]λ3→L min_ _α0, mink∈[K]_ _[α][0][,k]_


_,_
�


(76)


-----

We show that _t_ _T_, we have
_∀_ _≤_
σσσminmaxmin (���FWW¯¯1[ru] ru ru[)]��[ ⩾]� ⩾≤[1]2[1]2[α]2[3][λ][λ][¯][0][l][, u][l][, u][ u][ ∈{][ ∈{][ ∈{][0][0][0][, . . ., t][, . . ., t][, . . ., t][}][}][}][, l][,][, l][ ∈][ ∈{][[][L][3][]][,][, . . ., L][}][,]

_σmin_ _F1[ru],k_ _≥_ [1]2 _[α][0][,k][, u][ ∈{][0][, . . ., t][}][, k][ ∈]_ [[][K][]][,]

EρΦ[rt]��θΦ¯≤ru�ρ,θ�¯ru⩽��(1 + 3(1ρCηµCη)[u] Φ)[u]�Φθ[0]�[�]θ, u[0][�] _, u ∈{0, . . ., t0, . . ., t}_

_≤_ _−_ _∈{_ _}_


_._ (77)


where _λ[¯]l is defined in (22) and λl is the smallest eigen value of the weight matrix, C, µ, ρ defined in_
B.2 and µC = µ[′].

The above recursive equation describes the weight matrix and loss function in each communication
round. To prove (77), we decompose the recursive equation into two steps, as follows
Step1: For a fixed t and v [r 1], given
_∈_ _−_
σmax �W¯lru[�] ⩽ 2[3] _[λ][¯][l][, u][ ∈{][0][, . . ., t][}][, l][ ∈]_ [[][L][]][,]
σρσΦE[rt]minmin��θΦ¯≤ru (��FWρθ�¯¯;1ru[ru]≤lru��[)](1 + 3[ ⩾][�]≤⩾[1]2(1[α][1]2 _ρCη[λ][0] −[, u][l][, u]µCη[ ∈{])[ ∈{][u]Φ)[0][u]�[0][, . . ., t]θΦ[, . . ., t][0]�[�]θ, u[0][�][}] ∈{[}], u[,]_ _[, l] ∈{[ ∈]0, . . ., t[[][L]0, . . ., t[]][,]_ _}_ _}_

_._ (78)

Φσσσρ[rt]maxminmink[+]�[v]θ��[−]�k[rt]WFW[1][+]1[rt][q]l,k,k[rt]l,k[rt]�[+][+][+]ρ,[q]≤[q]�[q]��(1 + 3≥≤≤[1]4[1]4[7]4[α][λ][λ][¯][0]ρC[l][l][,k][, q][, q][, q][′]η[ ∈{][ ∈{])[ ∈{][q]Φ[0][0]k[, . . ., v][0][, . . ., v] ([, . . ., v]θk[rt][)][, q][ −][ −][ −][ ∈{][1][1][}][1][}][, l][}][, l][0][, k][ ∈][, . . ., v][ ∈][ ∈][[][[][L][L][[][]][K][]][, k][, k][ −][]][,][ ∈][ ∈][1][}][[][, k][[][K][K][]][ ∈][]][,][,] [[][K][]][,]

_≤_


we aim to show
σσmaxmin ��WWl,k[rt]l,k[rt][+][+][q][q]�� _≥≤_ 4[1]4[7][λ][λ][¯][l][l][, q][, q][ ∈{][ ∈{][0][0][, . . ., v][, . . ., v][}][}][, l][, l][ ∈][ ∈] [[][[][L][L][]][]][, k][, k][ ∈][ ∈] [[][[][K][K][]][]][,][,]

� �
_σmin_ _F1[rt],k[+][q]_ _≥_ [1]4 _[α][0][,k][, q][ ∈{][0][, . . ., v][}][, k][ ∈]_ [[][K][]]

ρΦ[rt]k[+]�[v]θk[rt]≤[+]ρ,[q]� _≤_ (1 + 3ρC _[′]η)[q]Φk (θk[rt][)][, q][ ∈{][0][,][ 1][, . . ., v][}][,]_ _k ∈_ [K].


_,_ (79)


where C _[′]_ = maxk � _N1k_ � 47 �2(L−1) minλ¯l[2]1∈→[LL] _λ[¯][2]l_ �.

Step 2: Given (78) and (79), we show
σσσminminmax (��FWW¯¯1[ru]llruru[)][ ⩾][�][�] ⩾⩽[1]2 _[α][1]22[3][λ][0][λ][¯][, u][l][l][, u][, u][ ∈{][ ∈{][ ∈{][0][0][, . . ., t][0][, . . ., t][, . . ., t][ + 1][ + 1][ + 1][}][}][,][}][, l][, l][ ∈][ ∈]_ [[][[][L][L][]][]][,][,]

_σmin (F1,k) ≥_ [1]2 _[α][0][,k][, u][ ∈{][0][, . . ., t][ + 1][}][, k][ ∈]_ [[][K][]][,]

ρΦE[r]�[(]�[t]θΦ¯[+1)]ru�θ�¯≤ru≤��ρ,(1 + 3(1ρCηµCη)[u]Φ)[u]�θΦ[0]�[�]θ, u[0][�] ∈{, u 0, . . ., t0, . . ., t + 1 + 1}, _._

_≤_ _−_ _∈{_ _}_


_._ (80)


Now we show Step 1 first.
(1) We first show
 � �
σmax _Wl,k[rt][+][q]_ ⩽ 4[7] _[λ][¯][l][, q][ ∈{][0][, . . ., v][}][, l][ ∈]_ [[][L][]][, k][ ∈] [[][K][]][,]

_._ (81)

� �

σmin _Wl,k[rt][+][q]_ ⩾ [1]4 _[λ][l][, q][ ∈{][0][, . . ., v][}][, l][ ∈]_ [[][L][]][, k][ ∈] [[][K][]][.]


-----

We have
���W rtl,k+v _−_ _W[¯]_ _l[rt]���F_

_[≤]_


_v−1_
�q=0 ���W rtl,k+q+1 _−_ _Wl,k[rt][+][q]���F_ _[≤]_ _[η]����_


_v−1_
�

_g˜l,k[rt][+][q]_
_q=0_


����2


_v−1_
�

_q=0_


��gkrt+q��2


(i)
_η_
_≤_


_v−1_
�

_q=0_


��g˜krt+q��2


(ii)
_ηρ_
_≤_


(82)

���f rtL,k+q _−_ _yk���2_ (83)


(iii)
_≤_ _[ηρL]Nk_ _∥Xk∥F (_ 4[7] [)][L][−][1][ ¯]min[λ][1][→]λ¯[L]l

_l∈[L]_

(iv)
_≤_ _[ηρL]Nk_ _∥Xk∥F (_ 4[7] [)][L][−][1][ ¯]min[λ][1][→]λ¯[L]l

_l∈[L]_


_v−1_
�

_q=0_


_v−1_
�(1 + 3ρC _[′]η)[q][ ��]fL,k[rt]_ _[−]_ _[y][k]��2_ _[,]_ (84)

_q=0_


where (i) is because the norm of concentated gradient is no smaller than norm of one-layer gradient;
(ii) results from Lemma (1); (iii) comes from (31) in Lemma 4; (iv) is because of the induction
assumption. Let η < _ρC1_ _[′][, we have]_


_v−1_

���W rtl,k+v _−_ _W[¯]_ _l[rt]���F_ _[≤]_ _[ηρL]Nk_ _∥Xk∥F (_ 4[7] [)][L][−][1][ ¯]lmin[λ]∈[[1]L[→]] _λ¯[L]l_ �q=0(1 + 3ρC _[′]η)[q][ ��]fL,k[rt]_ _[−]_ _[y][k]��2_ (85)

(≤i) _[ηρL]Nk_ _∥Xk∥F_ � 74 �l−1 ¯lminλ∈[1L→] _λ¯Ll_ _v�q=0−1_ 2[v][ ��]fL,k[rt] _[−]_ _[y][k]��2_

(≤ii) _[ηρL]Nk_ _∥Xk∥F_ � 74 �l−1 ¯lminλ∈[1L→] _λ¯Ll_ _v�q=0−1_ 2[v][ ��]fL[rt] _[−]_ _[y]��2_

_≤_ _[ηρL][(2]N[r]k[ −]_ [1)] _∥Xk∥F_ � 74 �L−1 ¯minλ1→λ¯Ll _∥fL[rt]_ _[−]_ _[y][∥][2]_ (86)

_l∈[L]_

_≤_ _[ηρL][(2]N[r]k[ −]_ [1)] _∥Xk∥F_ � 74 �L−1 ¯minλ1→λ¯Ll _∥fL[0]_ _[−]_ _[y][∥][2][(1 + 3][ρCη][)]_ _T2_

_l∈[L]_

_≤_ _[ηρL][(2]N[r]k[ −]_ [1)] _∥Xk∥F_ � 74 �L−1 ¯minλ1→λ¯Ll _∥fL[0]_ _[−]_ _[y][∥][2]_ _[·][ ξN]_ [Φ]ϵ�θ[0][�]

_l∈[L]_


� 1

4 _[λ][l][, l][ ∈{][3][, . . ., L][}][,]_

_≤_ 1 _._

6 _[, l][ ∈{][1][,][ 2][}]_

where (i) uses η < _ρC1_ _[′][ ; (ii) is because][ ∥][f][ rt]L,k_ _L_

_[−]_ _[y][k][∥][2][ ≤∥][f][ rt]_ _[−]_ _[y][∥][2][; the last inequality holds if we]_
choose small enough η. To be more specific, we can choose

� �
min min 14 _[λ][l][,][ 1]6_
_η <_ _ρL(2N[r]k−1)_ _∥Xk∥F_ � 74 �L−1l∈min[L]λ¯l1∈→[LL] _λ[¯]l_ _[∥][f][ 0]L_ _[−]_ _[y][∥]2_ _[·][ ξ][Φ(]ϵ[θ][0][)]_ (87)

By Weyl’s inequality, we have
 � � � �
σσminmax �WWl,k[rt]l,k[rt][+][+][v][v][+1][+1]� ⩾≤ _σσminmax_ �WWl,k[rt]l,k[rt] �−+4[1]4[1][λ][λ][¯][l][l][ ⩽][=][ 1]44[7][λ][λ][¯][l][l][, l][, l][ ∈{][ ∈{][3][3][, . . ., L][, . . ., L][}][}][, k][, k][ ∈][ ∈][[][K][[][K][]][,][]][,]

_._ (88)

� �

σσmaxmax �WW12[rt][rt],k,k� _≤≤_ 66[1][1] [+ 1 +][+ 1 +][ ∥][ ∥][W][W][ rt][ rt]12,k,k[∥][∥][2][2][ ≤][ ≤] [7][7]44 _[λ][λ][¯][¯][l][l][, k][, k][ ∈][ ∈]_ [[][[][K][K][]][]][,][.]


(2) We next show that


� �
_σmin_ _F1[rt],k[+][q]_ ⩾ 4[1] _[α][0][,k][, q][ ∈{][0][, . . ., v][}][, k][ ∈]_ [[][K][]][.] (89)


-----

� �
It is sufficient to show σmin _F1[rt],k[+][v]_ ⩾ [1]4 _[α][0][,k][,][ k][ ∈]_ [[][K][]][.]

���F rt1,k+v _−_ _F1[rt],k���F_ [=] ���σ �XkW1[rt],k[+][v]� _−_ _σ_ �XkW[¯] 1[rt],k)����F (90)

(i)
_≤_ _σmax(Xk)∥W1[rt],k[+][v]_ _−_ _W1[rt],k[∥][F]_ (91)

(≤ii) _σmax(Xk)_ _[ηρ][(2]N[r][ −]k_ [1)] _∥Xk∥F_ � 74 �L−1 ¯minλ1→λ¯Ll _∥fL[0]_ _[−]_ _[y][∥][2]_ _[·][ ξ][Φ]_ �ϵθ[0][�] (92)

_l∈[L]_

where (i) results from the Lipschitz gradient of σ in Assumption 3 and (ii) comes from (86).
If we choose small enough η, which satisfies


_η <_


1
4 _[α][0][,k][N][k]_

(93)

_σmax(Xk)ρ(2[r]_ _−_ 1) ∥Xk∥F � 74 �L−1 _lminλ∈¯1[L→]_ _λL¯l_ _[∥][f][ 0]L,k_ _[−]_ _[y][∥][2][ ·][ ξ][Φ(]ϵ[θ][0][)]_


then we have
���F rt1,k+v _−_ _F1[rt],k���F_ _[≤]_ 4[1] _[α][0][,k]_ (94)

(3) Next, we show that
_ρ[rt][+][v]_ _ρ._ (95)
_≤_

Since we have already shown in (81) that σmax(Wl,k[rt][+][v]) ≤ 4[7] _[λ][¯][l][,][ σ][min][(][W][ rt]l,k[+][v]) ≥_ 4[1] _[λ][¯][l][ and we have]_

shown in (89) that σmin(F1[rt][+][v]) ≥ 4[1] _[α][0][,k][. By lemma 1, we have]_

( 4[7] [)][L][−][1][LN] _[∥][X][∥][F]_ minλ¯1→λL¯l

_ρ(θk[rt][+][v]) ≤_ ( [1]4 [)][L][−][1][mγ][L][−][2][λ][3][→][L] _l[min]∈[L]_ _≤_ _ρ._ (96)

_k∈[K]_ _[α][0][,k]_

(4) Next, we prove


Φk �θk[rt][+][q]� ⩽ (1 + 3ρC _[′]η)[q]Φk_ �θk[rt]� _, q ∈{0, . . ., v},_ _k ∈_ [K]. (97)
We show
Φk �θk[rt][+][v]� ⩽ (1 + 3ρC _[′]η)[v]Φk_ �θk[rt]� _._ (98)
First, we need to show Φk has Lipschitz gradient within [θ[rt][+][v][−][1], θ[rt][+][v]]. This is similar to the proof
of (50) in Lemma 8. So we don’t include the details here. It is easy to show that, for θk[rt][+][v][−][1][,s] :=

� � � � ��
_θk[rt][+][v][−][1]_ + s(θk[rt][+][v] _−_ _θk[rt][+][v][−][1]), there is max_ _σmax_ _Wl,k[rt][+][v][−][1][,s]_ _, σmax_ _Wl,k[rt][+][v][−][1]_ _≤_ [7]4 _[λ][¯][l][. So]_

similarly we can derive the Lipschitz constant


_√_

_L_

_Qk =_ _[L]_

_Nk_


� 7

4


�2(L−1) _∥Xk∥F[2]_ minλ¯[2]1→λ¯L[2]l + _[L]N√kL_ _∥Xk∥F (1 + Lβ ∥Xk∥F R[′]) R[′][ ��]fL,k[0]_ _[−]_ _[y][k]��2_ _[,]_

_l∈[L]_

(99)


such that _s_ [0, 1],
_∀_ _∈_
���gkrt+v−1,s _−_ _gk[rt][+][v][−][1]���2_ _k_ _−_ _θk[rt][+][v][−][1]∥2._ (100)

_[≤]_ _[Q][k][∥][θ][rt][+][v][−][1][,s]_

With Lipschitz gradient within [θk[rt][+][v][−][1], θk[rt][+][v]], by Lemma 5, we have


Φk �θk[rt][+][v]� _≤_ Φk �θk[rt][+][v][−][1]� + ⟨∇Φk �θk[rt][+][v][−][1]� _, θk[rt][+][v]_ _−_ _θk[rt][+][v]�_ + _[Q][k]_

2


��θkrt+v−1 _−_ _θk[rt][+][v][−][1]��22_
(101)


= Φk �θk[rt][+][v][−][1]� + �gk[rt][+][v][−][1], −ηg˜k[rt][+][v][−][1]� + _[Q]2[k]_ ��ηg˜krt+v−1��22 (102)

_≤_ Φk �θk[rt][+][v][−][1]� + η ��gkrt+v−1��2 ��g˜krt+v−1��2 [+][ Q]2[k] _[η][2][ ��][g][˜]k[rt][+][v][−][1]��22_ (103)

_≤_ Φk �θk[rt][+][v][−][1]� + ηρ ��gkrt+v−1��22 [+][ Q]2[k] _[η][2][ρ][2][ ��][g]k[rt][+][v][−][1]��22_ (104)


-----

Let η < _Q1kρ_ [, we have the above inequality]

Φk �θk[rt][+][v]� _≤_ Φk �θk[rt][+][v][−][1]� + ηρ ��gkrt+v−1��22 [+][ Q]2[k] _[η][2][ρ][2][ ��][g]k[rt][+][v][−][1]��22_ (105)

_≤_ Φk �θk[rt][+][v][−][1]� + 2[3] _[ρη]_ ��gkrt+v−1��22 (106)

_≤_ Φk �θk[rt][+][v]� + [3]N[ρηL]k � 74 �2(L−1) ¯minλ21→λ¯L[2]l Φk �θk[rt][+][v][−][1]� _,_ (107)

_l∈[L]_


where the third inequality comes from (31) in Lemma 4. Recall C _[′]_ := maxk [(][ 1]Nk � 74 �2(L−1) _lmin∈λ¯[[2]1L→]_ _λ¯L[2]l_ [)][,]

we have

Φk �θk[rt][+][v][+1]� _≤_ Φk �θN[rt][+][v]� (1 + 3ρC _[′]η)._ (108)


Now Step 1 is proved. Next we show Step 2.
(1) Show
�σmax �W¯ _rul_ � ⩽ 2[3] _[λ][¯][l][, u][ ∈{][0][,][ 1][, . . . t][ + 1][}][, l][ ∈]_ [[][L][]] _._ (109)

_σmin_ �W¯ _rul_ � ⩾ 2[1] _[λ][¯][l]_ _u ∈{0, 1, . . . t + 1}_ _l ∈{3, . . ., L}_

Define _∇[˜]_ _Wl,kΦk(θk[rt][)][ be the stochastic gradient over layer][ l][ of each client. Denote][ ¯˜][g][l,rt][+][v][ :=]_
_K_
� _NNk_ _[∇][˜]_ _[W][l][,k][Φ][k][(][θ]k[rt][+][v]) We have_

_k=1_


��g¯˜l,ru + ¯˜gl,ru+1 + . . . + ¯˜gl,rt+r−1��2 (110)

_r−1_
�

_∥g˜[¯][l,ru][+][v]∥2_ (111)
_v=0_

_r−1_
�

_∥g˜[¯][ru][+][v]∥2_ (112)
_v=0_


���W¯ _rl_ (t+1) _−_ _Wl[0]���F_ [=][ η]

_η_
_≤_

_η_
_≤_


_t_
�

_u=0_

_t_
�

_u=0_

_t_
�

_u=0_


By Step 1, we know for v 0, 1, . . ., r 1, we have ρ[rt][+][v] _ρ. So by definition of ρ[rt][+][v], we have_
_∈{_ _−_ _}_ _≤_
where∥g˜[ru][+][v]Λ[¯]∥l2 = ≤[7]4ρ[λ][¯]∥[l]g[,][ Q][ru][k][+][ is defined in (63) and][v]∥2. Then it is easy to verify that the assumptions in Lemma 8 are satisfied,[ A][ = 1+3][ρC] _[′][η][. Then by Lemma 8, if][ η <]_ _ρC1_ _[′][, we have]_


_t_
�

_u=0_


�L−1 (2[r] _−_ 1) minλ¯1→λ¯Ll _∥fL[rt]_ _[−]_ _[y][∥][2]_ (113)

_l∈[L]_


� 7

4


_η_


_t_ _r−1_
� �

_∥g˜[¯][ru][+][v]∥2 ≤_ _η_

_u=0_ _v=0_


_ρL∥X∥F_

_N_


Using the definition of P = _[L][∥][X]N[∥][F]_ � 74 �L−1 (2r − 1), we have


_t_
�

_u=0_


�L−1 (2[r] _−_ 1) minλ¯1→λ¯Ll _∥fL[rt]_ _[−]_ _[y][∥][2]_ (114)

_l∈[L]_


� 7

4


_η_


_t_ _r−1_
� �

_∥g˜[¯][ru][+][v]∥2 ≤_ _η_

_u=0_ _v=0_


_ρL∥X∥F_

_N_


_ηρP_ _λ¯1→L_
_≤_ min _λ¯l_

_l∈[L]_

_ηρP_ _λ¯1→L_
_≤_ min _λ¯l_

_l∈[L]_


_t_
� _∥fL[rt]_ _[−]_ _[y][∥][2]_ (115)

_u=0_

_t_
�(1 + 3ρCη) _u2 ∥fL[0]_ _[−]_ _[y][∥][2][,]_ (116)

_u=0_

(117)


-----

where the last inequality comes from the induction assumption. Now let S = 1 + 3ρCη, if we

_[√]_
choose η < _ρC1_ [, we get]


�vr−=01 _∥g˜[¯][ru][+][v]∥2 ≤_ _ηρP_ _lminλ∈¯[1L→]_ _λ¯Ll_

= ηρP _λ¯1→L_

min _λ¯l_
_l∈[L]_

_ηρP_ _λ¯1→L_
_≤_ min _λ¯l_

_l∈[L]_

= ηρP _λ¯1→L_

min _λ¯l_
_l∈[L]_


_t_
�(1 + 3ρCη) _u2 ∥fL[0]_ _[−]_ _[y][∥][2]_

_u=0_

_t_
� _S[u]∥fL[0]_ _[−]_ _[y][∥][2]_

_u=0_


_η_


_t_
�

_u=0_


_S[T][ +1]_

_S[2]_ 1 [(][S][ + 1)][∥][f][ 0]L _[−]_ _[y][∥][2]_
_−_

_S[T][ +1]_ 3

_·_

3ρCη _L_ _[−]_ _[y][∥][2]_ (118)

_[∥][f][ 0]_


_ξΦ(θ[0])_
By Lemma 6, we have S[T] _≤_ _ϵ_ . Additionally, S ≤ 2, therefore, we have


�r−1 _λ¯1→L_ 2S[T] _· 3_

_v=0_ _∥g˜[¯][ru][+][v]∥2 ≤_ _ηρP_ _lmin∈[L]_ _λ¯l_ 3ρCη _[∥][f][ 0]L_ _[−]_ _[y][∥][2]_ (119)

_≤_ _C[P]_ minλ¯1→λ¯Ll _·_ [2][ξ][Φ]ϵ�θ[0][�] _∥fL[0]_ _[−]_ _[y][∥][2]_

_l∈[L]_

2 �θ[0][�]
= _L∥X∥F_ � 32 �L−1 minλ¯1→L _·_ _[ξ][Φ]_ _ϵ_ _∥fL[0]_ _[−]_ _[y][∥][2]_ (120)

_l∈[L]_ _[λ][l]_

� 1
2 _[λ][l][, l][ ∈{][3][, . . ., L][}][,]_ _,_
_≤_ 1, l 1, 2 _._
_∈{_ _}_


_η_


_t_
�

_u=0_


where the last inequality is from (68). So by Weyl’s ineuality, we have
σσminmax��WW¯¯l[r]l[r][(][(][t][t][+1)][+1)]�� ⩾≤ _σσminmax��W¯W¯ rtl rtl_ ��−+2[1]2[1][λ][λ][¯][l][l][ ⩽][=][ 1]22[3][λ][λ][¯][l][l][, l][, l][ ∈{][ ∈{][3][3][, . . ., L][, . . ., L][}][}][, k][, k][ ∈][ ∈][[][K][[][K][]][,][]][,]

σσmaxmax ��WW¯¯ _rt rt21_ �� _≤≤_ 1 +1 + σ σmaxmax( �WW[¯]¯2[rt] rt1,k�[)][ ≤]≤ [3]22[3][λ][¯][λ][¯][l][l][, k][, k][ ∈][ ∈] [[][[][K][K][]][]][.][,]

(2) Show


_._ (121)


_σmin (F1[ru][)][ ⩾]_ [1] _l ∈_ [L]. (122)

2 _[α][0][, u][ ∈{][0][, . . ., t][ + 1][}]_


Similarly, we have
���F r1 (t+1) _−_ _F1[0]���F_ [=] ���σ �XW[¯] 1[r][(][t][+1)]� _−_ _σ_ �XW1[0]�[�]��F (123)

(i)
_≤_ _σmax(X)_ ���W¯ _r1_ (t+1) _−_ _W1[0]���F_ (124)

(ii) 2 �θ[0][�]
_≤_ _σmax(X)_ _L∥X∥F_ � 32 �L−1 minλ¯1→λL¯l _·_ _[ξ][Φ]_ _ϵ_ _∥fL[0]_ _[−]_ _[y][∥][2]_ (125)

_l∈[L]_

2 �θ[0][�]
_≤∥X∥F_ _L∥X∥F_ � 32 �L−1 minλ¯1→λL¯l _·_ _[ξ][Φ]_ _ϵ_ _∥fL[0]_ _[−]_ _[y][∥][2]_ (126)

_l∈[L]_

(iii)

(127)

_≤_ [1]

2 _[α][0][,]_


-----

where (i) is because σ is 1 Lipschitz; (ii) comes from (120), and (iii) is because (69) in B.2. So
_−_
similarly by Weyl’s inequality, we have σmin �F1[r][(][t][+1)]� _≥_ _σmin_ �F1[0]� = α0 − 2[1] _[α][0]_ [=][ 1]2 _[α][0][.]_

(3) Show

_σmin_ �F1[ru],k� ⩾ [1] _l ∈_ [L]. (128)

2 _[α][0][,k][, u][ ∈{][0][, . . ., t][ + 1][}]_


Similarly, we have
���F r1,k(t+1) _−_ _F1[0],k���F_ [=] ���σ �XkW[¯] 1[r][(][t][+1)]� _−_ _σ_ �XkW1[0]�[�]��F (129)

(i)
_≤_ _σmax(Xk)_ ���W¯ _r1_ (t+1) _−_ _W1[0]���F_ (130)

(ii) 2 �θ[0][�]
_≤_ _σmax(Xk)_ _L∥X∥F_ � 32 �L−1 minλ¯1→λL¯l _·_ _[ξ][Φ]_ _ϵ_ _∥fL[0]_ _[−]_ _[y][∥][2]_ (131)

_l∈[L]_

2 �θ[0][�]
_≤∥Xk∥F_ _L∥X∥F_ � 32 �L−1 minλ¯1→λL¯l _·_ _[ξ][Φ]_ _ϵ_ _∥fL[0]_ _[−]_ _[y][∥][2]_ (132)

_l∈[L]_

(iii)

(133)

_≤_ [1]

2 _[α][0][,k][,]_


where (i) is because σ is 1 Lipschitz; (ii) comes from (120), and (iii) is because (69) in B.2. So
_−_
� � � �
similarly by Weyl’s inequality, we have σmin _F1[r],k[(][t][+1)]_ _≥_ _σmin_ _F1[0],k_ = α0,k − 2[1] _[α][0][,k]_ [=][ 1]2 _[α][0][,k][.]_

(3) Show

_ρ[r][(][t][+1)]_ _ρ._ (134)
_≤_


Since we have already shown in (81) that σmax �Wl,k[r][(][t][+1)]� _≤_ 2[3] _[λ][¯][l][,][ σ][min]_ �Wl,k[r][(][t][+1)]� _≥_ [1]2 _[λ][¯][l][, and we]_

have shown in (89) that σmin(F1[r][(][t][+1)]) ≥ 2[1] _[α][0][. By Lemma 1, we have]_


_ρ[r][(][t][+1)]_
_≤_


( [3]2 [)][L][−][1][LN] minλ¯1→λL¯l

_l∈[L]_ _< ρ._ (135)

( 2[1] [)][L][−][1][mγ][L][−][2][λ][3][→][L] [min]

_k∈[K]_ _[α][0]_


(4) Show
Φ �θ¯ru� ⩽ (1 + 3ρCη)[u]Φ �θ[0][�] _, u ∈{0, . . ., t + 1}, l ∈_ [L].

First, similar to the proof of (50), we can derive


_√_

_L_ � 3

_Q =_ _[L]_

_·_
_N_ 2


�2(L−1) _∥X∥F[2]_ minλ¯[2]1→λ¯L[2]l + _[L]N√L_ _∥X∥F (1 + Lβ∥X∥F R) R∥fL[0]_ _[−]_ _[y][∥][2][,]_ (136)

_l∈[L]_


_L_
where R = � max �1, [3]2 _[λ][¯][p]�, such that ∀θ[¯][rt,s]_ = θ[¯][rt] + s(θ[¯][r][(][t][+1)] _−_ _θ[¯][rt]), s ∈_ [0, 1], we have

_p=1_

���gr(t+1),s − _gr(t+1)���2_ ���θ¯r(t+1),s − _θ¯rt���2_ _[.]_ (137)

_[≤]_ _[Q]_

Then by Lemma 5 we have

Φ �θ¯[r][(][t][+1)][�] = Φ �θ¯rt − _ηg¯˜rt −_ _. . . −_ _ηg¯˜rt+r−1�_


Φ �θ¯rt� _η_ �g[rt], _g[¯]˜[rt]_ + . . . + g[¯]˜[rt][+][r][−][1][�] + _[Q]_ _rt+r−1[�]2_ (138)
_≤_ _−_ 2 _[η][2][ ���][g][¯˜][rt][ +][ . . .][ + ˜][g]_ ��2

_≤_ Φ �θ¯rt� + η ��grt��2 ��g¯˜rt + . . . + ¯˜grt+r−1��2 [+][ Q]2 _[η][2][ ��][g][¯˜][rt][ +][ . . .][ + ¯˜][g][rt][+][r][−][1][��]2[2]_


-----

By (49) in Lemma 8, if η < _ρC1_ [, we have][ A][ = 2][, and we have]


�L−1 (2[r] _−_ 1) minλ¯1→λ¯Ll _∥fL[rt]_ _[−]_ _[y][∥][2]_ [=][ ρP] minλ¯1→λ¯Ll _∥fL[rt]_ _[−]_ _[y][∥][2][.]_

_l∈[L]_ _l∈[L]_

(139)


��g¯˜rt + . . . + ¯˜grt+r−1��2 _[≤]_ _[ρL][∥]N[X][∥][F]_

Then we have


� 7

4


Φ �θ¯[r][(][t][+1)][�] _≤_ Φ �θ¯rt� + η ��grt��2 ��g¯˜rt + . . . + ¯˜grt+r−1��2 [+][ Q]2 _[η][2][ ��][g][¯˜][rt][ +][ . . .][ + ¯˜][g][rt][+][r][−][1][��]2[2]_

_≤_ Φ �θ¯rt� + η ��grt��2 _[·][ ρP]_ minλ¯1→λ¯Ll _∥fL[rt]_ _[−]_ _[y][∥][2]_ [+][ Q]2 _[η][2][ρ][2][P][ 2][ ¯]min[λ]1[2]→λ¯L[2]l_ _∥fL[rt]_ _[−]_ _[y][∥]2[2]_

_l∈[L]_ _l∈[L]_


_≤_ Φ �θ¯rt� + η ��grt��2 _[·][ ρP]_ minλ¯1→λ¯Ll _∥fL[rt]_ _[−]_ _[y][∥][2]_ [+][ Q]2 _[η][2][ρ][2][P][ 2][ ¯]min[λ]1[2]→λ¯L[2]l_ _∥fL[rt]_ _[−]_ _[y][∥]2[2]_

_l∈[L]_ _l∈[L]_


(140)

(141)

(142)


(i)
Φ �θ¯rt� + _[ηρPL][∥][X][∥][F]_
_≤_

_N_


� 32 �L−1 ¯minλ21→λ¯L[2]l _∥fL[rt]_ _[−]_ _[y][∥]2[2]_ [+][ Q]2 _[η][2][ρ][2][P][ 2][ ¯]min[λ]1[2]→λ¯L[2]l_ _∥fL[rt]_ _[−]_ _[y][∥]2[2][,]_

_l∈[L]_ _l∈[L]_

(143)


where (i) comes from (31) in Lemma 4. Let η < _[L][∥][X]QρP N[∥][F][ (][ 3]2_ [)][L][−][1] = _Qρ(_ [6]7(2[)][L][r][−]−[1]1) [, we get]


Φ �θ¯[r][(][t][+1)][�] Φ �θ¯rt� + _[ηρPL][∥][X][∥][F]_
_≤_

_N_


� 32 �L−1 ¯minλ21→λ¯L[2]l _∥fL[rt]_ _[−]_ _[y][∥]2[2]_ [+][ Q]2 _[η][2][ρ][2][P][ 2][ ¯]min[λ]1[2]→λ¯L[2]l_ _∥fL[rt]_ _[−]_ _[y][∥]2[2]_

_l∈[L]_ _l∈[L]_

(144)


_≤_ Φ �θ¯rt� + 2[3] _[·][ ηρPL]N[∥][X][∥][F]_ � 32 �L−1 ¯minλ21→λ¯L[2]l _∥fL[rt]_ _[−]_ _[y][∥]2[2]_

_l∈[L]_

Φ �θ¯rt� 1 + 3 _[ηρPL][∥][X][∥][F]_ � 3 �L−1 ¯λ21→L 
_≤_  _N_ 2 min _λ¯[2]l_ 

_l∈[L]_

Recall C = PL∥X∥F � 23 �L−1 _lmin∈λ¯[[2]1L→]_ _λ¯L[2]l_ [, then we have]


(5) Show

By (138), we have


Φ �θ¯[r][(][t][+1)][�] (1 + 3ρCη) Φ �θ¯rt� _._ (145)
_≤_

_E�Φ(θ[¯][r][(][t][+1)])�_ (1 _µCη)[u]Φ_ �θ[0][�] _, u_ 0, 1, . . ., t + 1
_≤_ _−_ _∈{_ _}_


Φ(θ[¯][r][(][t][+1)]) Φ �θ¯rt� _η_ �g[rt], _g[¯]˜[rt]_ + . . . + g[˜]˜[rt][+][r][−][1][�] + _[Q]_ _rt+r−1[�]2_ (146)
_≤_ _−_ 2 _[η][2][ ���][g][¯˜][rt][ +][ . . .][ + ˜][g]_ ��2


-----

Given _θ[¯][rt], take expectation of the stochastic gradient on both sides conditioned on_ _θ[¯][rt]_ and the past,
we get

_E[Φ(θ[¯][r][(][t][+1)])] ≤_ _E�Φ_ �θ¯rt� _−_ _η_ �g[rt], ¯g[rt] + . . . + ¯g[rt][+][r][−][1][�] + _[Q]2_ _[η][2][ρ][2][P][ 2][ ¯]min[λ]1[2]→λ¯L[2]l_ _∥fL[rt]_ _[−]_ _[y][∥]2[2]�_

_l∈[L]_


_E�Φ_ �θ¯rt� _η_ �g[rt], rg¯[rt][�] _η_ _g[rt],_
_≤_ _−_ _−_ _⟨_


_r−1_
�v=1 _g¯[rt][+][v]_ _−_ _g¯[rt]⟩_ + _[Q]2_ _[η][2][ρ][2][P][ 2][ ¯]lmin∈[λ][1[2]L→]_ _λ¯L[2]l_ _∥fL[rt]_ _[−]_ _[y][∥]2[2]�_


_≤_ _E�Φ_ �θ¯rt� _−_ _ηr∥g[rt]∥2[2]_ [+][ η][∥][g][rt][∥][2] _[× ∥]_


_r−1_
�v=1 _g¯[rt][+][v]_ _−_ _g¯[rt]∥2 +_ _[Q]2_ _[η][2][ρ][2][P][ 2][ ¯]lmin∈[λ][1[2]L→]_ _λ¯L[2]l_ _∥fL[rt]_ _[−]_ _[y][∥]2[2]�_


(147)

Now it is easy to verify the assumptions in Lemma 1 are satisfied. Let Λ[¯] _l =_ 4[7] _[λ][¯][l][,][ Q][ defined in (136),]_

_A = 1 + 3ρC_ _[′]η_ 2, by (51) in Lemma 8:, we have
_≤_
������vr−=11 _g¯[rt][+][v]_ _−_ _g¯[rt]�����2_ _≤_ _[ηρL][(2]N[r][ −]_ [1)] � 74 �L−1 ¯lminλ∈[1L→] _λ¯Ll_ ����k�K=1 _Q[2]k_ _[∥][X][k][∥]F[2]_ _[∥][f][ rt]L_ _[−]_ _[y][∥][2]_ (148)


Plug (148) into (147), we get:

�
_E�Φ(θ[¯][r][(][t][+1)])�_ _≤_ _E_ Φ �θ¯rt� _−_ _ηr∥g[rt]∥2[2]_ [+][ η][∥][g][rt][∥][2] _[× ∥]_


� _r−1_
_≤_ _E_ Φ �θ¯rt� _−_ _ηr∥g[rt]∥2[2]_ [+][ η][∥][g][rt][∥][2] _[× ∥]_ �v=1 _g¯[rt][+][v]_ _−_ _g¯[rt]∥2 +_ _[Q]2_ _[η][2][ρ][2][P][ 2][ ¯]lmin∈[λ][1[2]L→]_ _λ¯L[2]l_ _∥fL[rt]_ _[−]_ _[y][∥]2[2]_

_≤_ _E�Φ_ �θ¯rt� _−_ _ηr∥g[rt]∥2[2]_ [+][ η][∥][g][rt][∥][2] _[×][ ηρL][(2][r][ −]N[1)][∥][X][∥][F]_ � 74 �L−1 �����K _Q[2]k_ _[∥][X][k][∥]F[2]_

_k=1_


�


_×_ minλ¯1→λ¯Ll _∥fL[rt]_ _[−]_ _[y][∥][2]_ [+][ Q]2 _[η][2][ρ][2][P][ 2][ ¯]min[λ]1[2]→λ¯L[2]l_ _∥fL[rt]_ _[−]_ _[y][∥]2[2]�_

_l∈[L]_ _l∈[L]_


(i) �
_≤_ _E_ Φ �θ¯rt� _−_ _ηr∥g[rt]∥2[2]_ [+][ η][2][L][2][ρ][(2][r][ −] [1)][∥][X][∥][F] _× (_ [21]

_N_ [2] 8 [)][L][−][1]


�� _K_
��
� _Q[2]k_ _[∥][X][k][∥]F[2]_

_k=1_


_×_ minλ¯[2]1→λ¯L[2]l _∥fL[rt]_ _[−]_ _[y][∥]2[2]_ [+][ Q]2 _[ρ][2][η][2][P][ 2][ ¯]min[λ]1[2]→λ¯L[2]l_ _∥fL[rt]_ _[−]_ _[y][∥]2[2]�_

_l∈[L]_ _l∈[L]_


(≤ii) _E�Φ_ �θ¯rt� _−_ _η N[r][2][ γ][2(][L][−][2)]_ � 12 �2(L−1) _λ[2]3→L[α][2]0_

� �� �
:=µ[′]


_∥fL[rt]_ _[−]_ _[y][∥]2[2][+]_


� _K_

_η[2]_ � _L[2]ρ(2[r]_ _−_ 1)∥X∥F ( [21]8 [)][L][−][1] _k�=1_ _Q[2]k_ _[∥][X][k][∥]F[2]_ + _[Q]_ � _λ¯21→L_

_N_ [2] 2 _[ρ][2][P][ 2]_ min _λ¯[2]l_

_l∈[L]_
� �� �
:=B


_∥fL[rt]_ _[−]_ _[y][∥]2[2]_


�
_,_


where (i) uses (31) to provide an upperbound for _g[rt]_, (ii) uses (28) to provide a lower bound for
_∥_ _∥_
_∥g[rt]∥. Let η <_ 2[µ]B[′] [, we have]

_E[Φ(θ[¯][r][(][t][+1)])] ≤_ _E�Φ_ �θ¯rt� _−_ _ηµ[′]∥fL[rt]_ _[−]_ _[y][∥]2[2]_ [+][ η][2][B][∥][f][ rt]L _[−]_ _[y][∥]2[2]�_ (149)

_E�Φ_ �θ¯rt� (1 _ηµ[′])_ � (150)
_≤_ _−_

= E[Φ �θ¯rt�] �1 − _η N [r]_ _[γ][2(][L][−][2)]_ � 21 �2(L−1) _λ[2]3→L[α][2]0�_ (151)


-----

Let µ =


_Nr_ _[γ][2(][L][−][2)][(][ 1]2_ [)]C2(L−1)λ23→L[α][2]0, we have


_E�Φ(θ[¯][r][(][t][+1)])�_ (1 _µCη)E[Φ_ �θ¯rt�], (152)
_≤_ _−_

where µC = µ[′].

_Now we summarize the choice of η, it should be smaller than all the following quantities:_


_ρC1_ _[′][,]_ _ρ max(1_ _Qk)_ _[,][ 1]ρC [,][ 1]µC [,]_ _ρL(2N[r]k−1)_ _∥Xk∥Fmin�_ 74 ��Lmin−1 minl∈[λ¯Ll1∈]→[L14L][λ]λ[¯][l]l[,][ 1][∥]6[f]�[ 0]L _[−]_ _[y][∥]2_ _[·][ ξ][Φ(]ϵ[θ][0][)]_ _,_

1
4 _[α][0][,k][N][k]_

_._ (153)

_σmax (Xk) ρ(2[r]_ _−_ 1) ∥Xk∥F � 74 �L−1 minλ¯l1∈→[LL] _λ[¯]l_ ���f 0L,k _[−]_ _[y]���2_ _ϵ_

_[·][ ξ][Φ(][θ][0][)]_

**Remark 8. To satisfy the initialization assumptions defined in (69) and (68), we initialize the neural**
_network coefficients such that we have_ _λ[¯]1→L ∼O(1/ϵ). Note from the definition of µ[′]_ _in (152) that_
_this implies µ[′]_ _∼O(λ[2]3→L[) =][ O][(¯][λ][2]1→L[) =][ O][(1][/ϵ][2][)][. Also, note from the choice of step-size in]_
(153) that, we have η ∼O(ϵ × 1/λ[¯]1→L) = O(ϵ[2]). Note that this follows from the fact that η is
_smaller than each quantity defined in (153) above. Thus, we have µ[′]η =_ (1) and we can always
_O_
_choose η = c/µ[′]_ _for some c_ (0, 1), which guarantees linear convergence of the objective in each
_∈_
_communication round (see Theorem 1)._

### C EXPERIMENT SETTING AND RESULT

C.1 MODEL AND PARAMETER SETTINGS:

_To analyze the performance of FedAvg-SGD on the MNIST data set, we use a single hidden-layer_
_fully-connected neural network (MLP) with ReLU activation. We set the hidden-layer size to be_
32 (resp. 1, 000) for the small (resp. large) network. We choose the mini-batch size m = 10 and
_choose the number of local steps to be r = 10._ _Using the above network, we also compare the_
_random initialization with the special initialization strategy in (23),(24) with MNIST and Fashion_
_MNIST dataset. For the CIFAR-10 data set, we analyze the performance of FedAvg-SGD on two_
_network architectures – convolutional neural network (CNN) and ResNet. We design the smaller_
_CNN using two 5_ 5 convolutional layers followed by 2 2 max pooling, each has 6 and 16
_×_ _×_
_channels, connected by 2 fully-connected layers with 120 and 84 hidden neurons. For larger CNN,_
_we use three 3_ 3 convolutional layers each with 128 channels followed by 2 2 max pooling.
_×_ _×_
_The ReLU activation function is used after each hidden layer for small/large CNN. For ResNet, we_
_compare the performance on ResNet18 with ResNet50 architectures. For both the CNN and ResNet,_
_we use a mini-batch size of m = 32 and number of local steps to be r = 5. We randomly sample 10_
_clients in each epoch and perform FedAvg-SGD for more efficient training._

C.2 EXPERIMENT RESULT FOR MNIST

Figure 4: MNIST with MLP: Comparison of FedAvg on large and small size MLP.


-----

Figure 5: Log-scale loss.


-----


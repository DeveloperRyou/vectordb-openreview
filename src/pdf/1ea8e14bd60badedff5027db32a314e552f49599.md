# COMPUTATIONAL DOOB’S h-TRANSFORMS FOR ON## LINE FILTERING OF DISCRETELY OBSERVED DIFFU- SIONS

**Anonymous authors**
Paper under double-blind review

### ABSTRACT

This paper is concerned with online filtering of discretely observed nonlinear diffusion processes. Our approach is based on the fully adapted auxiliary particle
filter, which involves Doob’s h-transforms that are typically intractable. We propose a computational framework to approximate these h-transforms by solving
the underlying backward Kolmogorov equations using nonlinear Feynman-Kac
formulas and neural networks. The methodology allows one to train a locally
optimal particle filter prior to the data-assimilation procedure. Numerical experiments illustrate that the proposed approach can be orders of magnitude more
efficient than state-of-the-art particle filters in the regime of highly informative
observations, when the observations are extreme under the model, and if the state
dimension is large.

### 1 INTRODUCTION

Diffusion processes are fundamental tools in applied mathematics, statistics, and machine learning. Because this flexible class of models is easily amenable to computations and simulations,
diffusion processes are very common in biological sciences (e.g. population and multi-species models, stochastic delay population systems), neuroscience (e.g. models for synaptic input, stochastic
Hodgkin–Huxley model, stochastic Fitzhugh–Nagumo model), and finance (e.g. modeling multi
assets prices) (Allen, 2010; Shreve et al., 2004; Capasso & Capasso, 2021). In these disciplines,
tracking signal from partial or noisy observations is a very common task. However, working with
diffusion processes can be challenging as their transition densities are only tractable in rare and
simple situations such as (geometric) Brownian motions or Ornstein–Uhlenbeck (OU) processes.
This difficulty has hindered the use of standard methodologies for inference and data-assimilation
of models driven by diffusion processes and various approaches have been developed to circumvent
or mitigate some of these issues, as discussed in Section 4.

Consider a time-homogeneous multivariate diffusion process dXt = µ(Xt) dt + σ(Xt) dBt that is
discretely observed at regular intervals. Noisy observations yk of the latent process Xtk are collected
at equispaced times tk ≡ _k T for k ≥_ 1. We consider the online filtering problem which consists in
estimating the conditional laws πk(dx) = P(Xtk ∈ _dx|y1, . . ., yk), i.e. the filtering distributions,_
as observations are collected. We focus on the use of Particle Filters (PFs) that approximate the filtering distributions with a system of weighted particles. Although many previous works have relied
on the Bootstrap Particle Filter (BPF), which simulates particles from the diffusion process, it can
perform poorly in challenging scenarios as it fails to take the incoming observation yk into account.
The goal of this article is to show that the (locally) optimal approach given by the Fully Adapted
Auxiliary Particle Filter (FA-APF) (Pitt & Shephard, 1999) can be implemented. This necessitates
simulating a conditioned diffusion process, which can be formulated as a control problem involving
an intractable Doob’s h-transform (Rogers & Williams, 2000; Chung & Walsh, 2006). We propose
the Computational Doob’s h-Transform (CDT) framework for efficiently approximating these quantities. The method relies on nonlinear Feynman-Kac formulas for solving backward Kolmogorov
equations simultaneously for all possible observations. Importantly, this preprocessing step only
needs to be performed once before starting the online filtering procedure. Numerical experiments
illustrate that the proposed approach can be orders of magnitude more efficient than the BPF in the
regime of highly informative observations, when the observations are extreme under the model, and


-----

if the state dimension is large. A PyTorch implementation to reproduce our numerical experiments
[is available at https://anonymous.4open.science/r/CompDoobTransform/.](https://anonymous.4open.science/r/CompDoobTransform/)

**Notations.** For two matrices A, B ∈ R[d,d], their Frobenius inner product is defined as ⟨A, B⟩F =
�d
_i,j=1_ _[A][i,j][B][i,j][. The Euclidean inner product for][ u][,][ v][ ∈]_ [R][d][ is denoted as][ ⟨][u][,][ v][⟩] [=][ �]i[d]=1 _[u][i][v][i][.]_
For two (or more) functions F and G, we sometimes use the shortened notation [FG](x) to denote
the product F (x)G(x).

### 2 BACKGROUND

2.1 FILTERING OF DISCRETELY OBSERVED DIFFUSIONS

Consider a homogeneous diffusion process {Xt}t≥0 in X = R[d] with initial distribution ρ0(dx) and
dynamics

_dXt = µ(Xt) dt + σ(Xt) dBt,_ (1)

described by the drift and volatility functions µ : R[d] _→_ R[d] and σ : R[d] _→_ R[d,d]. The associated
semi-group of transition probabilities ps(dx� | x) satisfies P(Xt+s ∈ _A | Xt = x) =_ �A _[p][s][(][d][x][�][ |][ x][)]_

for any s, t > 0 and measurable A ⊂X . The process {Bt}t≥0 is a standard R[d]-valued Brownian
motion. The diffusion process {Xt}t≥0 is discretely observed at time tk = kT, for k ≥ 1, for
some inter-observation time T > 0. The Y-valued observation Yk ∈Y at time tk is modelled
by the likelihood function g : X × Y → R+ in the sense that for any measurable A ⊂Y, we
have P(Yk ∈ _A | Xtk = xk) =_ �A _[g][(][x][k][,][ y][)][ d][y][ for some dominating measure][ d][y][ on][ Y][. For]_

a test function φ : X → R, the generator of the diffusion process {Xt}t≥0 is given by Lφ =
_⟨µ, ∇φ⟩_ + [1]2 _[⟨][σσ][⊤][,][ ∇][2][φ][⟩][F][.][ This article is concerned with approximating the filtering distributions]_

_πk(dx) = P(Xtk ∈_ _dx | y1, . . ., yk). For notational convenience, we set π0(dx) ≡_ _ρ0(dx) since_
there is no observation collected at the initial time t = 0.

2.2 PARTICLE FILTERING


Particle Filters (PF), also known as Sequential Monte Carlo methods, are a set of Monte Carlo
algorithms that can be used to solve filtering problems (see Chopin et al. (2020) for a recent textbook on the topic). PFs evolve a set of M ≥ 1 particles x[1:]t _[M]_ = (x[1]t _[, . . .,][ x][M]t_ [)][ ∈X][ M][ forward]
in time using a combination of propagation and resampling operations. To initialize the PF, each
initial particle x[j]0 _[∈X][ for][ 1][ ≤]_ _[j][ ≤]_ _[M][ is sampled independently from the distribution][ ρ][0][(][d][x][)][ so]_
that π0(dx) ≈ _M_ _[−][1][ �][M]j=1_ _[δ][(][d][x][;][ x]0[j]_ [)][. Approximations of the filtering distribution][ π][k][ for][ k][ ≥] [1]
are built recursively as follows. Given the Monte Carlo approximation of the filtering distribution at time tk, πk(dx) ≈ _M_ _[−][1][ �][M]j=1_ _[δ][(][d][x][;][ x]t[j]k_ [)][, the particles][ x]t[1:]k[M] are propagated independently
forward in time by **x[j]tk+1** _tk_ [)][, using a Markov kernel][ q][k][+1][(][d][x][�][ |][ x][)][ specified by]
� _[∼]_ _[q][k][+1][(][d][x][�][ |][ x][j]_
the user. The BPF corresponds to the Markov kernel qk[BPF]+1[(][d][x][�][ |][ x][) =][ P][(][X][t]k+1 _k_ [=]

_[∈]_ _[d][x][�][ |][ X][t]_
**x), while the FA-APF (Pitt & Shephard, 1999) corresponds to the (typically intractable) kernel**
_qk[FA-APF]+1_ (dx� | x) = P(Xtk+1 ∈ _dx� | Xtk = x, Yk+1 = yk+1). Each particle �x[j]tk+1_ [is associated]

_j_
with a normalized weight W _k+1_ [=][ W][ j]k+1[/][ �]i[M]=1 _[W][ i]k+1[, where the unnormalized weights][ W][ j]k+1_
(by time-homogeneity of (1)) are defined as

_pT (dx[j]tk+1_ _tk_ [)]
_Wk[j]+1_ [=] � _[|][ x][j]_ _g(x�[j]tk+1_ _[,][ y][k][+1][)][.]_ (2)

_qk+1(dx[j]tk+1_ _tk_ [)]
� _[|][ x][j]_

The BPF and FA-APF correspond respectively to having

_Wk[j,]+1[BPF]_ = g(x�[j]tk+1 _[,][ y][k][+1][)]_ and _Wk[j,]+1[FA-APF]_ = E[g(Xtk+1 _, yk+1) | Xtk = x[j]tk_ []][.] (3)

_j_
The weights are such that πk+1(dx) ≈ [�]j[M]=1 _[W]_ _k+1_ _[δ][(][d][x][;][ x]t[j]k+1_ [)][. The][ resampling][ step consists]

_i_
in defining a new set of particles x[1:]tk[M]+1 [with][ P][(][x]t[j]k+1 [=][ �][x]t[i]k+1 [) =][ W] _k+1[. This resampling scheme]_
ensures that the equally weighted set of particles x[1:]tk[M]+1 [provides a Monte Carlo approximation of the]


-----

filtering distribution at time tk+1 in the sense that πk+1(dx) ≈ _M_ _[−][1][ �][M]j=1_ _[δ][(][d][x][;][ x]t[j]k+1_ [)][. Note that]
the particles x[1:]tk[M]+1 [do not need to be resampled independently given the set of propagated particles]
**x�[1:]tk[M]+1** [. We refer the reader to Gerber et al. (2019) for a recent discussion of resampling schemes]
within PFs and to Del Moral (2004) for a book-length treatment of the convergence properties of
this class of Monte Carlo methods.

In most settings, the FA-APF (Pitt & Shephard, 1999) that minimizes a local variance criterion
(Doucet et al., 2009) generates particles that are more consistent with informative data and weights
that exhibit significantly less variability compared to the BPF. This gain in efficiency can be very
substantial when the signal-to-noise ratio is high or when observations contain outliers under the
model specification. Nevertheless, implementing FA-APF requires sampling from the transition
probability qk[FA-APF]+1 (dx� | x), which is typically not feasible in practice. We will show in the following
that this can be achieved in our setting by simulating a conditioned diffusion.

2.3 CONDITIONED AND CONTROLLED DIFFUSIONS

As the diffusion process (1) is assumed to be time-homogeneous, it suffices to focus on the initial
interval [0, T ] and study the dynamics of the diffusion X[0,T ] = {Xt}t∈[0,T ] conditioned upon the
first observation YT = y. It is a standard result that the conditioned diffusion is described by
diffusion process with the same volatility as the original diffusion but with a time-dependent drift
function that takes the future observation YT = y into account.

Before deriving the exact form of the conditioned diffusion, the notion of controlled diffusion needs
to be discussed. For an arbitrary control function c : X × Y × [0, T ] → R[d] and y ∈Y, consider the
controlled diffusion {X[c]t _[,][y]}t∈[0,T ] with generator L[c][,][y][,t]φ(x) = Lφ(x) + ⟨[σc](x, y, t), ∇φ(x)⟩_
and dynamics


_dX[c]t_ _[,][y]_ = µ(X[c]t _[,][y]) dt + σ(X[c]t_ _[,][y]) dBt_
� �� �
(original dynamics)


+ [σ c](X[c]t _[,][y], y, t) dt_
� �� �
(control drift term)


_._ (4)


If P[0,T ] and P[c][0[,],T[y] ] [denote the probability measures on the space of continuous functions]
C([0, T ], R[d]) generated by the original and controlled diffusions, Girsanov’s theorem shows that


_−_ [1]

2


�


_dP[0,T ]_
_dP[c][0[,],T[y]_ ] (X[0,T ]) = exp


�


� _T_ � _T_

_∥c(Xt, y, t)∥[2]_ _dt −_ _⟨c(Xt, y, t), dBt⟩_
0 0


_._ (5)


We now describe the optimal control function c⋆ : X × Y × [0, T ] → R[d] that is such that, for
any observation value y ∈Y, the controlled diffusion X[c][0[⋆],T[,][y] ] [has the same dynamics as the original]
diffusion X[0,T ] conditioned upon the observation YT = y. For this purpose, consider the function

�
_h(x, y, t) = E[g(XT, y) | Xt = x] =_ _g(xT, y) pT −t(dxT | x)_ (6)

_X_

that gives the probability of observing YT = y when the diffusion has state x ∈X at time t ∈ [0, T ].
Recall that the likelihood function g : X ×Y → R+ was defined in Section 2.1. Equation (6) implies
that h : X × Y × [0, T ] → R+ satisfies the backward Kolmogorov equation (Oksendal, 2013),

(∂t + L)h = 0, (7)

with terminal condition h(x, y, T ) = g(x, y) for all (x, y) . As described in Appendix
_∈X × Y_
A.1, the theory of Doob’s h-transformed shows that the optimal control is given by

**c⋆(x, y, t) = [σ[⊤]∇** log h](x, y, t). (8)

We refer readers to Rogers & Williams (2000) for a formal treatment of Doob’s h-transform.

### 3 METHOD

3.1 NONLINEAR FEYNMAN-KAC FORMULA

Obtaining the control function c⋆(x, y, t) = [σ[⊤]∇ log h](x, y, t) by solving the backward Kolmogorov equation in (7) for each observation y is computationally not feasible when filtering
_∈Y_


-----

many observations. Furthermore, when the dimensionality of the state-space becomes larger,
_X_
standard numerical methods for solving Partial Differential Equations (PDEs) such as Finite Differences or the Finite Element Method become impractical. For these reasons, we propose instead to
approximate the control function c⋆ with neural networks, and employ methods based on automatic
differentiation and the nonlinear Feynman-Kac approach to solve semilinear PDEs (Hartmann et al.,
2017; 2019; Kebiri et al., 2017; E et al., 2017; Chan-Wai-Nam et al., 2019; Hutzenthaler & Kruse,
2020; Hutzenthaler et al., 2020; Beck et al., 2019; Han et al., 2018; N¨usken & Richter, 2021).

As the non-negative function h typically decays exponentially for large **x**, it is computation_∥_ _∥_
ally more stable to work on the logarithmic scale and approximate the value function v(x, y, t) =
log[h(x, y, t)]. Using the fact that h satisfies the PDE (7), the value function satisfies
_−_

(∂t + L)v = [1] _v(x, y, T_ ) = − log[g(x, y)] for all (x, y) ∈X × Y. (9)

2

_[∥][σ][⊤][∇][v][∥][2][,]_

Let {X[c]t _[,][y]}t∈[0,T ] be a controlled diffusion defined in Equation (4) for a given control function c :_
_X × Y × [0, T_ ] → R[d] and define the diffusion process {Vt}t∈[0,T ] as Vt = v(X[c]t _[,][y], y, t). While any_
control function c(x, y, t) with mild growth and regularity assumptions can be considered within our
framework, we will see that iterative schemes that choose it as a current approximation of c⋆(x, y, t)
tend to perform better in practice. Since we have that ∂tv + Lv + ⟨σc, ∇v⟩ = (1/2) ∥σ[⊤]∇v∥[2] +
_⟨c, σ[⊤]∇v⟩, Itˆo’s Lemma shows that for any observation YT = y and 0 ≤_ _s ≤_ _T_, we have

� _T_ � 1 � � _T_
_VT = Vs +_ _dt +_ _⟨Zt, dBt⟩_

_s_ 2 _[∥][Z][t][∥][2][ +][ ⟨][c][,][ Z][t][⟩]_ _s_

with Zt = [σ[⊤]∇v](X[c]t _[,][y], y, t) and VT = −_ log[g(X[c]T[,][y][,][ y][)]][. For notational simplicity, we sup-]
pressed the dependence of (Vt, Zt) on the control c and observation y. In summary, the pair of
processes (Vt, Zt) defined as Vt = v(X[c]t _[,][y], y, t) and Zt = [σ[⊤]∇v](X[c]t_ _[,][y], y, t) are such that the_
following equation holds,

� _T_ � 1 � � _T_
_−_ log[g(X[c]T[,][y][,][ y][)] =][ V][s][ +] _dt +_ _⟨Zt, dBt⟩._ (10)

_s_ 2 _[∥][Z][t][∥][2][ +][ ⟨][c][,][ Z][t][⟩]_ _s_

Crucially, under mild growth and regularity assumptions on the drift and volatility function µ :
_X →_ R[d] and σ : X → R[d,d], the pair of processes (Vt, Zt) is the unique solution to Equation (10)
(Pardoux & Peng, 1990; 1992; Pardoux & Tang, 1999; Yong & Zhou, 1999). This result can be used
as a building block for designing Monte Carlo approximations of the solution to semilinear and fully
nonlinear PDEs (E et al., 2017; Han et al., 2018; Raissi, 2018; Beck et al., 2019; Hur´e et al., 2020;
Pham et al., 2021).

3.2 COMPUTATIONAL DOOB’S h-TRANSFORM

As before, consider a diffusion {X[c]t _[,][y]}t∈[0,T ] controlled by a function c : X ×Y ×_ [0, T ] → R[d] and
driven by the standard Brownian motion {Bt}t≥0. Furthermore, for two functions N0 : X ×Y → R
and N : X × Y × [0, T ] → R[d], consider the diffusion process {Vt}t∈[0,T ] defined as

� _s_ � 1 � � _s_
_Vs = V0 +_ _t_ _, y, t), Zt⟩_ _dt +_ _⟨Zt, dBt⟩,_ (11)

0 2 _[∥][Z][t][∥][2][ +][ ⟨][c][(][X][c][,][y]_ 0

where the initial condition V0 and the process {Zt}t∈[0,T ] are defined as

_V0 = N0(X[c]0[,][y][,][ y][)]_ and **Zt = N** (X[c]t _[,][y], y, t)._ (12)

Importantly, we remind the reader that the two diffusion processes X[c]t _[,][y]_ and Vt are driven by the
same Brownian motion Bt. The uniqueness result mentioned at the end of Section 3.1 implies that,
if for any choice of initial condition X[c]0[,][y] _∈X and terminal observation y ∈Y the condition_
_VT = −_ log[g(X[c]T[,][y][,][ y][)]][ is satisfied, then we have that for all][ (][x][,][ y][, t][)][ ∈X × Y ×][ [0][, T] []]

_N0(x, y) = −_ log h(x, y, 0) and _N_ (x, y, t) = −[σ[⊤]∇ log h](x, y, t). (13)
In particular, the optimal control is given by c⋆(x, y, t) = −N (x, y, t). These remarks suggest
parametrizing the functions N0(·, ·) and N (·, ·, ·) by two neural networks with respective parameters
_θ0 ∈_ Θ0 and θ ∈ Θ while minimizing the loss function

�� �2[�]
_L(θ0, θ; c) = E_ _VT + log[g(X[c]T[,][Y], Y)]_ _._ (14)


-----

The above expectation is with respect to the Brownian motion {Bt}t≥0, the initial condition X[c]0[,][Y] _∼_
_ηX(dx) of the controlled diffusion, and the observation Y ∼_ _ηY(dy) at time T_ . In (14), we fix the
dynamics of X[c]t _[,][y]_ and optimize over the dynamics of Vt. The spread of the distributions ηX and
_ηY should be large enough to cover typical states under the filtering distributions πk, k ≥_ 1 and
future observations to be filtered respectively. Specific choices will be detailed for each application
in Section 5. For offline problems, one could learn in a data-driven manner by selecting ηY as the
empirical distribution of actual observations. We stress that these choices only impact training of
the neural networks, and will not affect the asymptotic guarantees of our filtering approximations.

**CDT algorithm.** The following outlines our training procedure to learn neural networks N0 and N
that satisfy (13). To minimize the loss function (14), any stochastic gradient algorithm can be used
with a user-specified mini-batch size of J 1. The following steps are iterated until convergence.
_≥_

1. Choose a control c : X × Y × [0, T ] → R[d], possibly based on the current neural network
parameters (θ0, θ) ∈ Θ0 × Θ.

2. Simulate independent Brownian paths B[j][0,T ][, initial conditions][ X]0[j]

_[∼]_ _[η][X][(][d][x][)][, and obser-]_
vations Y[j] _∼_ _ηY(dy) for 1 ≤_ _j ≤_ _J._

3. Generate the controlled trajectories: the j-th sample path X[j][0,T ] [is obtained by forward]

integration of the controlled dynamics in Equation (4) with initial condition X[j]0[, control]
**c(·, Y[j], ·), and the Brownian path B[j][0,T ][.]**

4. Generate the value trajectories: the j-th sample path V[0[j] _,T ]_ [is obtained by forward integra-]

tion of the dynamics in Equation (11)–(12) with the Brownian path B[j][0,T ] [and the current]
neural network parameters (θ0, θ) ∈ Θ0 × Θ.

5. Construct a Monte Carlo estimate of the loss function (14):


_J_
_L� = J_ _[−][1]_ �(VT[j] [+ log[][g][(][X]T[j] _[,][ Y][j][)])][2]_ (15)

_j=1_

6. Use automatic differentiation to compute ∂θ0 _L[�] and ∂θL[�] and update the parameters (θ0, θ)._

Importantly, if the control function c in Step:1 does depend on the current parameters (θ0, θ), the
gradient operations executed in Step:6 should not be propagated through the control function c. A
standard stop-gradient operation available in most popular automatic differentiation frameworks can be used for this purpose.

**Time-discretization of diffusions.** For clarity of exposition, we have described our algorithm
in continuous-time. In practice, one would have to discretize these diffusion processes, which is
entirely straightforward. Although any numerical integrator could potentially be considered, the experiments in Section 5 employed the standard Euler–Maruyama scheme (Kloeden & Platen, 1992).

**Parametrizations of functions N0 and N** **.** In all numerical experiments presented in Section
5, the functions N0 and N are parametrized with fully-connected neural networks with two hidden layers, number of neurons that grow linearly with dimension d, and the Leaky ReLU activation function except in the last layer. Future work could explore other neural network architectures for our setting. In situations that are close to a Gaussian setting (e.g. Ornstein–
Uhlenbeck process observed with additive Gaussian noise) where the value function has the form
_v(x, y, t) =_ **x, a(y, t)x** + _b(y, t), x_ + c(y, t), a more parsimonious parametrization could cer_⟨_ _⟩_ _⟨_ _⟩_
tainly be exploited. Furthermore, the function N (x, y, t) could be parametrized to automatically
satisfy the terminal condition N (x, y, T ) = [σ[⊤] log g](x, y). A possible approach consists
_−_ _∇_
in setting N (x, y, t) = (1 _t/T_ )N (x, y, t) (t/T )[σ[⊤] log g](x, y) for some neural network
_−_ [�] _−_ _∇_
_N� : X × Y × [0, T_ ] → R[d]. These strategies have not be used in the experiments of Section 5.

**Choice of controlled dynamics.** In challenging scenarios where observations are highly informative and/or extreme under the model, choosing a good control function to implement Step:1 of the
proposed algorithm can be crucial. We focus on two possible implementations:


-----

    - CDT static scheme: a simple (and naive) choice is not using any control, i.e. c(x, y, t) ≡
0 ∈ R[d] for all (x, y, t) ∈X × Y × [0, T ].

    - CDT iterative scheme: use the current approximation of the optimal control c⋆ described
by the parameters (θ0, θ) ∈ Θ0 × Θ. This corresponds to setting c(x, y, t) = −N (x, y, t).

While using a static control approach can perform reasonably well in some situations, our results in
Section 5 suggest that the iterative control procedure is a more reliable strategy. This is consistent
with findings in the stochastic optimal control literature (Thijssen & Kappen, 2015; Pereira et al.,
2019). This choice of control function drives the forward process X[c]t _[,][y]_ to regions of the statespace where the likelihood function is large and helps mitigate convergence and stability issues.
Furthermore, Section 5 reports that (at convergence), the solutions N0 and N can be significantly
different. The iterative control procedure leads to more accurate solutions and, ultimately, better
performance when used for online filtering.

3.3 ONLINE FILTERING

Before performing online filtering, we first run the CDT algorithm described in Section 3.2 to construct an approximation of the optimal control c⋆(x, y, t) = [σ[⊤]∇ log h](x, y, t). For concreteness, denote by �c : X × Y × [0, T ] → R[d] the resulting approximate control, i.e. �c(x, y, t) =
_N_ (x, y, t) where N ( _,_ _,_ ) is parametrized by the final parameter θ Θ. Similarly, denote by
_−_ _·_ _·_ _·_ _∈_
_V�0 : X × Y →_ R the approximation of the initial value function v(x, y, 0) = − log h(x, y, 0), i.e.
_V�0(x, y) = N0(x, y) where N0(·, ·) is parametrized by the final parameter θ0 ∈_ Θ0.

For implementing online filtering with M ≥ 1 particles, consider a current approximation πk(dx) =
_M_ _[−][1][ �][M]j=1_ _[δ][(][d][x][;][ x]t[j]k_ [)][ of the filtering distribution at time][ t][k] _[≥]_ [0][. Given the future observation]
**Yk+1 = yk+1, the particles x[1:]tk[M]** are then propagated forward by exploiting the approximately
optimal control (x, t) �→ �c(x, yk+1, t − _tk). In particular, �x[j]tk+1_ [is obtained by setting][ �][x]t[j]k+1 [=]
**X�** _[j]tk+1_ [where][ {][X][ �] _t[j][}]t∈[tk,tk+1]_ [follows the controlled diffusion]

_dX[�]_ _[j]t_ [=][ µ][(][ �][X]t[j][)][ dt][ +][ σ][(][ �][X][j]t [)][ d][B][j]t + [σ�c](X[�] _[j]t_ _[,][ y][k][+1][, t][ −]_ _[t][k][)][ dt]_ (16)
� �� � � �� �
(original dynamics) (approximately optimal control)

initialized at **X[�]** _[j]tk_ = x[j]tk [.] Each propagated particle �x[j]tk+1 [is associated with a normalized]

_j_
weight W _k+1_ [=][ W][ j]k+1[/][ �]i[M]=1 _[W][ i]k+1_ [where][ W][ j]k+1 [= (][d][P][[][t]k[,t]k+1[]][/d][P][�][c][t[,]k[y],t[k]k[+1]+1][)(][ �][X][j][tk,tk+1][)][ ×]

_g(x�[j]tk+1_ _[,][ y][k][+1][)][. We recall that the probability measures][ P][tk,tk+1]_ [and][ P][�][c][t[,]k[y],t[k]k[+1]+1] [correspond to the]
original and controlled diffusions on the interval [tk, tk+1]. Girsanov’s theorem, as described in
Equation (5), implies that


�
_Wk[j]+1_ [= exp] _−_ [1]

2


� _tk+1_ � _tk+1_ �

_∥Z[j]t_ _[∥][2][ dt][ +]_ _⟨Z[j]t_ _[, d][B][j]t_ _[⟩]_ [+ log][ g][(][x][j]tk+1 _[,][ y][k][+1][)]_
_tk_ _tk_


where Z[j]t [=][ −][�][c][(][ �][X]t[j][,][ y][k][+1][, t][ −] _[t][k][)][. Similarly to Equation (11), consider the diffusion process]_
_{Vt[j][}]t∈[tk,tk+1]_ [defined by the dynamics][ dV]t[ j] = − 2[1] _[∥][Z]t[j][∥][2][ dt][ +][ ⟨][Z][j]t_ _[, d][B][j]t_ _[⟩]_ [with initialization at]

_Vt[j]k_ [=][ �][V][0][(][x]t[j]k _[,][ y][k][+1][)][. Therefore the weight can be re-written as]_

� � � �
_Wk[j]+1_ [= exp] _Vt[j]k+1_ [+ log][ g][(][x]t[j]k+1 _[,][ y][k][+1][)]_ exp _−V[�]0(x[j]tk_ _[,][ y][k][+1][)]_ _,_ (17)
� _≈��0_ �

and computed by numerically integrating the process {Vt[j][}]t∈[tk,tk+1][. Given the definition of the]
loss function in (14), we can expect the term within the first exponential to be close to zero. In the
ideal case where �c(x, y, t) ≡ **c⋆(x, y, t) and** _V[�]0(x, y) ≡−_ log h(x, y, 0), one recovers the exact
AF-APF weights in (3). Once the unnormalized weights (17) are computed, the resampling steps
are identical to those described in Section 2.2 for a standard PF. For practical implementations, all
the processes involved in the proposed methodology can be straightforwardly time-discretized. To
distinguish between CDT learning with static or iterative control, we shall refer to the resulting
approximation of FA-APF as Static-APF and Iterative-APF respectively. We note that these APFs
do not involve modified resampling probabilities as described e.g. in Chopin et al. (2020, p. 145).


-----

### 4 RELATED WORK

This section positions our work within the existing literature.

**MCMC methods: Several works have developed MCMC methods for smoothing and parameter**
estimation of SDEs; for example, Roberts & Stramer (2001) proposes to treat paths between observations as missing data. Our work concentrates on the online filtering problem: this cannot be
tackled with MCMC methods.

**Exact Simulation: Several methods have been proposed to reduce or eliminate the bias due to**
discretization (Beskos et al., 2006a;b; Fearnhead et al., 2010; 2008); these methods typically rely
on the Lamperti transform that is only rarely available in multivariate settings. Furthermore, when
filtering diffusion with highly-informative observations, the discretization bias is often orders of
magnitude smaller than other sources of errors. We also stress that our method is generic: it does
not exploit any specific structure of the diffusion process being assimilated.

**Gaussian Assumptions: In the data-assimilation literature, methods based on variations of the En-**
semble Kalman Filter (EnKF) Evensen (2003) have been successfully deployed in applied scenarios
and very high-dimensional settings. These methods do rely on strong Gaussian assumptions and are
inappropriate for highly nonlinear and non-Gaussian models. In contrast, our method is asymptotically exact in the limit when the number of particles M (up to discretization error). Indeed,
_→∞_
we do not expect our method to be competitive with this class of (approximate) methods in very
high-dimensional settings that are common in numerical weather forecasting. These methods typically achieve lower variance by increasing the bias. Our method is designed to filter diffusion
processes in low or moderate dimensional settings. It is likely that scaling our method to truly highdimensional settings with effective dimension D 10[2] would require introducing model-specific
_≫_
approximations (e.g. localization strategies).

**Steering particles towards observations: particle methods pioneered by Van Leeuwen (2010) are**
based on this natural principle in order to mitigate collapse of PFs in high-dimensional settings.
These methods typically rely on some model structure (e.g. linear Gaussian observation process)
and have a number of tuning parameters. They can be understood as parametrizing a linear control,
which is only expected to work well for linear Gaussian dynamics, admittedly very important in
applications such as geoscience.

**Implicit Particle Filter: the method of Chorin et al. (2010) attempts to transform standard i.i.d**
Gaussian samples into samples from the optimal proposal density. To implement this methodology
requires a number of assumptions and requires solving a non-convex optimization step for each
particle and each time step. This can quickly become computational burdensome.

**Guided Intermediate Resampling Filters (GIRF): the method of Del Moral & Murray (2015);**
Park & Ionides (2020) propagates particles at intermediate time intervals between observations with
the original dynamics and triggers resampling steps based on a guiding functions that forecast the
likelihood of future observations. The choice of guiding functions is crucial for good algorithmic
performance. We note that GIRF is in fact intimately related to Doob’s h-transform as the optimal
choice of guiding functions is given by (6) (Park & Ionides, 2020). However, even under this optimal
choice, the resulting GIRF is still sub-optimal when compared to an APF that moves particles using
the optimal control induced by Doob’s h-transform, i.e. it is better to move particles well rather than
rely on weighting and resampling. The latter behaviour is supported by our numerical experiments.
Appendix A.5 details our GIRF implementation and the connection to Doob’s h-transform.

### 5 EXPERIMENTS

We performed numerical experiments on three different models: an Ornstein–Uhlenbeck model, a
(nonlinear) Logistic diffusion model and a (nonlinear) diffusion model describing cell differentiation. This section presents experiments on the Ornstein–Uhlenbeck model; the two other studies
can be found in the Appendix A.3 and A.4. All experiments employed 2000 iterations of the Adam
optimizer with a learning rate of 0.01 and a mini-batch size of B = 1000 sample paths with 10
different observations each. Appendix A.6 describes how the CDT algorithm and the approximate
control functions behave during training. Training times took less than two minutes on a standard


-----

|2 4 05|6|8 1|
|---|---|---|
|0 03 01|||
|1 3|||


Figure 1: Results for Ornstein–Uhlenbeck model with d = 1 based on 100 independent repetitions
of each PF. The ELBO gap in the second row is relative to FA-APF.

d = 1 d = 2 d = 4 d = 8 d = 16 d = 32

100 100 100 100 100 100

75 75 75 75 75 75

50 50 50 50 50 50

25 25 25 25 25 25

0 0 0 0 0 0
10[2] 10[3] 10[2] 10[3] 10[2] 10[3] 10[2] 10[3] 10[2] 10[3] 10[2] 10[3]

150

0.750.50 1.0 32 1510 100 1000

0.25 0.5 1 5 50 500

0.00 0.0 0 0 0 0

10[4] 10[2] 10[3] 10[4] 10[2] 10[3] 10[4] 10[2] 10[3] 10[4] 10[2] 10[3] 10[4] 10[2] 10[3] 10[4] 10[2] 10[3]

10[2] 10[2] 10[2] 10[2] 10[2] 10[2]

10[0] 10[0] 10[0] 10[0] 10[0] 10[0]

10 2 10 2 10 2 10 2 10 2 10 2

10[2] 10[3] 10[2] 10[3] 10[2] 10[3] 10[2] 10[3] 10[2] 10[3] 10[2] 10[3]

K K K K K K

BPF Iterative-APF Exact-APF FA-APF GIRF

Figure 2: Results for Ornstein–Uhlenbeck model with σY = 1.0 based on 100 independent repetitions of each PF. The ELBO gap in the second row is relative to FA-APF.

|02 103|Col2|Col3|
|---|---|---|
||||
||||

|Col1|Col2|
|---|---|
|||

|02|103|10|
|---|---|---|
|||10 10 10|
||||


-----

CPU: it is negligible when compared to the cost of running filters with many particles and/or to
assimilate large number of observations. The inter-observation time was T = 1 and we employed
the Euler–Maruyama integrator with a stepsize of 0.02 for all examples. Our results are not sensitive to the choice of T and discretization stepsize as long as it is sufficiently small. We report the
Effective Sample Size (ESS) averaged over observation times and independent repetitions, the evidence lower bound (ELBO) E[log �p(y1, . . ., yK)], and the variance Var[log �p(y1, . . ., yK)], where
_p�(y1, . . ., yK) denotes its unbiased estimator of the marginal likelihood of the time-discretized filter_
_p(y1, . . ., yK). When testing particle filters with varying number of observations K, we increased_
the number of particles M linearly with K to keep marginal likelihood estimators stable (B´erard
et al., 2014). For non-toy models, our GIRF implementation relies on a sub-optimal but practical
choice of guiding functions that gradually introduce information from the future observation by
annealing the observation density using a linear (Linear-GIRF) or quadratic schedule (QuadraticGIRF).

5.1 ORNSTEIN–UHLENBECK MODEL

Consider a d-dimensional Ornstein–Uhlenbeck process given by (1) with µ(x) = −x, σ(x) = Id
and the Gaussian observation model g(x, y) = N (y; x, σY[2] **[I][d][)][. We chose][ η][X][ =][ N]** [(][0][d][,][ I][d][/][2)][ as the]
stationary distribution and ηY = N (0d, (1/2+σY[2] [)][I][d][)][ as the implied distribution of the observation]
when training neural networks with the CDT iterative scheme. We took different values of σY ∈
0.125, 0.25, 0.5, 1.0 to vary the informativeness of observations and d 1, 2, 4, 8, 16, 32 to
_{_ _}_ _∈{_ _}_
illustrate the impact of dimension. Analytical tractability in this example (Appendix A.2) consider
three idealized particle filters, namely an APF with exact networks (Exact-APF), FA-APF, and GIRF
with optimal guiding functions (Appendix A.5). Comparing our proposed Iterative-APF to ExactAPF and FA-APF enables us to distinguish between neural network approximation errors and timediscretization errors. We note that all PFs except the FA-APF involve time-discretization.

Columns 1 to 4 of Figure 1 summarize our numerical findings when filtering simulated observations
from the model with varying σY and fixed d = 1. We see that the performance of BPF deteriorates as
the observations become more informative, which is to be expected. Furthermore, when σY is small,
the impact of our neural network approximation and time-discretization becomes more noticeable.
For the values of σY and the number of observations K considered, Iterative-APF had substantial
gains in efficiency over BPF and typically outperformed GIRF. From Column 5, we note that these
gains over BPF become very large when we filter K = 100 observations simulated with observation
standard deviations that are multiples of σY = 0.25 which was used to run the filters. In particular,
while the ELBO of BPF diverges as we increase the degree of noise in the simulated observations,
the ELBO of Iterative-APF and GIRF remain stable.

Figure 2 shows the impact of increasing dimension d with fixed σY = 1.0 when filtering simulated
observations from the model. Due to the curse of dimensionality (Snyder et al., 2008; 2015), it is
not surprising for the performance of all PFs to degrade with dimension. Although the error of our
neural network approximation becomes more pronounced when d is large, the gain in efficiency of
Iterative-APF relative to BPF is very significant in the higher dimensional regime, and particularly
so when the number of observations K is also large. Iterative-APF also outperformed GIRF in most
settings, with comparable performance when d is large.

### 6 DISCUSSION

This paper introduced the CDT algorithm, a Sequential Monte-Carlo method for online filtering of
diffusion processes evolving in state-spaces of low to moderate dimensions. Contrarily to a number
of existing methods, the CDT approach is general and does not exploit any particular structure of the
diffusion process. Furthermore, numerical simulations suggests that the CDT algorithm is especially
worthwhile when compared to competing approaches (e.g. BPF or GIRF) in higher dimensional
settings or when the observations are highly informative. Ongoing work involves extending the
CDT framework to parameter estimation and experimenting with alternative formulations and/or
parameterizations to accelerate the training procedures.


-----

### REFERENCES

Linda JS Allen. An introduction to stochastic processes with applications to biology. CRC press,
2010.

Christian Beck, Weinan E, and Arnulf Jentzen. Machine learning approximation algorithms for highdimensional fully nonlinear partial differential equations and second-order backward stochastic
differential equations. Journal of Nonlinear Science, 29(4):1563–1619, 2019.

Jean B´erard, Pierre Del Moral, and Arnaud Doucet. A lognormal central limit theorem for particle
approximations of normalizing constants. Electronic Journal of Probability, 19:1–28, 2014.

Alexandros Beskos, Omiros Papaspiliopoulos, and Gareth O Roberts. Retrospective exact simulation of diffusion sample paths with applications. Bernoulli, 12(6):1077–1098, 2006a.

Alexandros Beskos, Omiros Papaspiliopoulos, Gareth O Roberts, and Paul Fearnhead. Exact and
computationally efficient likelihood-based estimation for discretely observed diffusion processes
(with discussion). Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68
(3):333–382, 2006b.

Vincenzo Capasso and Vincenzo Capasso. Introduction to Continuous-Time Stochastic Processes.
Springer, 2021.

Quentin Chan-Wai-Nam, Joseph Mikael, and Xavier Warin. Machine learning for semi linear PDEs.
_Journal of Scientific Computing, 79(3):1667–1712, 2019._

Nicolas Chopin, Omiros Papaspiliopoulos, et al. _An introduction to sequential Monte Carlo._
Springer, 2020.

Alexandre Chorin, Matthias Morzfeld, and Xuemin Tu. Implicit particle filters for data assimilation.
_Communications in Applied Mathematics and Computational Science, 5(2):221–240, 2010._

Kai Lai Chung and John B Walsh. Markov processes, Brownian motion, and time symmetry, volume
249. Springer Science & Business Media, 2006.

Paolo Dai Pra. A stochastic control approach to reciprocal diffusion processes. Applied mathematics
_and Optimization, 23(1):313–329, 1991._

Pierre Del Moral. Feynman-Kac formulae: genealogical and interacting particle systems with ap_plications, volume 88. Springer, 2004._

Pierre Del Moral and Lawrence M Murray. Sequential Monte Carlo with highly informative observations. SIAM/ASA Journal on Uncertainty Quantification, 3(1):969–997, 2015.

Brian Dennis and Robert F Costantino. Analysis of steady-state populations with the gamma abundance model: application to Tribolium. Ecology, 69(4):1200–1213, 1988.

Arnaud Doucet, Adam M Johansen, et al. A tutorial on particle filtering and smoothing: Fifteen
years later. Handbook of nonlinear filtering, 12(656-704):3, 2009.

Weinan E, Jiequn Han, and Arnulf Jentzen. Deep learning-based numerical methods for highdimensional parabolic partial differential equations and backward stochastic differential equations. Communications in Mathematics and Statistics, 5(4):349–380, 2017.

Geir Evensen. The ensemble kalman filter: Theoretical formulation and practical implementation.
_Ocean dynamics, 53(4):343–367, 2003._

Paul Fearnhead, Omiros Papaspiliopoulos, and Gareth O Roberts. Particle filters for partially observed diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70
(4):755–777, 2008.

Paul Fearnhead, Omiros Papaspiliopoulos, Gareth O Roberts, and Andrew Stuart. Random-weight
particle filtering of continuous time processes. Journal of the Royal Statistical Society: Series B
_(Statistical Methodology), 72(4):497–512, 2010._


-----

Mathieu Gerber, Nicolas Chopin, and Nick Whiteley. Negative association, ordering and convergence of resampling methods. The Annals of Statistics, 47(4):2236–2260, 2019.

Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations
using deep learning. _Proceedings of the National Academy of Sciences, 115(34):8505–8510,_
2018.

Carsten Hartmann, Lorenz Richter, Christof Sch¨utte, and Wei Zhang. Variational characterization
of free energy: Theory and algorithms. Entropy, 19(11):626, 2017.

Carsten Hartmann, Omar Kebiri, Lara Neureither, and Lorenz Richter. Variational approach to rare
event simulation using least-squares regression. Chaos: An Interdisciplinary Journal of Nonlinear
_Science, 29(6):063107, 2019._

Cˆome Hur´e, Huyˆen Pham, and Xavier Warin. Deep backward schemes for high-dimensional nonlinear PDEs. Mathematics of Computation, 89(324):1547–1579, 2020.

Martin Hutzenthaler and Thomas Kruse. Multilevel Picard approximations of high-dimensional
semilinear parabolic differential equations with gradient-dependent nonlinearities. SIAM Journal
_on Numerical Analysis, 58(2):929–961, 2020._

Martin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, Tuan Anh Nguyen, and Philippe von Wurstemberger. Overcoming the curse of dimensionality in the numerical approximation of semilinear
parabolic partial differential equations. Proceedings of the Royal Society A, 476(2244):20190630,
2020.

Omar Kebiri, Lara Neureither, and Carsten Hartmann. Adaptive importance sampling with forwardbackward stochastic differential equations. In International workshop on Stochastic Dynamics
_out of Equilibrium, pp. 265–281. Springer, 2017._

Peter E Kloeden and Eckhard Platen. Stochastic differential equations. In Numerical Solution of
_Stochastic Differential Equations, pp. 103–160. Springer, 1992._

Jonas Knape and Perry De Valpine. Fitting complex population models by combining particle filters
with Markov chain Monte Carlo. Ecology, 93(2):256–263, 2012.

Nikolas N¨usken and Lorenz Richter. Solving high-dimensional Hamilton–Jacobi–Bellman PDEs
using neural networks: perspectives from the theory of controlled diffusions and measures on
path space. Partial Differential Equations and Applications, 2(4):1–48, 2021.

Bernt Oksendal. _Stochastic differential equations: an introduction with applications._ Springer
Science & Business Media, 2013.

Etienne Pardoux and Shige Peng. Adapted solution of a backward stochastic differential equation.
_Systems & Control Letters, 14(1):55–61, 1990._

Etienne Pardoux and Shige Peng. Backward stochastic differential equations and quasilinear
parabolic partial differential equations. In Stochastic partial differential equations and their ap_plications, pp. 200–217. Springer, 1992._

Etienne Pardoux and Shanjian Tang. Forward-backward stochastic differential equations and quasilinear parabolic PDEs. Probability Theory and Related Fields, 114(2):123–150, 1999.

Joonha Park and Edward L Ionides. Inference on high-dimensional implicit dynamic models using
a guided intermediate resampling filter. Statistics and Computing, 30(5):1497–1522, 2020.

Marcus Pereira, Ziyi Wang, Ioannis Exarchos, and Evangelos A Theodorou. Learning deep stochastic optimal control policies using forward-backward SDEs. arXiv preprint arXiv:1902.03986,
2019.

Huyen Pham, Xavier Warin, and Maximilien Germain. Neural networks-based backward scheme
for fully nonlinear PDEs. SN Partial Differential Equations and Applications, 2(1):1–24, 2021.

Michael K Pitt and Neil Shephard. Filtering via simulation: Auxiliary particle filters. Journal of the
_American Statistical Association, 94(446):590–599, 1999._


-----

Maziar Raissi. Forward-backward stochastic neural networks: Deep learning of high-dimensional
partial differential equations. arXiv preprint arXiv:1804.07010, 2018.

Gareth O Roberts and Osnat Stramer. On inference for partially observed nonlinear diffusion models
using the Metropolis–Hastings algorithm. Biometrika, 88(3):603–621, 2001.

L Chris G Rogers and David Williams. Diffusions, Markov processes and Martingales: Volume 2:
_Itˆo Calculus, volume 2. Cambridge university press, 2000._

Steven E Shreve et al. _Stochastic calculus for finance II: Continuous-time models, volume 11._
Springer, 2004.

Chris Snyder, Thomas Bengtsson, Peter Bickel, and Jeff Anderson. Obstacles to high-dimensional
particle filtering. Monthly Weather Review, 136(12):4629–4640, 2008.

Chris Snyder, Thomas Bengtsson, and Mathias Morzfeld. Performance bounds for particle filters
using the optimal proposal. Monthly Weather Review, 143(11):4750–4761, 2015.

Sep Thijssen and HJ Kappen. Path integral control and state-dependent feedback. Physical Review
_E, 91(3):032104, 2015._

Peter Jan Van Leeuwen. Nonlinear data assimilation in geosciences: an extremely efficient particle
filter. Quarterly Journal of the Royal Meteorological Society, 136(653):1991–1999, 2010.

Jin Wang, Kun Zhang, Li Xu, and Erkang Wang. Quantifying the Waddington landscape and biological paths for development and differentiation. Proceedings of the National Academy of Sciences,
108(20):8257–8262, 2011.

Jiongmin Yong and Xun Yu Zhou. Stochastic controls: Hamiltonian systems and HJB equations,
volume 43. Springer Science & Business Media, 1999.

### A APPENDIX

A.1 DOOB’S h-TRANSFORM

This section gives an heuristic derivation of the Equation (8) that describes the optimal control. To
simplify notation, we shall denote the conditioned process X[0,T ] (YT = y) as **X[0,T ]. Recall the**
_|_ [�]
function

�
_h(x, y, t) = E[g(XT, y) | Xt = x] =_ _g(xT, y) pT −t(dxT | x)_ (18)

_X_

which gives the probability of observing YT = y when the diffusion process has state x ∈X at
time t ∈ [0, T ]. The definition in (6) implies that the function h : X × Y × [0, T ] → R+ satisfies
the backward Kolmogorov equation (Oksendal, 2013),

(∂t + L)h = 0, (19)

with terminal condition h(x, y, T ) = g(x, y) for all (x, y) ∈X × Y. For φ : X → R and an
infinitesimal increment δ > 0, we have


E[φ(X[�] _t+δ)|X[�]_ _t = x] = E[φ(Xt+δ) g(XT, y) | Xt = x] / E[g(XT, y)|Xt = x]_
= E[φ(Xt+δ) h(Xt+δ, y, t + δ) | Xt = x] / h(x, y, t)

� [φ h] �
_L_
= φ(x) + δ (x, y, t) + O(δ[2]).

_h_


(20)


Furthermore, since the function h satisfies (7), some algebra shows that [φ h]/h = _φ +_
_L_ _L_
_σσ[⊤]_ log h, _φ_ . Taking δ 0, this heuristic derivation shows that the generator of the condi_⟨_ _∇_ _∇_ _⟩_ _→_
tioned diffusion equals Lφ+⟨σσ[⊤]∇ log h, ∇φ⟩. Hence **X[�]** [0,T ] satisfies the dynamics of a controlled
diffusion (4) with control function

**c⋆(x, y, t) = [σ[⊤]∇** log h](x, y, t) (21)

This proves Equation (8).


-----

A.2 ANALYTICAL TRACTABILITY OF THE ORNSTEIN–UHLENBECK MODEL

The transition probability of the Ornstein–Uhlenbeck process considered in Section 5.1 is
_pt(dxˆ | x) = N_ (ˆx; µX(x, t), σX[2] [(][t][)][I][d][)][d][x][ˆ]
for time t > 0, with mean µX(x, t) = x exp(−t) and variance σX[2] [(][t][) =][ {][1][ −] [exp(][−][2][t][)][}][/][2][. From]
(6), we have

�
_h(x, y, t) =_ **Y[I][d][)][ N]** [(][x][T] [;][ µ][X][(][x][, T][ −] _[t][)][, σ]X[2]_ [(][T][ −] _[t][)][I][d][)][d][x][T]_

R[d][ N] [(][y][;][ x][T][, σ][2]

� 2[�]
1 _µX(x, T −_ _t)_

= (2π)[−][d/][2]σX[−][d][(][T][ −] _[t][)][σ]Y[−][d][σ]h[d][(][T][ −]_ _[t][) exp]_ _h[(][T][ −]_ _[t][)]_ + **[y]**

2 _[σ][2]_ ���� _σX[2]_ [(][T][ −] _[t][)]_ _σY[2]_ ����

� �
exp
_×_ _−_ _[∥][µ][X][(][x][, T][ −]_ _[t][)][∥][2]_ _−_ _[∥][y][∥][2]_

2σX[2] [(][T][ −] _[t][)]_ 2σY[2]

where σh[2][(][t][) =][ {][σ]X[−][2][(][t][) +][ σ]Y[−][2] Hence we can compute the value function v(x, y, t) =

_[}][−][1][.]_
log[h(x, y, t)]. Next, the optimal control function is
_−_

**c⋆(x, y, t) = [σ[⊤]∇** log h](x, y, t)

_h[(][T][ −]_ _[t][) exp][{−][(][T][ −]_ _[t][)][}]_ � _µX(x, T −_ _t)_ �
= _[σ][2]_ + **[y]** _−_ [exp][{−][(][T][ −] _[t][)][}]_ _µX(x, T −_ _t)._

_σX[2]_ [(][T][ −] _[t][)]_ _σX[2]_ [(][T][ −] _[t][)]_ _σY[2]_ _σX[2]_ [(][T][ −] _[t][)]_

The distribution of XT conditioned on X0 = x0 and YT = y is N (µh(x0, y, T ), σh[2][(][T] [)][I][d][)][ with]

� _µX(x0, T_ ) �
_µh(x0, y, T_ ) = σh[2][(][T] [)] + **[y]** _._

_σX[2]_ [(][T] [)] _σY[2]_

A.3 LOGISTIC DIFFUSION MODEL

In this section we consider a logistic diffusion process (Dennis & Costantino, 1988; Knape &
De Valpine, 2012) to model the dynamics of a population size {Pt}t≥0, defined by

_dPt = (θ3[2][/][2 +][ θ][1]_ _[−]_ _[θ][2][P][t][)][P][t]_ _[dt][ +][ θ][3][P][t]_ _[d][B][t][.]_ (22)
We apply the Lamperti transformation Xt = log(Pt)/θ3 and work with the process {Xt}t≥0 that
satisfies (1) with µ(x) = θ1/θ3 _−(θ2/θ3) exp(θ3x) and σ(x) = 1. Following (Knape & De Valpine,_
2012), we adopt a negative binomial observation model g(x, y) = NB(y; θ4, exp(θ3x)) for counts
**y ∈** N0 with dispersion θ4 > 0 and mean exp(θ3x). We set (θ1, θ2, θ3, θ4) as the parameter
estimates obtained in (Knape & De Valpine, 2012). Noting that (22) admits a Gamma distribution
with shape parameter 2(θ3[2][/][2 +][ θ][1][)][/θ]3[2] 3 [as stationary distribution]

_[−]_ [1][ and rate parameter][ 2][θ][2][/θ][2]
(Dennis & Costantino, 1988), we select ηX as the push-forward under the Lamperti transformation
and ηY as the implied distribution of the observation when training neural networks under both static
and iterative CDT schemes. To induce varying levels of informative observations, we considered
_θ4 ∈{1.069, 4.303, 17.631, 78.161}._

Figure 3 displays our filtering results for various number of simulated observations from the model
(Columns 1 to 4) and for K = 100 observations that are simulated with observation standard deviations larger than θ4 = 17.631 used to run the filters (Column 5). In the latter setup, we solved for
different values of θ4 in the negative binomial observation model to induce larger standard deviations. The behaviour of BPF and Iterative-APF is similar to the previous example as the observations
become more informative with larger values of θ4. Iterative-APF outperformed all other algorithms
over all combinations of θ4 and K considered, and also when filtering observations that are increasingly extreme under the model. We note also that the APFs trained using the CDT static scheme can
sometimes give unstable results, particularly in challenging scenarios.

A.4 CELL MODEL

This section examines a cell differentiation and development model from (Wang et al., 2011). Cellular expression levels Xt = (Xt,1, Xt,2) of two genes are modelled by (1) with

_µ(x) =_ �x[4]1[/][(2][−][4][ +][ x][4]1[) + 2][−][4][/][(2][−][4][ +][ x][4]2[)][ −] **[x][1]�** (23)
**x[4]2[/][(2][−][4][ +][ x][4]2[) + 2][−][4][/][(2][−][4][ +][ x][4]1[)][ −]** **[x][2]**


-----

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||

|Col1|Col2|
|---|---|
|||

|Col1|Col2|
|---|---|
|||

|Col1|Col2|Col3|
|---|---|---|
||||

|1 04|1 2|2 3|3 4|4 5|5 6|
|---|---|---|---|---|---|
|0 02 00 2||||||
|||||||


Figure 3: Results for logistic diffusion model based on 100 independent repetitions of each PF. The
ELBO gap in the second row is relative to Iterative-APF.

100 Y[ = 0.25] 100 Y[ = 0.5] 100 Y[ = 1.0] 100 Y[ = 2.0] 100

75 75 75 75 80

60

50 50 50 50 40

25 25 25 25 20

0 0 0 0 0
10[2] 10[3] 10[2] 10[3] 10[2] 10[3] 10[2] 10[3] 2 4 6 8 10

6

8

400 10 6 4 2000

200 5 4 2 1000

2

0 0 0 0 0

10[3] 10[2] 10[3] 10[3] 10[2] 10[3] 10[3] 10[2] 10[3] 10[3] 10[2] 10[3] 10[5] 2 4 6 8 10
10[2] 10[2] 10[2] 10[2] 10[3]
10[1] 10[1] 10[1] 10[1]
10[0] 10[0] 10[0] 10[0] 10[1]

10 1 10 1 10 1 10 1 10 1

10[2] 10[3] 10[2] 10[3] 10[2] 10[3] 10[2] 10[3] 2 4 6 8 10

K K K K Standard deviation

BPF Static-APF Iterative-APF Linear-GIRF Quadratic-GIRF

Figure 4: Results for cell model based on 100 independent repetitions of each PF. The ELBO gap in
the second row is relative to Iterative-APF.

_√_
and σ(x) = 0.1Id. The terms in (23) describe self-activation, mutual inhibition and inactivation

respectively, and the volatility captures intrinsic and external fluctuations. We initialize the diffusion
process from the undifferentiated state of X0 = (1, 1) and consider the Gaussian observation model
_g(x, y) = N_ (y; x, σY[2] **[I][2][)][. To train neural networks under both static and iterative CDT schemes,]**
we selected ηX and ηY as the empirical distributions obtained by simulating states and observations
from the model for 2000 time units.

Figure 4 illustrates our numerical results for various number of observations K and σY _∈_
0.25, 0.5, 1.0, 2.0 . It shows that Iterative-APF offers significant gains over all other algorithms
_{_ _}_
when filtering observations that are informative (see Columns 1 to 4) and highly extreme under the
model specification of σY = 0.5 (see Column 5). In this example, Static-APF did not exhibit any
unstable behaviour and its performance lies somewhere in between BPF and Iterative-APF.

A.5 GUIDED INTERMEDIATE RESAMPLING FILTERS.

We first describe our implementation of GIRF for online filtering. For M 1 particles, let
_≥_
_πk(dx) = M_ _[−][1][ �][M]j=1_ _[δ][(][d][x][;][ x]t[j]k_ [)][ denote a current approximation of the filtering distribution at]
time tk 0. Given the future observation Yk+1 = yk+1 at time tk+1, GIRF introduces a sequence
_≥_
of intermediate time steps tk = s0 < s1 < · · · < sP = tk+1 between the observation times, and a

|01 1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|


-----

sequence of guiding functions {Gp}p[P]=0 [satisfying]


_P_
�

_G0(xs0_ _, yk+1)_ _Gp(xsp−1_ _, xsp_ _, yk+1) = g(xtk+1_ _, yk+1)._ (24)

_p=1_


For each intermediate step p ∈{1, . . ., P _}, the particles x[1:]sp[M]_ are then propagated forward according to the original SDE (1), i.e. �x[j]sp+1 _[∼]_ _[p][∆][s]p+1_ [(][d][x][�][ |][ x][j]sp [)][ with stepsize][ ∆][s][p][+1] [=][ s][p][+1] _[−]_ _[s][p][. In]_
practice, this propagation step can be replaced by a numerical integrator. Each particle �x[j]sp+1 [is then]

_j_
associated with a normalized weight W _p+1_ [=][ W][ j]p+1[/][ �][M]i=1 _[W][ i]p+1[, where the unnormalized weight]_

_Wp[j]_ [=][ G][p][(][x]s[j]p−1 _[,][ �][x]s[j]p_ _[,][ y][k][+1][)][,]_ _p ∈{1, . . ., P −_ 1},

_WP[j]_ [=][ G][P][ (][x]s[j]P −1 _[,][ �][x]s[j]P_ _[,][ y][k][+1][)][G][0][(][x][�]s[j]P_ _[,][ y][k][+2][)][,]_ if tk+1 is not the final observation time,

_WP[j]_ [=][ G][P][ (][x]s[j]P −1 _[,][ �][x]s[j]P_ _[,][ y][k][+1][)][,]_ if tk+1 is the final observation time.


After the unnormalized weights are computed, the resampling operation is the same as a standard
PF (see Section 2.2).

From the above description, we see that the role of {Gp}p[P]=0 [is to guide particles to appropriate]
regions of the state-space using the weighting and resampling steps. The optimal choice of guiding
functions (Park & Ionides, 2020) is

_G0(xs0_ _, yk+1) = h(xs0_ _, yk+1, s0),_ _Gp(xsp−1_ _, xsp_ _, yk+1) =_ _h(hx(sxp−s1p,, y ykk+1+1, s, spp−)_ 1) _[,]_ (25)

for p ∈{1, . . ., P _}, where h : X × Y × [0, T_ ] → R+ defined in (6) is given by Doob’s h-transform.
The condition (24) is satisfied as we have a telescoping product and h(xtk+1 _, yk+1, tk+1) =_
_g(xtk+1_ _, yk+1)._ For the Ornstein–Uhlenbeck model of Section 5.1, we leveraged analytical
tractability of (25) in our implementation of GIRF. When the optimal choice (25) is intractable, one
sub-optimal but practice choice that gradually introduces information from the future observation
by annealing the observation density is

_G0(xs0_ _, yk+1) = g(xs0_ _, yk+1)[λ][0]_ _,_ _Gp(xsp−1_ _, xsp_ _, yk+1) =_ _g(gx(sxp−sp1_ _,, y ykk+1+1))[λ][λ][p][p][−][1][,]_

for p ∈{1, . . ., P _}, where {λp}p[P]=0_ [is a non-decreasing sequence with][ λ][P] [= 1][. This construction]
clearly satisfies the condition in (24). It is interesting to note that under the choice λp = 0 for
_p_ 1, . . ., P 1, GIRF recovers the BPF. In our numerical implementation, we considered both
_∈{_ _−_ _}_
linear and quadratic annealing schedules {λp}p[P]=0 [which determine the rate at which information]
from the future observation is introduced.

Lastly, we explain why GIRF with the optimal guiding functions (25) is still sub-optimal compared
to an APF that move particles using the optimal control c⋆ : X ×Y×[0, T ] → R[d] induced by Doob’s
_h-transform. We consider the law of {Xsp_ _}p[P]=1_ [conditioned on][ X][s]0 [=][ x][s]0 [and][ Y][k][+1] [=][ y][k][+1]

_P_
�

_p∆sp_ (dxsp | xsp−1 )g(xsP, yk+1). (26)
_p=1_

Under the condition (24), we can write the law (26) as


_P_
�

_G0(xs0_ _, yk+1)_ _p∆sp_ (dxsp | xsp−1 )Gp(xsp−1 _, xsp_ _, yk+1)._ (27)

_p=1_


GIRF can be understood as a Sequential Monte Carlo (SMC) algorithm (Chopin et al., 2020) approximating the law (27) with Markov transitions {p∆sp _}p[P]=1_ [and potential functions][ {][G][p][}]p[P]=0 [given by]
(25). We can rewrite (27) as


_G0(xs0_ _, yk+1)_


_P_
�

_p[h]∆sp_ [(][d][x][s]p _[|][ x][s]p−1_ [)][,] (28)
_p=1_


-----

where Markov transitions {p[h]∆sp _[}]p[P]=1_ [are defined as]

_p[h]∆sp_ [(][d][x][s]p _[|][ x][s]p−1_ [) =][ p][∆][s][p] [(][d][x]h[s]([p]x[ |]s[ x]p−[s]1[p],[−] y[1]k[)]+1[h][(], s[x][s]p[p]−[,][ y]1)[k][+1][, s][p][)] (29)


for p ∈{1, . . ., P _}. By the Markov property, we have h(xsp−1_ _, yk+1, sp−1) =_ �X _[p][∆][s][p]_ [(][d][x][s][p][ |]

**xsp−1** )h(xsp _, yk+1, sp), hence (29) is a valid Markov transition kernel. Moreover, it follows from_
Dai Pra (1991, Theorem 2.1) that {p[h]∆sp _[}]p[P]=1_ [are the transition probabilities of the controlled dif-]
fusion process in (4) with optimal control c⋆(x, y, t) = [σ[⊤]∇ log h](x, y, t). Hence an APF propagating particles according to this optimally controlled process can be seen as SMC algorithm approximating (28) with Markov transitions {p[h]∆sp _[}]p[P]=1_ [and a single potential function][ G][0][. By viewing]
GIRF and APF as specific instantaneous of SMC algorithms, it is clear that the former is sub-optimal
compared to the latter. Intuitively, this means that better particle approximations can be obtained by
moving particles well instead of relying on weighting and resampling.

A.6 COMPUTATIONAL DOOB’S h-TRANSFORM ALGORITHM

In this section, we provide figures to illustrate how our proposed CDT algorithm behaves. We
report the training curves (i.e. loss v.s. iteration), as well as describe the evolution of the approximate control functions parametrized by the neural networks. In the analytically tractable
Ornstein–Uhlenbeck case, comparison with the optimal control is possible. See Figures 5 and 6
for the Ornstein–Uhlenbeck model of Section 5.1, Figures 7 and 8 for the logistic diffusion model
of Section A.3, and Figure 9 for the cell model of Section A.4.


-----

(a) Evolution of loss estimate over first
500 optimization iterations.


(b) Evolution of neural network N0(x, y) (black to copper) approximating the initial value function v(x, y, 0) (red) over first 500
optimization iterations for a typical (left) and an extreme (right)
observation y.

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|1 0 0 0 0|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||
|||||||||0|
|||||||||0|
|||||||||0|
|||||||||1|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||


(c) Evolution of neural network −N (x, y, t) (black to copper) approximating the optimal control function
**c⋆(x, y, t) (red) over first 500 optimization iterations for a typical (upper row) and an extreme (lower row)**
observation y.

Figure 5: Results for Ornstein–Uhlenbeck model with d = 1 and σY = 1.0 during initial training
phase.


-----

(a) Evolution of loss estimate over
2000 optimization iterations under
static and iterative CDT schemes.


(b) Neural network approximation N0(x, y) of the initial value
function v(x, y, 0) after training with the static and iterative CDT
schemes for a typical (left) and an extreme (right) observation y.

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||||||
||||||
||||||


(c) Neural network approximation −N (x, y, t) of the optimal control function c⋆(x, y, t) after training with
the static and iterative CDT schemes for a typical (upper row) and an extreme (lower row) observation y.

Figure 6: Results for Ornstein–Uhlenbeck model with d = 1 and σY = 1.0 after training.


-----

(a) Evolution of loss estimate over first
500 optimization iterations.


(b) Evolution of neural network N0(x, y) (black to copper) approximating the initial value function v(x, y, 0) over first 500 optimization iterations for a typical (left) and an extreme (right) observation y.


(c) Evolution of neural network −N (x, y, t) (black to copper) approximating the optimal control function
**c⋆(x, y, t) over first 500 optimization iterations for a typical (upper row) and an extreme (lower row) observa-**
tion y.

Figure 7: Results for logistic diffusion model with θ4 = 1.069 during initial training phase.


-----

|4 = 7|78.161|
|---|---|
|||
|||
|||
|||
|||
|||
|||


(a) Evolution of loss estimate over 2000 optimization iterations under static and iterative CDT schemes and
various levels of informative observations.

7.55 9.8

7.50

9.7

7.45

9.6

7.40

9.5

7.35

9.4

7.30

9.3

6.0 6.5 7.0 7.5 8.0 8.5 6.0 6.5 7.0 7.5 8.0 8.5

x x

Static-CDT Iterative-CDT

(b) Neural network approximation N0(x, y) of the initial value function v(x, y, 0) after training with the static
and iterative CDT schemes for a typical (left) and an extreme (right) observation y.

t = 0.25 t = 0.5 t = 0.75
0.6

0.8

0.5

0.6

0.4 0.6

0.3 0.4 0.4

0.2 0.2 0.2

0.1

0.0 0.0 0.0

6.0 6.5 7.0 7.5 8.0 8.5 6.0 6.5 7.0 7.5 8.0 8.5 6.0 6.5 7.0 7.5 8.0 8.5

1.6 2.00 2.25

1.4 1.75 2.00

1.2 1.50 1.75

1.0 1.25 1.50

0.8 1.00 1.25

0.6 0.75 1.00

0.4 0.50 0.75

0.2 0.25 0.50

0.0 0.25

6.0 6.5 7.0 7.5 8.0 8.5 6.0 6.5 7.0 7.5 8.0 8.5 6.0 6.5 7.0 7.5 8.0 8.5

x x x

Static-CDT Iterative-CDT

(c) Neural network approximation −N (x, y, t) of the optimal control function c⋆(x, y, t) after training with
the static and iterative CDT schemes for a typical (upper row) and an extreme (lower row) observation y.

Figure 8: Results for logistic diffusion model with θ4 = 1.069 after training.


-----

(a) Evolution of loss estimate over 2000 optimization iterations under static and iterative CDT schemes and
various levels of informative observations.

3 3 5

2 2

4

1 1

0 0 3

1 1 2

0 2 0 2

3 3 14

2 2 12

1 1 10

0 0 8

1 1 6

0 2 0 2

x1 x1

(b) Neural network approximation N0(x, y) of the initial value function v(x, y, 0) after training with the static
(left column) and iterative (right column) CDT schemes for a typical (upper row) and an extreme (lower row)
observation y.

t = 0.25 t = 0.5 t = 0.75 t = 0.25 t = 0.5 t = 0.75

3 3 3 3 3 3

2 2 2 2 2 2

1 1 1 1 1 1

0 0 0 0 0 0

1 1 1 1 1 1

2 2 2 2 2 2

1 0 1 2 3 4 1 0 1 2 3 4 1 0 1 2 3 4 1 0 1 2 3 4 1 0 1 2 3 4 1 0 1 2 3 4

3 3 3 3 3 3

2 2 2 2 2 2

1 1 1 1 1 1

0 0 0 0 0 0

1 1 1 1 1 1

2 2 2 2 2 2

1 0 1 2 3 4 1 0 1 2 3 4 1 0 1 2 3 4 1 0 1 2 3 4 1 0 1 2 3 4 1 0 1 2 3 4

x1 x1 x1 x1 x1 x1

(c) Neural network approximation −N (x, y, t) of the optimal control function c⋆(x, y, t) after training with
the static (upper row) and iterative (lower row) CDT schemes for a typical and (columns 1-3) an extreme
(columns 4-6) observation y.

Figure 9: Results for cell model with σY = 0.5 after training.


-----


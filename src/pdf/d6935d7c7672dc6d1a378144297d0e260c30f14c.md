# RAINPROOF: AN UMBRELLA TO SHIELD TEXT GENER## ATORS FROM OUT-OF-DISTRIBUTION DATA

**Anonymous authors**
Paper under double-blind review

### ABSTRACT

As more and more conversational and translation systems are deployed in production, it is essential to implement and to develop effective control mechanisms
guaranteeing their proper functioning and security. An essential component to
ensure safe system behavior is out-of-distribution (OOD) detection, which aims at
detecting whether an input sample is statistically far from the training distribution.
Although OOD detection is a widely covered topic in classification tasks, it has
received much less attention in text generation. This paper addresses the problem of
OOD detection for machine translation and dialog generation from an operational
perspective. Our contributions include: (i) RAINPROOF a Relative informAItioN
Projection ODD detection framework; and (ii) a more operational evaluation setting
for OOD detection. Surprisingly, we find that OOD detection is not necessarily
aligned with task-specific measures. The OOD detector may filter out samples that
are well processed by the model and keep samples that are not, leading to weaker
performance. Our results show that RAINPROOF breaks this curse and achieve
good results in OOD detection while increasing performance.

### 1 INTRODUCTION

Significant progress have been made in Natural Language Generation (NLG) in recent years with
the development of powerful generic (e.g., GPT (Radford et al., 2018; 2019; Brown et al., 2020))
and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT
(Zhang et al., 2019)) text generators. Text generators power machine translation systems or chat
bots that are by definition exposed to the public and whose reliability is therefore a prerequisite for
adoption. Text generators are trained in the context of a so-called closed world (Antonucci et al.,
2021; Fei & Liu, 2016), where training and test data are assumed to be drawn i.i.d. from a single
distribution, known as the in-distribution. However, when deployed, these models operate in an open
world (Parmar et al., 2021; Zhou, 2022) where the i.i.d. assumption is often violated. This change in
data distribution is detrimental and induces a drop in performance as illustrated in Tab. 3 and Tab. 4.
Thus, to ensure the trustworthiness and adoption, it is necessary to develop tools to protect them from
harmful distribution shifts. For example, a trained translation model is not expected to be reliable
when presented with another language (e.g. a Spanish model exposed to Catalan, or a Dutch model
exposed to Afrikaans) or unexpected technical language (e.g., a colloquial translation model exposed
to rare technical terms from the medical field).

Most of the existing research, which aims to protect models from Out-Of-Distribution (OOD) data,
focuses on classification. Despite their importance, (conditional) text generation has received much
less attention even though it is among the most exposed applications. Existing solutions fall into two
categories. The first one called training-aware methods (Zhu et al., 2022; Vernekar et al., 2019a;b)
modifies the classifier training by exposing the neural network to OOD samples during training. The
second one called plug-in methods aims at distinguishing regular samples in the in distribution (IN)
from OOD samples based on the behavior of the model on a new input. Plug-in methods include
Maximum Softmax Prediction (MSP) (Hendrycks & Gimpel, 2016) or Energy (Lee et al., 2018a)
or feature-based anomaly detectors that compute a per-class anomaly score (Ming et al., 2022; Ryu
et al., 2017; Huang et al., 2020; Ren et al., 2021a). Although plug-in methods seem attractive, their
adaptation to text generation may not be straightforward. The sheer number of words present in the
vocabulary prevents it to be used directly within the classification framework.


-----

In this work, we aim at developing new tools to build more reliable text generators, which can be
used in practical systems. First, we work in the unsupervised detection setting where we do not
assume that we have access to OOD samples as they are often not available. Second, we work in
the black-box scenario, which is the most common in the Software as a Service framework Rudin &
Radin (2019). In the black-box setting detection methods only have access to the output of the DNN
architecture. Third, we want an easy-to-use and effective method to ensure adoptability. Last, we
argue that OOD detection impacts on tasks specific performance of the whole system should be taken
into account when choosing OOD detectors in an operational setting.

**Our contributions. Our main contributions can be summarized as follows:**

1. A more operational benchmark for text generation OOD detection. We present LOFTER the
**Language Out oF disTribution pErformance benchmaRk. Existing works on OOD detection for**
language modeling (Arora et al., 2021) focus on (i) english language only, (ii) the GLUE benchmark
and (iii) measure performance solely in terms of OOD detection. LOFTER is, in our view, a more
operational setting with a strong focus on neural machine translation (NMT) and dialog generation.
First, it introduces more realistic data shifts that go beyond English Fan et al. (2021): language shifts
induced by closely related language pairs (e.g., Spanish and Catalan or Dutch and Afrikaans[1]) and
domain change (e.g., medical vs news data or different types of dialogs). In addition, LOFTER comes
with an updated evaluation setting: detectors’ performance are jointly evaluated w.r.t the overall
system’s performance on the end task.
2. Novel information theoretic-based detectors. We present RAINPROOF: a Relative informAItioN
**Projection Out OF distribution detector. RAINPROOF is fully unsupervised. It is flexible and can**
be applied both when no reference samples (IN) are available (corresponding to scenario s0) and
when they are (corresponding to scenario s1). RAINPROOF tackles s0 by computing the models’
predictions negentropy (Brillouin, 1953). For s1, it relies its natural extension: the Information
Projection (Kullback, 1954; Csiszar´, 1967), an information-theoretic tool that remains overlooked by
the machine learning community.
3. New insights on the operational value of OOD detectors Our extensive experiments on LOFTER
show that OOD detectors may filter out samples that are well processed by the model and keep
_samples that are not, leading to weaker performance. Our results show that RAINPROOF breaks this_
curse and achieve good results in OOD detection while increasing performance.
4. Code and reproductibility. After acceptance, we will publish the open-source code on github.
com and the data to facilitate future research, ensure reproducibility and reduce computational costs.

### 2 PROBLEM STATEMENT & RELATED WORKS

2.1 NOTATIONS & CONDITIONAL TEXT GENERATION

Let us denote Ω a vocabulary of size |Ω| and Ω[∗] its Kleene closure (Fletcher et al., 1990)[2]. We
� �
denote P(Ω) = **p ∈** [0, 1][|][Ω][|] : [�]i[|][Ω]=1[|] **[p][i][ = 1]** the set of probability distributions defined over Ω.

Let Dtrain be the training set, composed of N ⩾ 1 i.i.d. samples {(x[i], y[i])}i[N]=1 _[∈]_ [(][X × Y][)][N]
with probability law pXY . We denote pX and pY the associated marginal laws of pXY . Each x[i] is a
sequence of tokens and we denote x[i]j _[∈]_ [Ω] [the][ j][th token of the][ i][th sequence.][ x]⩽[i] _t_ [=][ {][x]1[i] _[,][ · · ·][, x]t[i][} ∈]_
Ω[∗] denotes the prefix of length t. The same notations hold for y.

**Conditional textual generation. In conditional textual generation, the goal is to model a probability**
distribution p⋆(x, y) over variable-length text sequences (x, y) by finding pθ ≈ **p⋆(x, y) for**
any (x, y). In this work, we assume to have access to a pretrained conditional language model
_fθ : X × Y →_ **R[|][Ω][|]** where the output is the (unormalized) logits scores. fθ parameterized pθ, i.e.,
for any (x, y), pθ(x, y) = softmax(fθ(x, y)/T ) where T ∈ **R denotes the temperature. Given**
an input sequence x, the pretrained language fθ can recursively generate an output sequence ˆy by

1Afrikaans is a daughter language of Dutch (Jansen et al., 2007). The Dutch sentence: Appelen zijn
gewoonlijk groen, geel of rood can be translated in ”Appels is gewoonlik groen, geel of rooi.”
2The Kleene closure corresponds to sequences of arbitrary size written with words in Ω. Formally: Ω∗ =
�∞ Ω[i].

_i=0_


-----

sampling yt+1 ∼ **p[T]θ** [(][·|][x][,][ ˆ][y][⩽][t][)][, for][ t][ ∈] [[1][,][ |][y][|][]][. Note that][ ˆ][y][0][ is the start of sentence (][<][ SOS][ >]
token). We denote by (x), the set of normalized logits scores generated by the model when the
_S_
initial input is x i.e., S(x) = {softmax(fθ(x, ˆy⩽t))}t[|][y][ˆ]=1[|] [. Note that elements of][ S][(][x][)][ are discrete]
probability distributions on Ω.

2.2 PROBLEM STATEMENT

In OOD detection the goal is to find an anomaly score a : X → **R+ that quantifies how much a**
sample is far from the IN distribution. x is classified as IN or OUT according to the score a(x).
Following previous work (Hendrycks & Gimpel, 2016), one fixes a threshold γ and classifies the test
sample IN if a(x) ⩽ _γ or OOT if a(x) > γ. Formally, let us denote g(·, γ) the decision function, we_

� 1 if a (x) > γ
take: g(x, γ) =
0 if a (x) ⩽ _γ_

_Remark 1. In our setting, OOD examples are not available. In our experiments, we take γ such that_
at least 80% of the train set is classified as IN data. This assumption is reasonable since, in practice,
even a well tailored dataset might contains significant shares of outliers (Mishra et al., 2020).

2.3 REVIEW OF OOD DETECTORS

**OOD detection for classification. Most works on OOD detection have focused on detectors for**
classifiers and relies either on internal representations (features-based detectors) or on the final soft
probabilities produced by the classifier (softmax based detectors).

**Features-based detectors. They leverage latent representations to derive anomaly scores (Kirichenko**
et al., 2020; Zisselman & Tamar, 2020). The most well-known is the Mahanalobis distance (Lee et al.,
2018b; Ren et al., 2021b) but there are other methods employing Grams matrices (Sastry & Oore,
2020), Fisher Rao distance (Gomes et al., 2022) or other statistical tests (Haroush et al., 2021). Other
methods rely on the gradient space (Huang et al., 2021) or the moment of the features (Quintanilha
et al., 2019; Sun et al., 2021). These methods require access to the latent representations of the
models, which does not fit the black-box scenario. Moreover, they often rely on a per-class decision,
which is fine for classifiers but the sheer number of words in Ω makes it impossible to use for text
generation.

**Softmax-based detectors. These detectors rely on the soft probabilities produced by the model. The**
maximum softmax probability (Hendrycks & Gimpel, 2017; Hein et al., 2019; Liang et al., 2018; Hsu
et al., 2020) uses the probability of the mode while others take into account the entire distribution,
such as the Energy-based OOD detection scores (Liu et al., 2020). Due to the large vocabulary size,
it is unclear how these methods generalize to sequence generation tasks.

**OOD detection for text generation. Little work has been done on OOD detection for text generation.**
Therefore, we will follow Arora et al. (2021) and will rely on their baselines but also generalize
common OOD scores such as MSP or Energy to the context of text generation.

**Generalization to sequence generation. We generalize common OOD detectors for classification**
tasks by computing the average OOD score along the sequence at each step of the text generation.
We refer the reader to Sec. A.6 for more details.
_Remark 2. Note that features-based detectors assume a white-box framework where the internal_
representations of an input are accessible. By contrast to softmax-based detectors which only rely
on the final output. Following Arora et al. (2021), we work in a black-box framework (Chen et al.,
2020). We also compare our results to the Mahalanobis distance (Lee et al., 2018b), as it is known to
be a strong baseline.

### 3 RAINPROOF AN INFORMATION THEORETIC OOD DETECTORS

3.1 INFORMATION THEORETICAL BACKGROUND

An information measure : (Ω) (Ω) **R quantifies the similarity between any pair of**
_I_ _P_ _× P_ _→_
discrete distributions p, q (Ω). Since Ω is a finite set, we will adopt the following notations
_∈P_
**p = [p1, · · ·, p|Ω|] and q = [q1, · · ·, q|Ω|]. The development of new information measures for**


-----

specific applications has received much attention over the years (Fujisawa & Eguchi, 2008; Cichocki
et al., 2011) (we refer the reader to Basseville (2013) for a complete review). While there exist
information distances, it is, in general, difficult to build metrics that satisfy all the properties of a
distance, thus we often rely on divergences which drop the symmetry property and the triangular
inequality. In what follows, we motivate the information measures we will use in this work.

First, we rely on the Renyi divergences´ (Csiszar´, 1967). Renyi divergences belong to the´ _f_ -divergences
family and are parametrized by a parameter α ∈ **R+ −{1}. They are flexible and include well-known**
divergences such as the Kullback-Leiber divergence (KL) Kullback (1959) (when α 1) or the
_→_
Hellinger distance (Hellinger, 1909) (when α = 0.5). The Renyi divergence between´ **p and q is**
defined as follows:

1 �|Ω| **p[α]i** 
_Dα(p∥q) =_   _._ (1)

_α −_ 1 [log] _i=1_ **qi[α][−][1]**

The Renyi divergence is widely used in machine learning (Peters et al., 2019) because α allows
weighting the relative influence of the distributions’ tail.

Second, we investigate the Fisher-Rao distance (FR). FR is a distance on the Riemannian space
formed by the parametric distributions, using Fisher information matrix as its metric (Amari, 2012).
It computes the geodesic distance between two discrete distributions (Rao, 1992; Pinele et al., 2020)
and is defined as follows:

_|Ω|_
� _√_

FR(p∥q) = π[2] [arccos] **pi × qi.** (2)

_i=1_

It has recently found many applications (Picot et al., 2022; Colombo et al., 2022b;a) and is known to
be more accurate than popular divergence measures (Costa et al., 2015).

3.2 RAINPROOF FOR THE NO-REFERENCE SCENARIO (s0)

At inference time, the no-reference scenario (s0) does not assume the existence of a reference set of
IN samples to decide whether a new input sample is OOD. Softmax-based detectors such as MSP
(Hendrycks & Gimpel, 2016), Energy (Liu et al., 2020) or the sequence likelihood[3] (Arora et al.,
2021) are examples of OOD scores operating under s0.

Under these assumptions, our OOD detector RAINPROOF is composed of three steps. For a given
input x with generated sentence ˆy:

1. We first use fθ to extract the step-by-step sequence of soft distributions S(x).

2. We then compute an anomaly score (aI(x)) by averaging a step-by-step score provided by I. This
step-by-step score is obtained by measuring the similarity between a reference distribution u (Ω)
_∈P_
and one element of (x). Formally:
_S_


1
_aI(x) =_ (x)

_|S_ _|_


�

(p **u),** (3)
_I_ _∥_
**p∈S(x)**


where (x) = **yˆ** .
_|S_ _|_ _|_ _|_

3. The last step consists in thresholding the previous anomaly score aI(x). If aI(x) is over a given
threshold γ, we classify x as an OOD example.

**Interpretation of Eq. 3. aI(x) measures the average dissimilarity of the probability distribution of**
the next token to normality (as defined by u). aI(x) also corresponds to the token average uncertainty
of the model fθ to generate ˆy when the input is x. The intuition behind Eq. 3 is that the distributions
produced by fθ, when exposed to an OOD sample, should be far from normality and thus should
have a high score.

**Choice of u and I. The uncertainty definition of Eq. 3 depends on the choice of both the reference**
distribution u and the information measure . A natural choice for u is the uniform distribution,
_I_

3The likelyhood of the sequence is the same as the perplexity. In our work we report the log-likelyhood for
numerical stability reasons: i.e., aL(x) = − [�]t[|][y][ˆ]=0[|−][1] log pθ(ˆyt+1|x, ˆy⩽t)


-----

_i.e., u = [_ _|Ω[1]_ _|_ _[,][ · · ·][,]_ _|Ω1_ _|_ []][ which we will use in this work. It is worth pointing out that][ I][(][·||][u][)][ yields]

the negentropy of a distribution. Other possible choices for u include one hot or tf-idf distribution
(Colombo et al., 2022b). For I, we rely on the Renyi divergence to obtain´ _aDα and the Fisher-Rao_
distance to obtain aFR.

3.3 RAINPROOF FOR THE REFERENCE SCENARIO (s1)

In the with reference scenario (s1), we assume that one has access to a reference set of IN samples
_R = {x[i]_ : (x[i], y[i]) ∈Dtrain}i[|R|]=1 [where][ |R|][ is the size of the reference set. For example, the]
Mahalanobis distance works under this assumption. One of the weakness of Eq. 3 is that it imposes is
to imposes an ad-hoc choice when using u (the uniform distribution). In s1, we can leverage R, to
obtain a data-driven notion normality.

Under s1, our OOD detector RAINPROOF follows these four steps:

1. (Offline) For each x[i], we generate ˆy[i] and the associated sequence of probability distributions
_∈R_
(S(x[i])). Overall we thus generate [�]x∈R _[|][ˆy][i][|][ probability distributions which could explode for]_

long sequences[4]. To overcome this limitation, we rely on the bag of distributions of each sequence
(Colombo et al., 2022b). We form the set of these bags of distributions


�

**p**
**p∈S(x[i])**





(4)
 _[.]_


¯ = �
_S_ _[∗]_

**x[i]∈R**



 1

(x[i])

 _|S_ _|_


2. (Online) For a given input x with generated sentence ˆy, we compute its bag of distributions
representation

1 �
**¯p(x) =** **p.** (5)

(x)
_|S_ _|_ **p∈S(x)**

3. (Online) For x, we then compute an anomaly score a[⋆]
_I[(][x][)][ by projecting][ ¯p][(][x][)][ on the set][ ¯][S]_ _[∗][.]_
Formally, a[⋆]
_I[(][x][)][ is defined as:]_
_a[⋆]_ (6)
_I[(][x][) = min]p∈S[¯][⋆]_ _[I][(][p][∥][p][¯][(][x][))][.]_

We denote p[⋆](x) = arg min
**p∈S[¯][∗]** _[I][(][p][∥][p][¯][(][x][))][.]_

4. The last step consists of thresholding the previous anomaly score aI(x). If aI(x) is over a given
threshold γ, we classify x as an OOD example.

**Interpretation of Eq. 6. aI(x) relies on a Generalized Information Projection (Kullback, 1954;**
Csiszar´, 1975; 1984)[5] which measures the similarity between ¯p(x) and the set [¯] . Note that the
_S_ _[∗]_
closest element of [¯] in the sens of can give insights on the decision of the detector. It allows to
_S_ _[∗]_ _I_
interpret the decision of the detector as we will see in Tab. 5.

**Choice of I. Similarly to Sec. 3.2, we will rely on the Renyi divergence to define´** _a[⋆]Rα_ [(][x][)][ and the]
Fisher-Rao distance a[⋆]FR[(][x][)][.]

### 4 RESULTS ON LOFTER

4.1 LOFTER: LANGUAGE OUT OF DISTRIBUTION PERFORMANCE BENCHMARK

**LOFTER for NMT. We consider two main types of changes: language changes and domain changes,**
which both can occur in real-world situations. For each shift, we rely on pretrained generators from
the HuggingFace Hub. Further experiemental details are relegated to Ap. A. Language shifts can

4It is also worth pointing that doing a projection at each timestep would require a per-step reference set
in addition to the computational time required to actually compute the projections, therefore we decided to
aggregate the probability distributions over the sequence.

5The minimization problem of Eq. 6 finds numerous connections in the theory of large deviation (Sanov,

1958) or in statistical physics (Jaynes, 1957).


-----

Table 1: Summary of the performance and computational cost of every detector.


(a) Summary of the performance of our detectors (Ours) compared to commonly used strong baselines (Bas.). We report in bold the best detector for
each scenario and we underline the best overall.


(b) Computation time (in
seconds) for the different detectors. Off. (Onl.) stands
for offline (resp. online)
time.

|time.|Col2|
|---|---|
|Score|Off. Onl.|
|a aD Mα 2 1. .1 10 0− −3 s 4 s SP a 40s 3.10−3 s M a 9.10−2 s D α∗||

|Language shifts AUROC FPR F1|Domain shifts AUROC FPR F1|Dialog shifts AUROC FPR F1|
|---|---|---|
|AUROC FPR F1|AUROC FPR F1|AUROC FPR F1|
|aDα 0.95 0.25 0.84 Ours aFR 0.93 0.28 0.83 s0 aE 0.89 0.44 0.77 Bas. aMSP 0.87 0.44 0.75 aL 0.78 0.79 0.50|0.85 0.62 0.75 0.74 0.87 0.60|0.79 0.64 0.66 0.72 0.70 0.64|
||0.76 0.78 0.71 0.78 0.77 0.71 0.72 0.89 0.65|0.65 0.76 0.57 0.66 0.72 0.21 0.65 0.95 0.62|
|a aaD MFRα∗ 00 .. 98 28 00 .. .3 24 0 0. .7 78 O Bau sr .s 0 . 8 8 0 3 5 0 . 7 7 s1 ∗ 6 3 aC 0.71 0.80 0.62|0.86 0.50 0.70 0.81 0.69 0.69|0.86 0.52 0.59 0.76 0.75 0.38|
||0.78 0.59 0.40 0.68 0.76 0.67|0.84 0.55 0.56 0.72 0.61 0.48|


appear when a translation system is exposed to a language that is extremely similar to the language
the system has been trained on (e.g., Afrikaans for a system trained on Dutch) and, therefore, can
lead to significant translation errors (see Tab. 7)). For language shifts, we focus on closely related
language pairs coming from the Tatoeba dataset (Tiedemann, 2012b) (see Tab. 6). We study the shifts
induced by Catalan-Spanish, Portugese-Spanish and Afrikaans-Dutch. Domain shifts, which occur
when the model is exposed to a specific topic that was not seen during training, can also affect the
quality of the translation (see Tab. 4). To simulate domain shifts, we use the language Tatoeba MT
dataset (Tiedemann, 2020) and the news commentary dataset (Tiedemann, 2012b) as base datasets
and the shifts are induced by the EuroParl dataset (Tiedemann, 2012a) and EMEA (Tiedemann, 2012b)
dataset.

**LOFTER for dialogs. For conversational agents, an interesting scenario is when a goal-oriented agent**
designed to handle a specific type of conversations (e.g., customer conversations, daily dialogue) is
exposed to an unexpected conversation. In this case, it is crucial to interrupt the agent so it does not
damage the user’s trust with misplaced responses (Perez et al., 2022). We rely on the Multi WOZ
dataset (Zang et al., 2020), a human to human dataset collected in the Wizard-of-Oz set-up (Kelley,

1984), for IN distribution data. This choice is mostly motivated by the availability of pretrained
models on Multi WOZ. For dialog shifts, we use spoken datasets coming from various sources
which are part of the SILICONE benchmark (Chapuis et al., 2020). Specifically, we use a goaloriented dataset (i.e., Switchboard Dialog Act Corpus (SwDA) (Stolcke et al., 2000)), a multi-party
meetings dataset (i.e., MRDA (Shriberg et al., 2004) and Multimodal EmotionLines Dataset MELD
(Poria et al., 2018)), daily communication dialogs ( i.e., DailyDialog DyDA Li et al. (2017)), and
scripted scenarii (i.e., IEMOCAP Tripathi et al. (2018)). We refer the curious reader to Sec. A.4 for
more details on each dataset.

**Metrics. OOD detection is usually framed as an unbalanced binary classification problem where the**
class of interest is OUT. We can assess the performance of our OOD detectors focusing on the False
**alarm rate (FPR) and on the True detection rate (TPR). To evaluate the performance on the OOD**
task we report the AUROC and the FPR.
_Area Under the Receiver Operating Characteristic curve (AUROC) (Bradley, 1997). The AUROC can_
be interpreted as the probability that an IN-distribution example has an higher anomaly score than an
OOD sample. For this metric, higher is better.
_False Positive Rate at r% True Positive Rate (FPR). In many practical application, we have to detect_
at least r% of the the OOD samples. This corresponds to pre-defined safety level. FPR quantifies
the share of IN samples we wrongly detect under this constraint. It leads to select a threshold γr
such that the corresponding TPR equals r. In our work r is set to 95%. Additional details on these
metrics can be found in Sec. A.1. F1, precision and recall. In addition we report the F1 scores of
the detectors with a threshold designed such that 80% of the IN dataset is actually classified as IN.

4.2 EXPERIMENTS IN MACHINE TRANSLATION AND RESULTS

**Results on language shifts. We assess, for each language pair, the OOD detection performance of**
RAINPROOF and report the average AUROC and FPR in Tab. 1a. We provide the detailed results
in Tab. 8. We find that our no-reference methods (aDα and aFR) achieve better performance that


-----

common no-reference baselines but also outperform the reference-based baseline. In particular, aDα,
by achieving an AUROC of 0.95 and FPR of 0.25, outperforms all considered methods. Moreover,
while no-reference baselines only capture up to 62% of the OOD samples on average, ours detect up
to 83.5%, achieving even better results than the with-reference baseline (75.3%).

**Results on domain shifts. We eval-**
uate the OOD detection performance Table 2: Correlation between OOD scores and translation
of RAINPROOF on domain shifts in metrics BLEU and BERT-S on domain shifts datasets.
Spanish and German with technical
medical data and parliamentary data. Bertscore f1 Bleu score
We report the average OOD detec- ALL IN OUT ALL IN OUT

Scenario Score

tion performance in Tab. 1a. In s0,
we observe that aDα and aFR outper- Ours _aaDFRα_ -0.29-0.36 -0.26-0.30 -0.18-0.27 -0.17-0.24 -0.22-0.26 -0.09-0.19
form the strongest baselines (i.e., En- s0 _aE_ -0.19 -0.24 -0.33 -0.26 -0.19 -0.39
ergy, MSP and sequence likelihood) Baselines _aaLMSP_ -0.47-0.15 -0.51-0.19 -0.48-0.29 -0.49-0.24 -0.45-0.16 -0.49-0.37
by several AUROC points. Interest- Ours _aDα∗_ -0.12 0.00 -0.09 -0.19 0.00 -0.09
ingly enough even our no-reference s1 Baselines _aaFRC_ _[∗]_ -0.11-0.02 -0.040.00 -0.120.01 -0.17-0.13 -0.050.00 -0.10-0.08
detectors outperform the reference- _aM_ 0.02 0.00 0.06 -0.06 0.00 0.07
based baseline (i.e., aM ). However,
we find that relying on a reference set
is a must-have in terms of FPR. While aDα achieves similar AUROC performance to its information
projection counterpart aDα∗, the latter achieve much better FPR.

4.3 EXPERIMENTS IN DIALOG GENERATION AND RESULTS


**Results on Dialog shifts. The dialog shifts benchmark is more difficult than NMT benchmark as**
all detectors achieve lower performances. It is the only case where our no-reference detectors do
not outperform the Mahalanobis baseline and achieve only 0.79 in AUROC. The best baseline is
the Mahalanobis distance and achieves better performance on dialog task than on NMT domain
shifts reaching an AUROC of 0.84. However, our reference based detector based on the Renyi´
information projection secures better AUROC (0.86) and better FPR (0.52). Even though RAINPROOF
outperforms all the baselines, shifts in dialog are hard to detect and will require further investigations.
Non-aggregated results for dialog are provided in Ap. C. They show that RAINPROOF consistently
outperforms baselines on all datasets.

**Importance of distribution tails. Our results show that,**
when it comes to domain shift (domain shifts in translation 0.9
or dialog shifts), reference-based detectors are required to | 400|
obtain good results. They also show that, the more these 0.8 800
detectors take into account the tail of the distributions, the 12001600
better they are, as displayed in Sec. B.1. We find that low 0.7 2000
values of α (near 0) yields better results with the Renyi´
Information projection aDα∗ . It suggests that the tail of the 0.6
distributions used during text generation carries context information and insights on the processed texts. Such results 0.5
are consistent with findings of recent works in the context
of automatic evaluation of text generation (Colombo et al.,
2022b).
**Comparison to the Mahalanobis distance.** Our Figure 1: Impact of α on the perforreference-based detector work with a small reference set. mance of the Renyi information projec-´
In our experiments, we use reference sets of size 10 to tion for dialog shifts detection. A smaller
2000. The Mahalanobis distance requires to approximate _α increases the weight of the tail of the_
the covariance matrix of the reference set. In our simula- distribution. An α of 0 would consist in
tions, the embeddings of dimension 512 make the estima- counting the number of the common non
tion unreliable. On the contrary, RAINPROOF, which rely zero elements.
on information projections, remains numerically sound with small reference set.


-----

Table 3: Average impact of different OOD detectors on the BLEU score for different type of dataset:
IN data only, OOD data and the combination of both ALL. For each we report the absolute average
BLEU score (Abs.), the average gains in BLEU (G.s) compared to a setting without OOD filtering
(fθ only) and the share of the subset removed by the detector (R.Sh.). These results are achieved by
setting γ such that we remove 20% of the IN dataset.

IN OOD ALL
Absolute Gains Removed shares Absolute Gains Removed shares Absolute Gains Removed shares

53.6 +0.0 0.0% 30.8 +0.0 0.0% 44.4 +0.0 0.0%

Ours _aDα_ 57.2 +3.6 19.7% 40.9 +10.1 57.1% 55.6 +11.2 35.1%

_aFR_ 56.6 +3.1 19.0% 39.9 +9.1 60.8% 54.6 +10.1 36.7%

s0 _aE_ 56.3 +2.7 20.0% 31.9 +1.1 31.9% 48.3 +3.9 24.1%

Bas. _aL_ 58.1 +4.5 19.2% 34.6 +3.8 43.7% 52.4 +8.0 28.9%

_aMSP_ 52.4 -1.2 18.5% 26.7 -4.1 38.2% 43.2 -1.2 28.1%

Ours _aDα∗_ 54.0 +0.4 19.2% 31.6 +0.8 61.3% 48.9 +4.5 38.0%
s1 Bas. _aaFRC_ _[∗]_ 54.054.2 +0.4+0.7 19.4%14.6% 31.631.3 +0.8+0.5 61.4%17.9% 49.046.1 +4.5+1.6 38.1%15.5%

_aM_ 53.6 +0.1 20.0% 31.6 +0.9 59.0% 47.4 +3.0 37.9%

### 5 TOWARDS A PRACTICAL EVALUATION OF OOD DETECTORS

Following previous work, we measure the performance of the detectors on the OOD detection task
based on AUROC and FPR. However, this evaluation framework neglects the impact of the detector
on the overall system’s performance. We identify three main evaluation criteria that are important in
practice: execution time, overall system performance in terms of quality of the generated sentences,
and interpretability of the decision. Our study is conducted on NMT because due to the existence
of relevant and widely adopted metrics for assessing the quality of a generated sentence (i.e., BLEU
(Papineni et al., 2002) and BERTSCORE (BERT-S) (Unanue et al., 2021)).

5.1 COMPLEXITY STUDY

**Runtime and memory costs. We report in Tab. 1b the runtime of all methods. Detectors for s0 are**
faster than the ones for s1. Contrarily to detectors using references, the no-reference detectors do
not require additional memory. They can be setup easily in a plug&play manner at the output of any
model.

**Numerical stability. The Mahalanobis distance requires to estimate both µ and Σ[−][1]** (see Sec. A.6).
The dimension of the latent space of the considered pre-trained model is either 768 or 512. In this
setting, when the size of the reference set is small, the estimation of the Mahalanobis parameters is
numerically unstable. For s1, RAINPROOF relies on information projection and does not involve
numerically unstable computations but requires a larger memory footprint (0.5 GB) to store the
reference set (2000 probability distributions of dimension 50K).

5.2 IMPACT OF OOD FILTERING ON TRANSLATION QUALITY

The main objective of OOD filtering is to remove samples that are far from the training distribution.
On these samples, the user has no guarantee that the model will produce a good quality translation. In
this experiment, we compare the performance of the system with and without the different detectors
in terms of the quality of the generated sentence.

**Global performance. In Tab. 3, we report the global performance of the systems (fθ) without and**
with OOD detectors on IN samples, OOD samples and all samples (ALL). From the first row of
Tab. 3, we notice that OOD samples are harmful to the model. We observe that, in most of the cases,
adding detectors increases the model performance on IN, OOD and all samples. Exceptions include
_aMSP (for OOD, IN and ALL) and aM (for OOD). Results indicate that no-reference RAINPROOF_
outperforms the reference-based version of RAINPROOF. Thus, OOD detector evaluation should
**consider the final task performance. Overall, it is worth noting that directly adapting classical**
OOD detection methods (e.g., MSP or Energy) to the sequence generation problem leads to poor
results in terms of performance gains (i.e., as measured by BLEU or BERT-S). In others words, the
**final task does not benefit from adding classical OOD detectors.**

**Finer performance analysis. In Tab. 4, we report the per-shift-types performance of fθ with and**
without OOD detector. In Tab. 4, we observe a decrease in performance in the case of language and


-----

Table 4: Detailed impacts on NMT performance results per tasks (Domain- or Language-shifts) of
the different OOD detectors. We present results on the different part of the data: IN data, OOD data
and the combination of both, ALL. For each we report the absolute average BLEU score (Abs.), the
average gains in BLEU (G.s.) compared to a setting without OOD filtering (fθ only) and the share
of the subset removed by the detector (R.Sh.). We provide more detailed results on each dataset in
Ap. D

Domain shifts Language shifts
IN OOD ALL IN OOD ALL
Abs. G. Rh. Abs. G. Rh. Abs. G. Rh. Abs. G. Rh. Abs. G. Rh. Abs. G. Rh.

46.9 +0.0 0.0% 43.3 +0.0 0.0% 45.1 +0.0 0.0% 60.2 +0.0 0.0% 18.3 +0.0 0.0% 43.8 +0.0 0.0%

Ours _aaFRDα_ 50.549.7 +3.6+2.8 19.6%19.3% 48.547.6 +5.2+4.4 29.8%40.6% 50.549.2 +5.4+4.1 24.7%29.9% 63.863.5 +3.6+3.3 19.8%18.7% 33.332.1 +15.0+13.8 84.3%81.0% 60.760.0 +17.0+16.2 45.4%43.5%

s0 _aE_ 49.3 +2.4 20.0% 45.5 +2.2 17.8% 47.6 +2.5 18.9% 63.3 +3.1 20.0% 18.4 +0.1 46.0% 49.1 +5.3 29.4%

Bas. _aL_ 50.7 +3.8 19.2% 47.5 +4.3 24.2% 49.9 +4.8 21.7% 65.4 +5.2 19.2% 21.6 +3.3 63.2% 54.9 +11.1 36.1%

_aMSP_ 45.8 -1.1 19.2% 33.4 -9.9 45.7% 40.8 -4.3 32.5% 59.0 -1.3 17.7% 20.1 +1.8 30.8% 45.7 +1.9 23.7%

Ours _aDα∗_ 47.1 +0.2 18.9% 37.8 -5.4 62.7% 45.9 +0.9 40.8% 60.9 +0.6 19.5% 25.4 +7.1 60.0% 51.9 +8.2 35.2%
s1 Bas. _aaaFRMC_ _[∗]_ 47.147.646.9 +0.2+0.7-0.0 18.9%13.6%20.0% 37.743.543.0 +0.3-5.5-0.3 62.6%61.9%3.6% 46.045.444.4 +0.9+0.3-0.7 40.7%41.0%8.6% 60.960.960.4 +0.7+0.7+0.1 19.9%15.6%20.0% 25.519.120.3 +7.2+0.8+2.0 60.2%32.2%56.0% 52.046.750.4 +8.2+2.9+6.6 35.5%22.5%34.9%

domain shifts, the latter being more harmful. On domain shifts, we observe that reference-based
detectors decrease system’s performance on OOD samples. This means that the detectors tend to filter
out samples that are well-handled by the model and ignore sentences that are not. It is worth noting
that reference-based detectors remove, in proportion, twice as many samples as their no-reference
counterparts, while the threshold selection procedure remains the same. This observation also holds
when removing less samples (i.e., calibrating γ that we remove 10%, 5% or even 1% of the IN
dataset) (Tab. 15).

**Threshold free analysis. In Tab. 2, we report the correlation between OOD scores and final task**
performance for the case of domain shifts. We refer the reader to Tab. 14 for the results on language
shifts. We observe that the likelihood score is the most correlated with the final sentence quality, as
measured by BLEU or BERT-S. This finding illustrates that higher correlation with sentence quality
does not necessarily translate into higher performance gains when filtering OOD samples. This result
_suggests that Quality Estimation (Specia et al., 2010; Blatz et al., 2004), while closely related, is a_
_different problem._

5.3 TOWARDS AN INTERPRETABLE DECISION


An important dimension fostering adoption is the ability to verify the decision taken by the automatic system
(Montavon et al., 2018). RAINPROOF offers a step in
this direction when used with references: for each input
sample, RAINPROOF finds the closest sample (in the sens
of the Information Projection) in the reference set to take
its decision. We present in Tab. 5 some OOD samples
along with their translation scores, projection scores, and
their projection on the reference set. We notice that, in
general, sentences that are close to the reference set, and
whose projection has a close meaning, are better handled
by fθ. Therefore, one can visually interpret the prediction
of RAINPROOF, and validate it. This observation further
validate our method.

### 6 CONCLUSIONS


Table 5: OOD inputs, their translations
and projections onto the reference set.
The first 2 are far from the reference set
and not well translated whereas the next
2 are very close to the reference set and
well translated. We can, for that matter,
notice that the projection is quite close to
the input sentence grammatically speaking.


In this work, we introduced both a detection framework sourceGround-truth Austr`alia no ´es Australia isn’t Austria.Austria.[`]
called RAINPROOF as well as a new benchmark called Generated Austr`alia is not Austria. BLEU 21.86
LOFTER for detecting OOD samples when using textual **p[⋆](x)** La vida no es f´acil. **Score 0.82**
generators in the black-box scenario. Our work adopts
an operational perspective by not only considering OOD
performance but also task-specific metrics. Our results show that, despite the good results obtained in

|Source Ground truth Generated p⋆(x)|Ahir a la nit va`rem treballar fins a les deu. Last night we worked until 10 p.m. Ahir a la nit va`rem treballar fins a les deu. BLEU 3.75 Dar gato por liebre. Score 1.23|
|---|---|
|Source Ground-truth Generated p⋆(x)|Aquesta cola s’ha esbravat i no te´ bon gust. This cola has lost its fizz and doesn’t taste any good. This tail s’ha esbravat i no tea bon gust. BLEU 4.09 Esta cuchara es de te´. Score 1.14|
|source Ground-truth Generated p⋆(x)|Aquesta e´s una carta molt estranya. This is a very strange letter. This is a molt estranya card. BLEU 26.27 Este carro es chiquito. Score 0.74|
|source Ground-truth Generated p⋆(x)|Austra`lia no e´s A`ustria. Australia isn’t Austria. Austra`lia is not Austria. BLEU 21.86 La vida no es fa´cil. Score 0.82|


-----

pure OOD detection, OOD filtering can harm the performance of the final system, as it is the case for
_MSP or Mahanalobis. We found that, RAINPROOF breaks this curse and induces significant gains in_
translation performance both on OOD samples and in general. In conclusion, this work paves the way
to the development of detectors tailored for text generators and calls for a global evaluation when
benchmarking future OOD detectors.


-----

### REFERENCES

Shun-ichi Amari. Differential-geometrical methods in statistics, volume 28. Springer Science &
Business Media, 2012.

Alessandro Antonucci, Alessandro Facchini, and Lilith Mattei. Structural learning of probabilistic
sentential decision diagrams under partial closed-world assumption. 2021. doi: 10.48550/ARXIV.
[2107.12130. URL https://arxiv.org/abs/2107.12130.](https://arxiv.org/abs/2107.12130)

Udit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect them.
_arXiv preprint arXiv:2109.06827, 2021._

Michele Basseville. Divergence measures for statistical data processing—an annotated bibliography.´
_Signal Processing, 93(4):621–633, 2013._

John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. Confidence estimation for machine translation. In Coling 2004:
_Proceedings of the 20th international conference on computational linguistics, pp. 315–321, 2004._

Andrew P Bradley. The use of the area under the roc curve in the evaluation of machine learning
algorithms. Pattern recognition, 30(7):1145–1159, 1997.

L. Brillouin. The negentropy principle of information. Journal of Applied Physics, 24(9):1152–
[1163, September 1953. doi: 10.1063/1.1721463. URL https://doi.org/10.1063/1.](https://doi.org/10.1063/1.1721463)
[1721463.](https://doi.org/10.1063/1.1721463)

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Emile Chapuis, Pierre Colombo, Matteo Manica, Matthieu Labeau, and Chloe Clavel. Hierar-´
chical pre-training for sequence labelling in spoken dialog. In Findings of the Association
_for Computational Linguistics: EMNLP 2020, pp. 2636–2648, Online, November 2020. As-_
sociation for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.239. URL

[https://www.aclweb.org/anthology/2020.findings-emnlp.239.](https://www.aclweb.org/anthology/2020.findings-emnlp.239)

Jianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-efficient
decision-based attack. In 2020 ieee symposium on security and privacy (sp), pp. 1277–1294. IEEE,
2020.

Andrzej Cichocki, Sergio Cruces, and Shun-ichi Amari. Generalized alpha-beta divergences and their
application to robust nonnegative matrix factorization. Entropy, 13(1):134–170, 2011.

Pierre Colombo, Guillaume Staerman, Nathan Noiry, and Pablo Piantanida. Learning disentangled
textual representations via statistical measures of similarity. arXiv preprint arXiv:2205.03589,
2022a.

Pierre Jean A Colombo, Chloe Clavel, and Pablo Piantanida. Infolm: A new metric to evaluate´
summarization & data2text generation. In Proceedings of the AAAI Conference on Artificial
_Intelligence, volume 36, pp. 10554–10562, 2022b._

Sueli IR Costa, Sandra A Santos, and Joao E Strapasson. Fisher information distance: A geometrical
reading. Discrete Applied Mathematics, 197:59–69, 2015.

Imre Csiszar. Information-type measures of difference of probability distributions and indirect´
observation. studia scientiarum Mathematicarum Hungarica, 2:229–318, 1967.

Imre Csiszar. I-divergence geometry of probability distributions and minimization problems.´ _The_
_annals of probability, pp. 146–158, 1975._

Imre Csiszar. Sanov property, generalized i-projection and a conditional limit theorem.´ _The Annals_
_of Probability, pp. 768–793, 1984._

Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In
_Proceedings of the 23rd international conference on Machine learning, pp. 233–240, 2006._


-----

Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal,
Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond englishcentric multilingual machine translation. J. Mach. Learn. Res., 22(107):1–48, 2021.

Geli Fei and Bing Liu. Breaking the closed world assumption in text classification. In Proceedings
_of the 2016 Conference of the North American Chapter of the Association for Computational_
_Linguistics: Human Language Technologies, pp. 506–514, 2016._

Peter Fletcher, Hughes Hoyle, and C Wayne Patty. Foundations of discrete mathematics. Brooks/Cole,
Florence, KY, November 1990.

Hironori Fujisawa and Shinto Eguchi. Robust parameter estimation with a small bias against heavy
contamination. Journal of Multivariate Analysis, 99(9):2053–2081, 2008.

Eduardo Dadalto Camara Gomes, Florence Alberge, Pierre Duhamel, and Pablo Piantanida.
Igeood: An information geometry approach to out-of-distribution detection. arXiv preprint
_arXiv:2203.07798, 2022._

Matan Haroush, Tzviel Frostig, Ruth Heller, and Daniel Soudry. A statistical framework for efficient
out of distribution detection in deep neural networks, 2021.

Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield highconfidence predictions far away from the training data and how to mitigate the problem. 2019
_IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 41–50, 2019._

Ernst Hellinger. Neue begrundung der theorie quadratischer formen von unendlichvielen¨
ver¨anderlichen. Journal f¨ur die reine und angewandte Mathematik, 1909(136):210–271, 1909.

Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.

Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. In International Conference on Learning Representations, 2017.

Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-ofdistribution image without learning from out-of-distribution data. In Proceedings of the IEEE/CVF
_Conference on Computer Vision and Pattern Recognition, pp. 10951–10960, 2020._

Haiwen Huang, Zhihan Li, Lulu Wang, Sishuo Chen, Bin Dong, and Xinyu Zhou. Feature space
singularity for out-of-distribution detection. arXiv preprint arXiv:2011.14654, 2020.

Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional
shifts in the wild. ArXiv, abs/2110.00218, 2021.

Carel Jansen, Robert Schreuder, and Anneke Neijt. The influence of spelling conventions on perceived
plurality in compounds: A comparison of afrikaans and dutch. Written language & literacy, 10(2):
185–194, 2007.

Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.

John F Kelley. An iterative design methodology for user-friendly natural language office information
applications. ACM Transactions on Information Systems (TOIS), 2(1):26–41, 1984.

Polina Kirichenko, Pavel Izmailov, and Andrew G Wilson. Why normalizing flows fail to detect
out-of-distribution data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
_Advances in Neural Information Processing Systems, volume 33, pp. 20578–20589. Curran As-_
[sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/ecb9fe2fbb99c31f567e9823e884dbec-Paper.pdf)
[ecb9fe2fbb99c31f567e9823e884dbec-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/ecb9fe2fbb99c31f567e9823e884dbec-Paper.pdf)

Solomon Kullback. Information theory and statistics. Courier Corporation, 1954.

Solomon Kullback. Information Theory and Statistics. John Wiley, 1959.


-----

Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. Advances in neural information processing
_systems, 31, 2018a._

Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
_Systems 31, pp. 7167–7177. Curran Associates, Inc., 2018b._

Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
[out-of-distribution samples and adversarial attacks, 2018c. URL https://arxiv.org/abs/](https://arxiv.org/abs/1807.03888)
[1807.03888.](https://arxiv.org/abs/1807.03888)

Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A manually
labelled multi-turn dialogue dataset. In Proceedings of The 8th International Joint Conference on
_Natural Language Processing (IJCNLP 2017), 2017._

Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image
detection in neural networks. In International Conference on Learning Representations, 2018.
[URL https://openreview.net/forum?id=H1VGkIxRZ.](https://openreview.net/forum?id=H1VGkIxRZ)

Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection.
_Advances in Neural Information Processing Systems, 2020._

Yifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. Cider: Exploiting hyperspherical embeddings
for out-of-distribution detection. arXiv preprint arXiv:2203.04450, 2022.

Swaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva, Chris Bryan, and Chitta Baral. Dqi:
[Measuring data quality in nlp, 2020. URL https://arxiv.org/abs/2005.00816.](https://arxiv.org/abs/2005.00816)

Gregoire Montavon, Wojciech Samek, and Klaus-Robert M´ uller. Methods for interpreting and¨
understanding deep neural networks. Digital signal processing, 73:1–15, 2018.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the
_Association for Computational Linguistics, pp. 311–318, Philadelphia, Pennsylvania, USA,_
July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL
[https://aclanthology.org/P02-1040.](https://aclanthology.org/P02-1040)

Jitendra Parmar, Satyendra Singh Chouhan, Vaskar Raychoudhury, and Santosh Singh Rathore.
[Open-world machine learning: Applications, challenges, and opportunities, 2021. URL https:](https://arxiv.org/abs/2105.13448)
[//arxiv.org/abs/2105.13448.](https://arxiv.org/abs/2105.13448)

Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese,
Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv
_preprint arXiv:2202.03286, 2022._

Ben Peters, Vlad Niculae, and Andre FT Martins. Sparse sequence-to-sequence models.´ _arXiv_
_preprint arXiv:1905.05702, 2019._

Marine Picot, Francisco Messina, Malik Boudiaf, Fabrice Labeau, Ismail Ben Ayed, and Pablo
Piantanida. Adversarial robustness via fisher-rao regularization. IEEE Transactions on Pattern
_Analysis and Machine Intelligence, 2022._

Julianna Pinele, Joao E. Strapasson, and Sueli I. R. Costa. The fisher–rao distance between multivari-˜
ate normal distributions: Special cases, bounds and applications. Entropy, 22(4), 2020. ISSN 1099[4300. doi: 10.3390/e22040404. URL https://www.mdpi.com/1099-4300/22/4/404.](https://www.mdpi.com/1099-4300/22/4/404)

Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada
Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations. arXiv
_preprint arXiv:1810.02508, 2018._


-----

Igor M. Quintanilha, Roberto de M. E. Filho, Jose Lezama, Mauricio Delbracio, and Leonardo O.´
Nunes. Detecting out-of-distribution samples using low-order deep features statistics, 2019. URL
[https://openreview.net/forum?id=rkgpCoRctm.](https://openreview.net/forum?id=rkgpCoRctm)

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

C Radhakrishna Rao. Information and the accuracy attainable in the estimation of statistical parameters. In Breakthroughs in statistics, pp. 235–247. Springer, 1992.

Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint
_arXiv:2106.09022, 2021a._

Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan.
A simple fix to mahalanobis distance for improving near-ood detection, 2021b.

Cynthia Rudin and Joanna Radin. Why are we using black box models in ai when we don’t need to?
a lesson from an explainable ai competition. 2019.

Seonghan Ryu, Seokhwan Kim, Junhwi Choi, Hwanjo Yu, and Gary Geunbae Lee. Neural sentence
embedding using only in-domain sentences for out-of-domain sentence detection in dialog systems.
_Pattern Recognition Letters, 88:26–32, 2017._

Ivan N Sanov. On the probability of large deviations of random variables. United States Air Force,
Office of Scientific Research, 1958.

Chandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with Gram
matrices. In Hal Daume III and Aarti Singh (eds.),´ _Proceedings of the 37th International Conference_
_on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 8491–8501._
[PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/sastry20a.](https://proceedings.mlr.press/v119/sastry20a.html)
[html.](https://proceedings.mlr.press/v119/sastry20a.html)

Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy Ang, and Hannah Carvey. The icsi meeting
recorder dialog act (mrda) corpus. Technical report, INTERNATIONAL COMPUTER SCIENCE
INST BERKELEY CA, 2004.

Lucia Specia, Dhwaj Raj, and Marco Turchi. Machine translation evaluation versus quality estimation.
_Machine translation, 24(1):39–50, 2010._

Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky,
Paul Taylor, Rachel Martin, Marie Meteer, and Carol Van Ess-Dykema. Dialogue act modeling for
automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):
339–371, 2000.

Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations.
_ArXiv, abs/2111.12797, 2021._

Jorg Tiedemann. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani,
Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Eight International Conference on
_Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may 2012a. European Language_
Resources Association (ELRA). ISBN 978-2-9517408-7-7.

Jorg Tiedemann. The Tatoeba Translation Challenge – Realistic data sets for low resource and¨
multilingual MT. In Proceedings of the Fifth Conference on Machine Translation, pp. 1174–
[1182, Online, November 2020. Association for Computational Linguistics. URL https://www.](https://www.aclweb.org/anthology/2020.wmt-1.139)
[aclweb.org/anthology/2020.wmt-1.139.](https://www.aclweb.org/anthology/2020.wmt-1.139)


-----

Jorg Tiedemann and Santhosh Thottingal. OPUS-MT — Building open translation services for the¨
World. In Proceedings of the 22nd Annual Conferenec of the European Association for Machine
_Translation (EAMT), Lisbon, Portugal, 2020._

Jorg Tiedemann. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),¨
Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan
Odijk, and Stelios Piperidis (eds.), Proceedings of the Eight International Conference on Language
_Resources and Evaluation (LREC’12), Istanbul, Turkey, may 2012b. European Language Resources_
Association (ELRA). ISBN 978-2-9517408-7-7.

Samarth Tripathi, Sarthak Tripathi, and Homayoon Beigi. Multi-modal emotion recognition on
iemocap dataset using deep learning. arXiv preprint arXiv:1804.05788, 2018.

Inigo Jauregi Unanue, Jacob Parnell, and Massimo Piccardi. Berttune: Fine-tuning neural machine
[translation with bertscore, 2021. URL https://arxiv.org/abs/2106.02208.](https://arxiv.org/abs/2106.02208)

Sachin Vernekar, Ashish Gaurav, Vahdat Abdelzad, Taylor Denouden, Rick Salay, and Krzysztof
[Czarnecki. Out-of-distribution detection in classifiers via generation, 2019a. URL https:](https://arxiv.org/abs/1910.04241)
[//arxiv.org/abs/1910.04241.](https://arxiv.org/abs/1910.04241)

Sachin Vernekar, Ashish Gaurav, Taylor Denouden, Buu Phan, Vahdat Abdelzad, Rick Salay, and
Krzysztof Czarnecki. Analysis of confident-classifiers for out-of-distribution detection. arXiv
_preprint arXiv:1904.12220, 2019b._

Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text
[generation, 2021. URL https://arxiv.org/abs/2106.11520.](https://arxiv.org/abs/2106.11520)

Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong
Chen. Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking
baselines. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational
_AI, ACL 2020, pp. 109–117, 2020._

Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. Defending against neural fake news. Advances in neural information processing
_systems, 32, 2019._

Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted
gap-sentences for abstractive summarization. In International Conference on Machine Learning,
pp. 11328–11339. PMLR, 2020.

Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao,
Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational
response generation. arXiv preprint arXiv:1911.00536, 2019.

Zhi-Hua Zhou. Open-environment machine learning. National Science Review, 9(8):nwac123, 2022.

Qiuyu Zhu, Guohui Zheng, and Yingying Yan. Effective out-of-distribution detection in classifier
[based on pedcc-loss, 2022. URL https://arxiv.org/abs/2204.04665.](https://arxiv.org/abs/2204.04665)

Ev Zisselman and Aviv Tamar. Deep residual flow for out of distribution detection. In The IEEE
_Conference on Computer Vision and Pattern Recognition (CVPR), June 2020._


-----

### 7 APPENDIX

**A Experimental setting** **17**

A.1 Additionnal Details on Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

A.2 Language pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

A.3 Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

A.4 Dialog datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

A.5 Choices of models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

A.6 Generalization of existing OOD detectors to Sequence Generation . . . . . . . . . 18

**B** **Parameters tuning** **19**

B.1 Impact of α . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

**C Performance of our detectors in OOD detection** **20**

C.1 Summary of our results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

C.2 Detailed results of OOD detection performances . . . . . . . . . . . . . . . . . . . 20

C.3 ROC AUC curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

**D NTM performance** **24**

D.1 Absolute performances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

D.2 Gains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

D.3 Effect of a Larger threshold on NMT performance . . . . . . . . . . . . . . . . . . 27

**E** **Negative results** **27**

E.1 Different aggregation of OOD metrics . . . . . . . . . . . . . . . . . . . . . . . . 27

E.2 Negentropy of bag of distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 27

E.3 Different reference distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

E.4 Impact of additional finetuning on IN data . . . . . . . . . . . . . . . . . . . . . . 28


-----

### A EXPERIMENTAL SETTING

In this section we dive into the details and definitions of our experimental setting. First we present
our OOD detection performance metrics (Sec. A.1), then we provide a couple samples for one of the
small language shifts (Sec. A.3). We also discusse the choices of pretrained model (Sec. A.5) and
how we adapted common OOD detectors to the text generation case (Sec. A.6).

A.1 ADDITIONNAL DETAILS ON METRICS

OOD Detection is usually an unbalanced binary classification problem where the class of interest
is OUT. Let us denote Z the random variable corresponding to actually being out of distribution.
We can assess the performance of our OOD detectors focusing on the False alarm rate and on the
**True detection rate. The False alarm rate or False positive rate (FPR) is the proportion of samples**
missclassified as OUT. For a score threshold γ, we have FPR = Pr �a(x) > γ | Z = 0�. The True
**detection rate or True positive rate (TPR) is the proportion of OOD samples that are detected by the**
method. It is given by TPR = Pr �a(x) > γ _Z = 1�._
_|_

In order to evaluate the performance of our methods we will focus and report mainly the AUROC and
the FPR, we provide more detailed metrics and experiments in Sec. A.1.

**Area Under the Receiver Operating Characteristic curve (AUROC) Bradley (1997). The Receiver**
Operating Characteristic curve is curve obtained by plotting the True positive rate against the False
positive rate. The area under this curve is the probability that an in-distribution example Xin has
a anomaly score higher than an OOD sample xout: AUROC= Pr(a(xin) > a(xout)). It is given by
_γ_ (Pr �a(x) > γ _Z = 0�, Pr_ �a(x) > γ _Z = 1�)._
_�→_ _|_ _|_

**False Positive Rate at 95% True Positive Rate (FPR). We accept to allow only a given false**
positive rate r corresponding to a defined level of safety and we want to know what share of positive
samples we actually catch under this constraint. It leads to select a threshold γr such that the
corresponding TPR equals r. At this threshold, one then computes: Pr(a(x) > γr | Z = 0) with γr
s.t. TPR(γr) = r. r is chosen depending of the difficulty of task at hand and the required level of
safety.

For the sake of brevity we present only AUROCand FPRmetrics in our aggregated results but we also
used Detection error and Area Under the Precision-Recall curve metrics and those are presented in
our full results section (Ap. C).

**Detection error. It is simply the probability of miss-classification for a given True positive rate.**

**Area Under the Precision-Recall curve (AUPR-IN/AUPR-OUT) Davis & Goadrich (2006). The**
Precision-Recall curve plots the recall (true detection rate) against the precision (actual proportion
of OOD amongst the predicted OOD). The area under this curve γ �→ (Pr �Z = 1 | s(X) ⩽
_γ�, Pr_ �s(X) ⩽ _γ | Z = 1�) captures the trade-off between precision and recall made by the model._
A high value represents a high precision and a high recall i.e. the detector captures most of the
positive samples while having few False positive.

A.2 LANGUAGE PAIRS

**Model** **IN data** **OUT data**

**Language shift**

DE-EN Tatoeba DE News FR
DE-EN Tatoeba DE Tatoeba NLD

ES-EN Tatoeba ES News FR
ES-EN Tatoeba ES Tatoeba CAT
ES-EN Tatoeba ES Tatoeba POR

NLD-EN Tatoeba ES AFR

**Domain shift**

DE-EN Tatoeba DE EMEA DE
DE-EN Tatoeba DE Eurparl DE
ES-EN Tatoeba ES EMEA DE
ES-EN Tatoeba ES Eurparl DE

Table 6: Summary of models and studied shifts.


-----

A.3 SAMPLES

Source sentence Expected translation Translation BLEU

A en Tom li agrada la tecnologia. Tom likes technology. Tom li likes technology. 42.73
Ac´ı est`a la teua bossa. Here is your bag. Ac´ı est´a la teua bossa. 8.12
Aix`o et posar`a en perill. That’ll put you in danger. Aix`o et posar`a en perill. 8.12
A Londres hi han molts parcs bonics. There are many beautiful parks in London. To London hi han molts parcs bonics. 6.57
Aquest pa ´es molt delici´os. This bread is very delicious. Aquest pa ´es molt delici´os. 8.12
A tots els meus amics els agraden els videojocs. All my friends like playing videogames. A tots els meus amics els agrade els videojocs. 4.20
Ac¸`o ´es un peix. This is a fish. Aaaaaaaaaaaaaaaaaaaa ... aaaaaaaaaaaaaaaaaaaaaaaaaa 0.00
Moltes felicitats! Congratulations! Moltes congrats! 27.52
Bon any nou! Happy New Year! Bon any nou! 15.97
Aquell que menteix, robar`a. He that will lie, will steal. The one who’s mindless, he’ll steal. 12.22
Jo s´oc qui t´e la clau. I’m the one who has the key. Jo soc qui te la clau. 5.69
En Tom surt a treballar cada mat´ı a dos quarts de set. Tom leaves for work at 6:30 every morning. In Tom surt to pull each mat´ı to two quarts of set. 3.67
Ell m’ha dit que la seva casa era embruixada. He told me that his house was haunted. Ell m’ha dit that the seva house was haunted. 27.78
Aquest ´es el lloc on va n`eixer el meu pare. This is the place where my father was born. Aquest is the lloc on va n`eixer el meu pare. 8.30

Table 7: Example of behavior of a language model trained to handle Spanish inputs on Catalan inputs.

A.4 DIALOG DATASETS

**Switchboard Dialog Act Corpus (SwDA) is a corpus of telephonic conversations. The corpus**
provides labels, topic and speaker information (Stolcke et al., 2000).

**ICSI MRDA Corpus (MRDA) contains transcript 75h of naturally occuring meetings involving more**
than 50 people (Shriberg et al., 2004).

**DaylyDialog Act Corpus (DyDA) contains daily common communications between people, covering**
topic such as small talk, meteo or daily activities (Li et al., 2017).

**Interactive Emotional Dyadic Motion Capture IEMOCAP)(Tripathi et al., 2018) consists of**
transcripts of improvisations or scripted scenarii supposed to outline the expression of emotions.

A.5 CHOICES OF MODELS

A lot of pretrained model for conditional text generation are available. To perform our experiments
we needed models that were already well installed and deployed and that would also support OOD
settings. For translation tasks we needed specialized models for a notion of OOD to be easily defined.
It would be indeed more hazardous to define a notion of OOD language when working with a
multilingual model. The same is true for conversational models.

**Neural Machine Translation model. We benchmark our OOD method on translation models**
provided by Helsinky NLP Tiedemann & Thottingal (2020) on several pairs of languages with large
and small shifts. We extended the experiment to detect domain shifts. These models are indeed
specialized in each language pairs and are widely recognize in the neural machine translation field.
For our experiments we used the testing set provided along these models, so we can consider that
they have been fine tuned over the same distribution.

**Conversational model. We used a dialogGPT Zhang et al. (2019) model fine-tuned on the Multi**
WOZ dataset as chat bot model. The finetuning on daily dialogue type tasks ensure that the model is
specialized, thus allowing us to get a good definition of samples not being in its range of expertise.
Moreover, the choice of the architure, DialogGPT, guarantee that our results are valid on a very
common architecture.

**Additional finetuning. We further finetuned the models on the reference set to check whether**
additional finetuning on the distribution would affect the results. It did not change significantly the
results Tab. 17. It is not surprising considering that the models we used were already trained on a
very similar distribution.

A.6 GENERALIZATION OF EXISTING OOD DETECTORS TO SEQUENCE GENERATION

In this section, we extend classical OOD detection score to the conditional text generation settting.
Common OOD detectors were built for classification tasks and we need to adapt them to conditional
text generation. Our task can be viewed as a sequence of classification problems with a very large
number of classes (the size of the vocabulary). We chose the most naive approach which consists of


-----

averaging the OOD scores over the sequence. We experimented with other aggregation such as the
min/max or the standard deviation without getting interesting results.

**Likelihood Score The most naive approach to build a OOD score is to rely solely on the log-**
likelihood of the sequence. For a conditioning x we define the log-likelyhood score by aL(x) =
_−_ [�]t[|][y][ˆ]=0[|−][1] log pθ(ˆyt+1|x, ˆy⩽t). The likelihood is the same as the perplexity.

**Average Maximum Softmax Probability score The maximum softmax probability Hendrycks**
& Gimpel (2017) takes the probability of the mode of the categorical distribution as score of
OOD. We extend thise definition in the case of sequence of probability distribution by averaging
this score along the sequence. For a given conditioning x, we define the average MSP score
_aMSP(x) =_ _|y1ˆ|_ �|tyˆ=1| _i∈max[|0,K|][p]θ[T]_ [(][i][|][x][,][ ˆ][y][⩽][t][))][. While it is closely linked to uncertainty measures it]

discards most of the information contained in the probability distribution. It discards the whole
probability distribution. We claim that much more information can be retrieve by studying the whole
distribution.

**Average Energy score We extend the definition of the energy score described in Liu et al. (2020) to**
a sequence of probability distributions by averaging the score along the sequence. For a given
conditioning x and a temperature T we define the average energy of the sequence:aE(x) ≜
_−_ _|[T]yˆ|_ �t|yˆ=1| [log][ �]i[|][Ω][|] _e[f][θ][(][x][,][y][ˆ][⩽][t][)][i][/T]_ . It corresponds to the normalization term of the softmax function

applied on the logits. While it takes into account the whole distribution, it only takes into account the
amount of unormalized mass before normalization without attention to how this mass is distributed
along the features.

**Mahalanobis distance Following Lee et al. (2018c) compute the Mahalanobis matrice based on the**
samples of a given reference set . In our case we are using encoder-decoder models we use the
_R_
output of the last hidden layer of the encoder as embedding. Let’s denote ϕ(x) this embedding for a
conditionning x. Let’s µ and Σ be respectively the mean and the covariance of these embedding on
the reference set. We define aM(x) = �1 + (ϕ(x) − _µ)[⊤]Σ[−][1](ϕ(x) −_ _µ)�−1._

### B PARAMETERS TUNING

Detectors depend on their anomaly score to make decision and these scores can be parametric. First of
all, soft probability based scores depend on the soft probability distribution and its scaling, therefore
the temperature is a crucial parameter to tune to get the most performance. While a small temperature
tend to make the distribution more picky, higher value spread the probability mass along the classes.
Moreover, the renyi divergence and its related informaiton projection depend on a factor α. We
provide here further results and analysis of those parameters on our results.

B.1 IMPACT OF α

Indeed, in Fig. 1 we present the impact of the size of the reference set and of the paramet α on Renyi
information projection to distinguish dialog shifts, as expected, the larger the reference set, the better.
However, we see that smaller values of α yield better results.

We recall that the Renyi divergence is defined as Dα(p∥q) = _α−1_ 1 [log] ��|iΩ=1| _qi[α]p[α]i[−][1]_ �, where α ∈

R+ −{1}. Smaller values of α distribute the weight of each feature more equally in the final
divergence, more specifically they tend to give an equal weight to the very likely outcome as well as
to the less likely ones, therefore giving more weight to the tail of the distribution. When α tends to 0
the Renyi divergence actually counts the number of nonzero common probabilities. That makes sens
in terms of topic detection, it counts the common tokens considered during text generation.


-----

Task = Domain shifts

0.3 0.4 0.5 0.6 0.7 0.8 0.9

FPR


Task = Dialog shifts

0.3 0.4 0.5 0.6 0.7 0.8 0.9

FPR


0.95

0.90


Ours
Baselines


0.85

0.80


0.75

0.70


0.65


Task = Language shifts

0.3 0.4 0.5 0.6 0.7 0.8 0.9

FPR


Score

aD
aFR
aE
aMSP
aL
aD [*]

aFR [*]

aDKL[*]


aM


1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0


spa-cat

0.0 0.2 0.4 0.6 0.8 1.0


Figure 2: Trade-offs between AUROCand FPRfor each tasks and metrics

deu-fr deu-nl nld-afr


spa-fr

0.0 0.2 0.4 0.6 0.8 1.0


spa-por

0.0 0.2 0.4 0.6 0.8 1.0


Figure 3: ROCAUC curves for our uncertainty based metrics compared to common baselines for
language shifts detection. Baselines are represented in dashed lines.

### C PERFORMANCE OF OUR DETECTORS IN OOD DETECTION


C.1 SUMMARY OF OUR RESULTS

In Fig. 2 we present the different level of performance of all the detectors we studied. We can
see that in every task our detectors outperform the baselines but also that in dialog shift, while the
Mahalanobis distance outperform clearly our detectors for s0, they still outperform baselines for their
scenario by far.


C.2 DETAILED RESULTS OF OOD DETECTION PERFORMANCES

In this section we present the performances of our OOD detectors on each detailed tasks, i.e. for each
pair of IN and OOD data with all the considered metrics. We show that our metrics outperform other
OOD detectors baselines in almost all scenarios.


-----

Table 8: Detailed results of the performances of our OOD detectors on different language shifts. The
first language of the pair is the reference language of the model and the second one is the studied
shift.


AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ **1.000.980.980.800.99** **0.990.950.710.980.99** **1.000.980.780.990.99** **0.030.040.440.040.05** **0.910.900.690.900.91** **0.010.040.840.030.05** **0.830.750.830.830.83** **1.000.990.631.001.00**

_aaDDαKL∗[∗]_ **1.000.99** **1.001.00** **1.001.00** 0.030.03 **0.910.91** **0.000.01** **0.830.83** **1.001.00**
_aDαidf_ 0.99 0.98 0.99 0.06 0.90 0.06 **0.83** 0.99

s1 Ours _aaaaFRDDDαKLKLmean[mean]idf[∗]_ **1.001.001.000.99** **1.001.000.980.99** **1.001.000.981.00** **0.020.060.020.03** **0.910.900.910.91** **0.000.070.000.00** **0.830.830.830.83** **1.000.990.981.00**

_aaFRFRidf[mean]_ 0.570.99 0.580.97 0.590.99 0.470.03 0.380.90 0.900.00 0.590.83 0.280.99

Baselines _aaCM_ 0.760.79 0.820.86 0.810.82 0.340.25 0.420.68 0.640.44 0.560.75 0.340.62

(a) deu-fr

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ **0.860.860.700.680.68** **0.940.820.790.830.93** **0.910.770.750.730.90** **0.290.590.590.590.32** **0.770.630.520.520.77** **0.420.880.880.880.47** **0.700.630.350.520.70** **1.000.861.001.001.00**

_aaDDαKL∗[∗]_ 0.580.52 0.780.74 0.690.66 0.600.61 0.540.49 0.890.92 0.580.54 0.510.45
_aDαidf_ 0.59 0.77 0.70 0.59 0.54 0.89 0.58 0.50

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.480.680.560.55 0.690.670.760.80 0.600.620.680.76 0.640.660.600.60 0.430.510.520.62 0.960.990.890.90 0.500.560.560.62 0.370.470.490.61

_aaFRFRidf[mean]_ 0.350.51 0.650.63 0.500.55 0.630.66 0.270.00 0.950.99 0.370.00 0.210.00

Baselines _aaCM_ 0.550.52 0.770.82 0.670.71 0.570.53 0.530.53 0.850.79 0.550.57 0.570.49

(c) spa-cat

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFREMSPL_ _α_ **0.760.710.740.700.72** **0.900.880.880.880.88** **0.850.800.820.820.81** **0.510.530.550.580.55** 0.630.480.480.650.67 0.750.820.780.720.78 0.600.310.310.610.62 **1.000.661.001.001.00**

_aaDDαKL[∗]∗_ 0.710.66 0.940.94 0.860.87 0.230.22 0.650.69 0.320.30 0.610.64 0.690.75
_aDαidf_ 0.41 0.74 0.59 0.64 0.37 0.91 0.43 0.32

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.340.750.430.70 0.700.700.880.95 0.530.560.830.87 0.670.680.570.22 0.000.000.460.67 0.960.960.810.30 0.000.000.500.62 0.000.000.430.72

_aaFRFRidf[mean]_ 0.340.65 0.710.84 0.530.76 0.650.61 0.000.00 0.930.87 0.000.00 0.000.00

Baselines _aaCM_ 0.440.47 0.790.85 0.650.71 0.620.55 0.400.49 0.890.78 0.400.52 0.400.47

(e) nld-afr

C.3 ROC AUC CURVES

C.3.1 LANGUAGE SHIFTS


AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDLMSPFRE_ _α_ 0.940.800.770.820.95 **0.940.750.710.760.94** 0.770.740.950.790.94 0.460.450.160.450.17 0.670.670.870.710.87 0.880.850.280.850.29 0.500.700.820.770.82 0.921.001.001.001.00

_aaDDαKL∗[∗]_ 0.810.77 0.720.70 0.770.75 0.440.46 0.700.68 0.840.88 0.760.75 0.650.62
_aDαidf_ 0.80 0.74 0.78 0.43 0.69 0.81 0.76 0.64

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.710.860.770.79 0.590.580.720.77 0.670.700.770.83 0.500.520.450.42 0.580.660.690.76 0.950.990.840.80 0.710.750.750.78 0.500.590.630.73

_aaFRFRidf[mean]_ 0.510.60 0.510.44 0.510.48 0.500.52 0.310.00 0.951.00 0.520.00 0.220.00

Baselines _aaCM_ 0.670.74 0.670.84 0.700.82 0.440.30 0.630.76 0.850.56 0.710.79 0.600.74

(b) spa-por

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ **1.001.000.990.690.98** **1.001.000.960.720.98** **1.001.000.980.730.98** **0.030.030.040.410.05** **0.910.910.900.600.90** **0.000.030.760.040.01** **0.830.710.830.830.83** **1.000.521.001.001.00**

_aaDDαKL∗[∗]_ **1.000.94** 0.970.99 0.960.99 0.070.03 0.900.91 0.090.01 **0.830.83** 0.991.00
_aDαidf_ 0.97 0.98 0.98 0.08 0.90 0.11 **0.83** 0.98

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.971.001.000.99 0.971.000.990.99 0.971.000.990.99 0.090.020.030.03 0.900.910.900.91 0.140.000.020.01 **0.830.830.830.83** 0.970.991.001.00

_aaFRFRidf[mean]_ 0.570.99 0.560.97 0.580.99 0.480.04 0.390.90 0.920.02 0.590.83 0.980.29

Baselines _aaCM_ 0.670.88 0.720.95 0.720.92 0.430.12 0.000.89 0.820.19 0.000.83 0.960.00

(d) spa-fr

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ **1.000.990.980.760.98** **0.990.930.740.970.99** **0.990.970.780.980.99** **0.030.050.400.050.04** **0.910.900.700.900.91** **0.010.060.750.060.03** **0.830.750.830.830.83** **1.000.651.001.001.00**

_aaDDαKL∗[∗]_ **1.000.98** **1.000.99** **1.000.99** 0.030.03 0.900.91 0.020.00 **0.830.83** 0.991.00
_aDαidf_ 0.98 0.98 0.98 0.06 0.90 0.07 **0.83** 0.98

s1 Ours _aaaaDFRDDKLKLα[mean]idfmean[∗]_ 0.991.000.981.00 0.960.971.000.99 0.991.000.971.00 0.030.070.020.03 0.900.900.910.91 0.100.000.000.00 **0.830.830.830.83** 0.970.990.991.00

_aaFRFRidf[mean]_ 0.570.99 0.570.94 0.580.98 0.480.03 0.380.90 0.920.02 0.580.83 0.980.28

Baselines _aaCM_ 0.710.76 0.740.84 0.750.80 0.410.27 0.490.65 0.780.50 0.610.74 0.410.58

(f) deu-nl


1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0


spa-cat

0.0 0.2 0.4 0.6 0.8 1.0


deu-fr deu-nl nld-afr


spa-fr

0.0 0.2 0.4 0.6 0.8 1.0


spa-por

0.0 0.2 0.4 0.6 0.8 1.0


Figure 4: ROC-AUC curves for our reference based metrics compared to common baselines for
language shifts detection. Baselines are represented in dashed lines.


-----

Table 9: Detailed results of the performances of our OOD detectors on different domain shifts. For
Spanish (spa) and German (de), we present two domains shifts: Technical medical (EMEA) data and
legal parlementary texts (parl) against common language emboddied by the Tatoeba dataset (tat).


AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDEFRLMSPα_ **0.900.880.870.860.89** **0.760.750.730.730.76** **0.860.810.830.820.85** **0.430.460.490.480.44** 0.770.780.760.790.81 0.860.930.910.840.82 0.790.790.770.800.80 0.751.000.741.001.00

_aaDDαKL[∗]∗_ **0.900.89** 0.830.88 0.880.90 0.330.25 0.820.82 0.620.45 0.800.81 0.830.86
_aDαidf_ 0.79 0.68 0.76 0.46 0.68 0.88 0.75 0.61

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.790.880.870.85 0.680.730.730.70 0.750.840.830.80 0.470.470.480.50 0.660.780.770.74 0.890.890.910.94 0.750.790.790.77 0.590.770.750.72

_aaFRFRidf[mean]_ 0.520.86 0.530.67 0.530.80 0.490.52 0.320.75 0.930.98 0.530.78 0.220.72

Baselines _aaCM_ 0.880.87 0.890.90 **0.900.89** 0.250.22 0.000.81 0.460.39 0.000.80 0.000.81

(a) de:news-EMEA

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDEFRLMSPα_ **0.750.750.610.630.75** **0.750.750.650.580.75** **0.710.650.640.680.68** **0.410.450.510.450.46** 0.420.670.670.670.67 0.840.960.850.860.78 0.610.500.660.510.66 0.321.001.001.001.00

_aaDDαKL[∗]∗_ **0.690.66** **0.660.64** **0.680.68** 0.430.46 0.300.00 0.810.88 0.800.00 0.220.00
_aDαidf_ 0.61 0.59 0.62 0.48 0.28 0.90 0.51 0.20

s1 Ours _aaaaDDDFRKLαKL[mean]idfmean[∗]_ 0.590.650.650.63 0.580.650.650.64 0.600.660.680.68 0.480.450.450.45 0.000.000.000.00 0.900.860.860.85 0.000.000.000.00 0.000.000.000.00

_aaFRFRidf[mean]_ 0.510.64 0.520.64 0.510.67 0.490.45 0.000.00 0.930.86 0.000.00 0.000.00

Baselines _aaCM_ 0.520.58 0.610.66 0.620.59 0.470.41 0.000.40 0.890.78 0.000.52 0.000.33

(c) de:news-parl

C.3.2 DOMAIN SHIFTS


AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ **0.750.660.750.680.75** **0.750.670.750.610.75** **0.760.700.680.710.71** **0.410.440.490.440.45** 0.450.670.670.670.67 0.840.940.830.860.76 0.640.500.690.500.67 0.351.001.001.001.00

_aaDDαKL∗[∗]_ 0.660.67 0.650.63 0.680.67 0.440.48 0.000.00 0.840.90 0.830.00 0.000.00
_aDαidf_ 0.60 0.59 0.61 0.48 0.15 0.90 0.83 0.08

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.600.670.660.62 0.590.670.650.64 0.610.700.690.65 0.480.450.450.44 0.000.000.000.00 0.910.850.850.82 0.000.000.000.00 0.000.000.000.00

_aaFRFRidf[mean]_ 0.510.65 0.520.67 0.510.69 0.490.43 0.000.00 0.940.80 0.000.00 0.000.00

Baselines _aaCM_ 0.570.62 0.610.66 0.600.66 0.460.44 0.270.00 0.870.83 0.470.00 0.190.00

(b) es:news-parl

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ **0.920.890.900.860.90** **0.810.750.770.730.80** **0.890.850.860.820.87** **0.370.440.440.470.41** 0.790.800.760.810.83 0.820.830.890.770.70 0.800.800.770.800.81 0.781.000.741.001.00

_aaDDαKL∗[∗]_ 0.880.89 0.830.85 **0.880.88** **0.290.32** 0.810.81 0.590.54 0.800.80 0.820.82
_aDαidf_ 0.79 0.70 0.76 0.45 0.67 0.85 0.75 0.60

s1 Ours _aaaaDFRDDKLKLα[mean]idfmean[∗]_ 0.880.810.790.90 0.680.750.660.77 0.840.760.750.86 0.450.500.470.44 0.770.700.660.79 0.850.940.890.83 0.790.760.750.80 0.750.650.580.79

_aaFRFRidf[mean]_ 0.520.87 0.510.70 0.520.81 0.500.49 0.330.75 0.950.94 0.540.78 0.240.72

Baselines _aaCM_ 0.670.81 0.590.83 0.640.83 0.490.31 0.000.75 0.940.58 0.000.78 0.000.72

(d) es:news-EMEA


1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0


spa:tat-EMEA

0.0 0.2 0.4 0.6 0.8 1.0


de:news-EMEA de:news-parl


spa:tat-parl

0.0 0.2 0.4 0.6 0.8 1.0


Figure 5: ROC-AUC curves for our uncertainty based metrics compared to common baselines for
domain shifts detection. baselines are represented in dashed lines.


-----

Table 10: Detailed performance results of our OOD detectors on dialog shift against the Multi WOZ
dataset as reference set.


AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ **0.870.730.750.810.53** **0.870.810.750.790.64** **0.870.790.620.820.57** **0.310.340.400.390.42** 0.670.670.760.320.78 0.630.750.720.780.56 0.750.530.750.530.79 0.601.000.770.220.77

_aaDDαKL∗[∗]_ 0.690.66 0.690.68 0.720.70 0.430.44 0.600.56 0.810.82 0.720.70 0.520.47
_aDαidf_ 0.61 0.64 0.64 0.45 0.45 0.85 0.64 0.35

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.620.620.640.69 0.630.650.650.69 0.650.650.670.72 0.460.440.460.43 0.470.470.510.60 0.870.830.860.82 0.650.650.670.71 0.370.370.410.51

_aaFRFRidf[mean]_ 0.540.55 0.550.57 0.550.57 0.490.48 0.340.37 0.930.91 0.550.58 0.250.27

Baselines _aaCM_ 0.880.87 0.890.84 0.900.87 0.250.33 0.750.80 0.460.61 0.860.80 0.670.80

(a) dailydialog-default

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ 0.520.410.630.400.36 0.860.870.660.860.87 0.690.470.670.720.63 0.550.730.520.560.52 0.370.430.430.520.31 0.731.000.740.690.69 0.390.270.280.500.34 0.351.001.000.540.28

_aaDDαKL∗[∗]_ 0.690.42 0.930.84 0.850.67 0.400.63 0.630.43 0.530.85 0.560.44 0.750.43
_aDαidf_ 0.38 0.82 0.64 0.64 0.38 0.86 0.40 0.36

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.380.390.400.53 0.810.830.820.88 0.630.650.650.75 0.650.630.650.58 0.370.390.400.54 0.870.840.880.77 0.390.410.410.51 0.350.380.380.59

_aaFRFRidf[mean]_ 0.310.32 0.770.79 0.570.59 0.690.68 0.300.32 0.930.91 0.340.35 0.280.29

Baselines _aaCM_ 0.540.53 0.890.87 0.700.74 0.740.73 0.590.56 0.910.99 0.550.53 0.640.60

(c) silicone-melde

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ 0.710.680.750.690.61 0.740.730.750.480.74 0.570.710.720.720.66 0.500.360.360.390.36 0.670.670.630.570.00 1.000.740.670.670.66 0.500.530.730.700.00 1.001.000.550.480.00

_aaDDαKL∗[∗]_ 0.880.68 0.890.70 0.880.70 0.240.42 0.800.56 0.440.80 0.790.70 0.820.47
_aDαidf_ 0.63 0.65 0.65 0.45 0.48 0.85 0.66 0.38

s1 Ours _aaaaDDDFRKLαKLidf[mean][mean][∗]_ 0.630.640.650.79 0.640.670.660.80 0.650.670.670.80 0.450.430.440.36 0.480.490.510.71 0.850.820.840.68 0.650.660.670.76 0.380.390.410.66

_aaFRFRidf[mean]_ 0.550.57 0.560.59 0.560.59 0.480.47 0.360.38 0.920.89 0.570.58 0.260.28

BaselinesBaselines _aaCM_ 0.870.87 0.870.95 0.880.93 0.150.10 0.800.86 0.210.21 0.800.82 0.840.90

(e) silicone-swda

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ 0.420.410.610.530.35 0.900.760.900.910.89 0.580.680.720.630.72 0.800.610.570.570.56 0.400.350.450.380.33 1.000.760.710.700.70 0.340.230.350.400.32 0.481.000.410.500.35

_aaDDαKL∗[∗]_ 0.670.29 0.950.85 0.860.63 0.440.72 0.580.34 0.550.90 0.480.32 0.770.36
_aDαidf_ 0.27 0.84 0.60 0.72 0.30 0.90 0.29 0.31

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.270.270.280.41 0.840.850.850.90 0.590.610.610.73 0.700.720.710.61 0.290.300.300.46 0.880.890.890.77 0.280.290.290.41 0.290.310.310.52

_aaFRFRidf[mean]_ 0.230.24 0.820.83 0.540.56 0.740.73 0.230.24 0.920.91 0.230.25 0.220.24

BaselinesBaselines _aaCM_ 0.450.42 0.920.89 0.750.70 0.780.78 0.530.47 0.970.99 0.500.42 0.560.54

(g) silicone-iemocap


AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ 0.520.410.630.400.36 0.860.870.660.860.87 0.690.470.670.630.72 0.550.730.560.520.52 0.370.430.430.520.31 0.731.000.740.690.69 0.390.270.280.500.34 0.351.001.000.540.28

_aaDDαKL∗[∗]_ 0.690.42 0.930.84 0.850.67 0.400.63 0.630.43 0.530.85 0.560.44 0.750.43
_aDαidf_ 0.38 0.81 0.63 0.64 0.37 0.85 0.39 0.35

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.360.390.390.53 0.800.820.810.88 0.620.640.640.75 0.660.630.670.58 0.360.380.400.54 0.890.850.900.77 0.390.400.410.51 0.340.360.380.59

_aaFRFRidf[mean]_ 0.310.31 0.780.80 0.570.58 0.680.66 0.290.29 0.910.88 0.320.33 0.260.26

Baselines _aaCM_ 0.540.53 0.890.87 0.700.74 0.740.73 0.590.56 0.910.99 0.550.53 0.640.60

(b) silicone-melds

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ 0.790.750.910.700.68 0.730.750.650.800.73 0.700.640.640.810.85 0.390.450.430.360.49 0.490.670.540.780.73 0.740.840.800.940.68 0.660.610.690.760.78 0.391.000.450.810.69

_aaDDαKL∗[∗]_ 0.920.60 0.910.60 0.910.61 0.230.47 0.830.44 0.420.89 1.000.63 0.870.34
_aDαidf_ 0.58 0.58 0.59 0.48 0.39 0.90 0.59 0.29

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.580.590.590.75 0.570.590.580.74 0.590.600.590.75 0.480.470.480.41 0.410.410.410.65 0.910.900.900.77 0.610.610.610.74 0.310.310.310.58

_aaFRFRidf[mean]_ 0.550.56 0.550.59 0.560.59 0.490.47 0.370.39 0.920.89 0.580.59 0.270.29

BaselinesBaselines _aaCM_ 0.870.85 0.870.93 0.880.91 0.150.10 0.800.83 0.210.21 0.800.81 0.840.85

(d) silicone-dydae

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDFRELMSPα_ 0.790.750.910.700.68 0.730.750.650.800.73 0.700.640.640.810.85 0.390.450.430.360.49 0.490.670.540.780.73 0.740.840.800.940.68 0.660.610.690.760.78 0.391.000.450.810.69

_aaDDαKL∗[∗]_ 0.920.60 0.910.60 0.910.61 0.230.47 0.830.44 0.420.89 1.000.63 0.870.34
_aDαidf_ 0.58 0.58 0.59 0.48 0.40 0.90 0.60 0.30

s1 Ours _aaaaDDDFRKLαKLidf[mean][mean][∗]_ 0.570.590.580.75 0.560.590.570.74 0.580.600.590.75 0.480.470.490.41 0.400.410.410.65 0.920.900.920.77 0.600.610.610.74 0.300.310.310.58

_aaFRFRidf[mean]_ 0.540.55 0.560.58 0.560.58 0.490.47 0.350.38 0.920.89 0.560.58 0.260.28

BaselinesBaselines _aaCM_ 0.840.85 0.870.93 0.870.91 0.130.10 0.810.83 0.220.21 0.810.81 0.840.85

(f) silicone-dydada

AUPR-IN AUPR-OUT AUROC ERR f1 FPR precision recall
Scenario Score

s0 OursBaselines _aaaaaDLMSPFRE_ _α_ 0.710.720.620.690.75 0.740.480.760.770.75 0.570.730.740.750.70 0.500.360.370.330.32 0.670.580.670.600.00 1.000.670.680.600.59 0.500.710.600.720.00 1.000.491.000.520.00

_aaDDαKL∗[∗]_ 0.860.66 0.880.71 0.870.70 0.230.41 0.790.52 0.420.77 0.790.68 0.800.42
_aDαidf_ 0.61 0.66 0.64 0.43 0.45 0.82 0.63 0.34

s1 Ours _aaaaDDDFRKLαKLidfmean[mean][∗]_ 0.620.630.630.77 0.640.680.650.80 0.640.660.660.79 0.440.420.440.36 0.460.450.480.69 0.840.790.840.66 0.640.640.660.75 0.360.350.380.63

_aaFRFRidf[mean]_ 0.550.57 0.570.59 0.570.59 0.480.47 0.360.40 0.910.90 0.570.60 0.270.30

BaselinesBaselines _aaCM_ 0.870.88 0.920.96 0.940.94 0.130.10 0.890.88 0.220.21 0.820.82 0.650.94

(h) silicone-mrda


1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0


de:news-EMEA de:news-parl

spa:tat-EMEA spa:tat-parl


-----

C.3.3 DIALOG SHIFTS


1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0


meld_s

0.0 0.2 0.4 0.6 0.8 1.0


daily_dialog-default dyda_da iemocap


mrda

0.0 0.2 0.4 0.6 0.8 1.0


swda

0.0 0.2 0.4 0.6 0.8 1.0


Figure 7: ROC-AUC curves for our uncertainty based metrics compared to common baselines for
dialog shifts detection. baselines are represented in dashed lines.


1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0


meld_s

0.0 0.2 0.4 0.6 0.8 1.0


daily_dialog-default dyda_da iemocap


mrda

0.0 0.2 0.4 0.6 0.8 1.0


swda

0.0 0.2 0.4 0.6 0.8 1.0


Figure 8: Rocauc curves for our reference based metrics compared to common baselines for dialog
shift detection. baselines are represented in dashed lines.

### D NTM PERFORMANCE

Surprisingly we show that common OOD detectors tend to exclude samples that are well handled by
the model and keep some that are not leading to decreasing overall performance in terms of translation
metrics. Moreoever it seems this phenomenon is more dominant in reference based detectors. We
show that our uncertainty based detectors mostly avoir that downfall and provide good OOD detection
and improved translation performances.

D.1 ABSOLUTE PERFORMANCES

It is clear (somewhat expected) that NMT models do not perform as well on OOD data as we can
see in Tab. 11b. However, we find that our OOD detectors are able to remove most of the worst case
samples and keep enough well translated samples so that with correct filtering our method actually
allow the model to achieve somewhat acceptable BLEU scores.


-----

60

50

40

30

20

10

0


ALL IN OOD

Dataset


spa-cat spa-por nld-afr


Figure 9: Gain in translation performances when filtering OOD samples with our method on different
datasets and language pairs.

spa-cat spa-por nld-afr spa:tat-parl de:news-parl spa:tat-EMEA de:news-EMEA
Scenario Score


(a) IN

spa-cat spa-por nld-afr spa:tat-parl de:news-parl spa:tat-EMEA de:news-EMEA
Scenario Score


(b) OOD

spa-cat spa-por nld-afr spa:tat-parl de:news-parl spa:tat-EMEA de:news-EMEA
Scenario Score


(c) ALL

Table 11: Absolue translation performances in terms of BLEU on the different subset (IN, OOD,
ALL) of each dataset of our translation OOD performance benchmark.


-----

spa-cat spa-por nld-afr spa:tat-parl de:news-parl spa:tat-EMEA de:news-EMEA
Scenario Score

Ours _aDα_ +2.75 +2.75 +5.24 +5.19 +2.61 +4.13 +2.62

_aFR_ +3.11 +3.11 +3.70 +4.02 +2.03 +3.11 +2.11

s0 _aE_ +2.93 +2.93 +3.34 +2.93 +1.79 +2.93 +1.79

Baselines _aL_ +5.03 +5.03 +5.41 +5.03 +2.65 +5.03 +2.65

_aMSP_ -1.10 -1.10 -1.60 -1.10 -1.07 -1.10 -1.07

Ours _aDα∗_ +0.70 +0.70 +0.55 +0.70 -0.26 +0.70 -0.26
s1 Baselines _aaFRC_ _[∗]_ +0.74+1.05 +0.74+1.05 +0.54-0.15 +0.74+1.05 +0.40-0.29 +0.74+1.05 +0.40-0.29

_aM_ +0.14 +0.14 +0.17 +0.14 -0.22 +0.14 -0.22

(a) IN

spa-cat spa-por nld-afr spa:tat-parl de:news-parl spa:tat-EMEA de:news-EMEA
Scenario Score

Ours _aDα_ +11.49 +21.48 +12.02 +8.52 +5.28 +4.74 +2.44

_aFR_ +10.53 +21.59 +9.22 +5.00 +3.75 +5.87 +2.85

s0 _aE_ -1.30 -2.42 +3.92 +1.89 +3.61 +1.12 +2.31

Baselines _aL_ +0.89 -0.70 +9.80 +7.06 +5.82 +1.94 +2.24

_aMSP_ +1.63 +3.57 +0.12 -2.36 -1.90 -17.87 -17.52

Ours _aDα∗_ +3.94 +7.41 +9.93 +0.88 -0.98 -13.13 -8.54
s1 Baselines _aaCFR[∗]_ +3.99+1.11 +7.49+1.32 +10.03-0.12 +0.95+0.14 +0.75-0.97 -13.56+0.17 +0.02-8.56

_aM_ +0.82 +3.90 +1.36 -2.43 -1.14 -0.22 +2.55

(b) OOD

spa-cat spa-por nld-afr spa:tat-parl de:news-parl spa:tat-EMEA de:news-EMEA
Scenario Score

Ours _aDα_ +15.28 +22.58 +13.09 +9.97 +4.14 +4.43 +3.20

_aFR_ +15.26 +22.93 +10.42 +6.37 +3.13 +3.84 +2.99

s0 _aE_ +4.51 +3.01 +8.44 +2.32 +2.98 +1.92 +2.66

Baselines _aL_ +10.13 +10.98 +12.26 +8.31 +4.42 +3.36 +3.10

_aMSP_ +2.38 +5.06 -1.70 -2.24 -1.72 -3.40 -9.74

Ours _aDα∗_ +5.48 +10.78 +8.31 +10.52 -0.85 -0.04 -6.20
s1 Baselines _aaCFR[∗]_ +5.57+3.74 +10.83+4.51 +8.27+0.58 +10.58-0.49 +0.56-0.85 +0.57-0.02 +0.72-6.23

_aM_ +4.46 +11.87 +3.46 +3.16 -0.79 +0.28 -5.26

(c) ALL

spa-cat spa-por nld-afr spa:tat-parl de:news-parl spa:tat-EMEA de:news-EMEA
Dataset ALL IN OOD ALL IN OOD ALL IN OOD ALL IN OOD ALL IN OOD ALL IN OOD ALL IN OOD
Scenario Score

Ours _aaDFRα_ 43%43% 20%20% 86%85% 56%56% 20%20% 92%92% 37%32% 20%16% 75%66% 37%28% 20%18% 54%39% 29%28% 20%20% 38%37% 20%48% 19%20% 20%76% 14%15% 20%19% 10%7%

s0 _aE_ 27% 20% 40% 28% 20% 36% 33% 20% 62% 19% 20% 19% 30% 20% 40% 12% 20% 4% 14% 20% 9%

Baselines _aL_ 33% 19% 59% 40% 19% 61% 35% 19% 69% 32% 19% 44% 30% 19% 40% 13% 19% 6% 13% 19% 6%

_aMSP_ 26% 19% 38% 32% 19% 45% 14% 16% 9% 15% 19% 12% 14% 20% 8% 52% 19% 86% 48% 20% 76%

s1 OursBaselines _aaaaDFRCMα∗_ _[∗]_ 29%30%23%30% 19%20%16%20% 47%48%37%49% 41%41%27%46% 19%20%16%20% 62%62%38%72% 36%36%17%28% 20%20%15%20% 71%71%22%47% 54%54%38%9% 19%20%16%20% 88%88%56%2% 12%12%10%17% 19%18%11%20% 10%14%6%6% 57%57%58%9% 19%20%16%20% 94%94%96%2% 40%40%50%6% 19%18%11%20% 62%62%81%0%

Table 13: Share of the datasets removed when taking γ so that we keep 80% of the IN distribution.


-----

D.2 GAINS

D.3 EFFECT OF A LARGER THRESHOLD ON NMT PERFORMANCE

Table 15: Detailed impacts on NMT performance results per tasks (Domain- or Language-shifts) of
the different OOD detectors with a threshold defined to keep 99% of the IN data. We present results
on the different part of the data: IN data, OOD data and the combination of both, ALL. For each
we report the absolute average BLEU score (Abs.), the average gains in BLEU (G.s.) compared to a
setting without OOD filtering (fθ only) and the share of the subset removed by the detector (R.Sh.).

Domain shifts Language shifts
IN OOD ALL IN OOD ALL
Abs. G.s R.Sh Abs. G.s R.Sh Abs. G.s R.Sh Abs. G.s R.Sh Abs. G.s R.Sh Abs. G.s R.Sh

47.1 +0.0 0.0% 43.4 +0.0 0.0% 45.3 +0.0 0.0% 60.5 +0.0 0.0% 18.1 +0.0 0.0% 43.9 +0.0 0.0%

Ours _aDα_ 47.3 +0.2 1.0% 44.2 +0.8 5.5% 45.8 +0.5 3.2% 60.7 +0.2 1.0% 21.8 +3.7 34.5% 49.2 +5.4 14.6%

_aFR_ 47.3 +0.2 1.0% 44.1 +0.7 7.2% 45.7 +0.4 4.1% 60.7 +0.2 1.0% 22.3 +4.2 37.0% 49.7 +5.9 15.8%

s0 _aE_ 47.3 +0.2 1.0% 44.0 +0.5 1.9% 45.6 +0.4 1.4% 60.9 +0.4 1.0% 18.7 +0.6 17.6% 46.0 +2.1 7.3%

Bas. _aL_ 47.3 +0.2 0.9% 44.0 +0.6 1.9% 45.6 +0.4 1.4% 60.8 +0.3 0.9% 19.1 +0.9 18.4% 46.2 +2.3 7.6%

_aMSP_ 47.0 -0.1 1.0% 40.3 -3.1 14.7% 43.5 -1.8 7.8% 60.4 -0.1 1.0% 18.5 +0.3 4.3% 44.3 +0.5 2.5%

Ours _aDα∗_ 47.0 -0.1 0.9% 40.3 -3.1 26.5% 43.9 -1.4 13.7% 60.5 -0.0 0.9% 19.3 +1.1 10.5% 45.3 +1.4 4.8%
s1 _aFR[∗]_ 47.0 -0.1 0.9% 40.3 -3.1 26.6% 43.9 -1.3 13.8% 60.5 -0.0 0.9% 19.3 +1.1 10.5% 45.3 +1.4 4.8%

Bas. _aM_ 47.0 -0.1 1.0% 41.6 -1.8 18.1% 44.6 -0.7 9.6% 60.5 -0.0 1.0% 18.4 +0.3 12.1% 45.3 +1.4 5.9%

### E NEGATIVE RESULTS

E.1 DIFFERENT AGGREGATION OF OOD METRICS


Most of our detectors are initially classification
OOD detectors that we adapted for text generation by averaging them over the generated
sequences and using this aggregated score as a
score for the whole sequence. We experimented
with other aggregations such as the standard deviation or the min/max along the sequence. If
the standard deviation gave relatively good results they were still less interesting that the naive
average.

E.2 NEGENTROPY
OF BAG OF DISTRIBUTIONS


Table 14: Correlation between OOD scores and
translation metrics BLEU and BERT-S

Bertscore f1 Bleu score
ALL IN OUT ALL IN OUT
Scenario Score

Ours _aDα_ -0.49 -0.33 -0.53 -0.37 -0.27 -0.38

_aFR_ -0.47 -0.33 -0.45 -0.38 -0.28 -0.37

s0 _aE_ -0.19 -0.30 0.00 -0.16 -0.21 0.00

Baselines _aL_ -0.22 -0.26 0.23 -0.30 -0.29 -0.06

_aMSP_ -0.28 -0.15 -0.43 -0.14 -0.09 -0.14

Ours _aDα∗_ -0.35 -0.22 -0.65 -0.24 -0.18 -0.35
s1 Baselines _aaFRC_ _[∗]_ -0.34-0.12 -0.22-0.07 -0.64-0.05 -0.24-0.11 -0.18-0.06 -0.37-0.12

_aM_ -0.23 -0.14 -0.29 -0.11 -0.06 -0.05


We introduced in Sec. 3.3 the bag of distributions as a way to aggregate a sequence of probability distribution and compare it to a set of reference
using information projections Sec. 3.3. A natural idea would be to apply the Negentropy methods
(Sec. 3.2) to these aggregated distributions.

More formally given a sequence of probability distribution Sθ(x) = {p[T]θ [(][x][,][ ˆ][y][⩽][t][)][}]t[n]=1 [we would]
compute its bag of distributions:


_p¯θ(x) ≜_ [1]

_y_
_|_ _|_


_|y|_
�

_pθ(x, y⩽t)_ (7)
_t=1_


And then compute as novelty score:

_JD(p) = D(p∥U_ ) (8)

Further experiments have shown that this process was unable to discriminate OOD samples or
improve performance translation. We suspect that the uncertainty at each step is key to capture
the behavior of the language model and that this uncertainty information is lost when averaging
probability distribution along the sequence.


-----

E.3 DIFFERENT REFERENCE DISTRIBUTIONS

In the no-reference scenario we used the uniform distribution as reference distribution to compare
against. However, we can obviously use other reference distributions. We tried two natural options:
the tf-idf Yuan et al. (2021) distribution and the average distribution on the reference set. The latter
effectively replacing the projection onto the reference set by the distance to the average element of it.

It is worth signaling that these methods falls into the reference scenario since we need it to compute
these statistics. They would be interesting though if they could maintain performance while being
less computationally expensive than the projection.

We found out that these references were not as efficient as the projection onto the reference set and
did not achieve better performance than their no reference counterparts.

Language shifts Domain shifts Dialog shifts
AUROC FPR F1 AUROC FPR F1 AUROC FPR F1

Ours _aDα_ **0.95** **0.25** **0.84** **0.85** **0.62** **0.75** **0.79** **0.64** **0.66**

_aFR_ 0.93 0.28 0.83 0.74 0.87 0.60 0.72 0.70 0.64

s0 _aE_ 0.89 0.44 0.77 0.76 0.78 0.71 0.65 0.76 0.57

Bas. _aMSP_ 0.87 0.44 0.75 0.78 0.77 0.71 0.66 0.72 0.21
_aL_ 0.78 0.79 0.50 0.72 0.89 0.65 0.65 0.95 0.62

_aDα∗_ 0.88 0.34 0.71 **0.86** **0.50** **0.70** **0.86** **0.52** **0.59**
s1 Ours _aaDFRα[mean][∗]_ 0.840.88 0.410.35 0.690.62 0.810.68 0.690.84 0.690.67 0.760.64 0.750.86 0.380.57
_aDαidf_ 0.82 0.48 0.62 0.66 0.87 0.67 0.63 0.87 0.57
_aFR[mean]_ 0.72 0.72 0.62 0.53 0.94 0.66 0.58 0.90 0.57
_aFRidf_ 0.67 0.81 0.61 0.52 0.94 0.66 0.56 0.92 0.57

Bas. _aM_ **0.92** **0.26** **0.73** 0.78 0.59 0.40 0.84 0.55 0.56
_aC_ 0.71 0.80 0.62 0.68 0.76 0.67 0.72 0.61 0.48

Table 16: Summary of the results including different custom reference distributions.

E.4 IMPACT OF ADDITIONAL FINETUNING ON IN DATA

Language shifts Domain shifts Dialog shifts
AUROC FPR F1 AUROC FPR F1 AUROC FPR F1

Ours _aDα_ **0.94** **0.26** **0.80** **0.86** **0.65** **0.70** **0.79** **0.70** **0.67**

_aFR_ 0.92 0.24 0.78 0.72 0.88 0.67 0.72 **0.70** 0.65

s0 _aE_ 0.89 0.44 0.72 0.76 0.85 0.67 0.65 0.76 0.59

Bas. _aMSP_ 0.87 0.44 0.73 0.78 0.83 0.67 0.66 0.72 0.56
_aL_ 0.78 0.79 0.61 0.74 0.92 0.66 0.65 0.95 0.51

_aDα∗_ 0.89 0.34 0.60 **0.87** **0.51** **0.71** **0.85** **0.52** **0.59**
_aFR[∗]_ 0.86 0.37 0.61 0.65 0.93 0.64 0.77 0.77 0.58
_aDαmean_ 0.84 0.41 0.61 0.68 0.84 0.69 0.64 0.86 0.55

Ours _aaDFRαidf[mean]_ 0.820.72 0.480.72 0.610.62 0.660.53 0.870.94 0.690.66 0.630.58 0.870.94 0.550.57
s1 _aFRidf_ 0.62 0.85 0.67 0.55 0.96 0.58 0.56 0.93 0.57

Bas. _aM_ **0.91** **0.28** **0.60** 0.71 0.76 0.57 0.78 0.55 0.57
_aC_ 0.70 0.80 0.63 0.65 0.76 0.64 0.71 0.61 0.48

|a 0.95 0.25 0.84 Ours Dα a 0.93 0.28 0.83 FR s a 0.89 0.44 0.77 0 E Bas. a 0.87 0.44 0.75 MSP a 0.78 0.79 0.50 L|0.85 0.62 0.75 0.74 0.87 0.60|0.79 0.64 0.66 0.72 0.70 0.64|
|---|---|---|
||0.76 0.78 0.71 0.78 0.77 0.71 0.72 0.89 0.65|0.65 0.76 0.57 0.66 0.72 0.21 0.65 0.95 0.62|
|a 0.88 0.34 0.71 D α∗ Ours a 0.88 0.35 0.69 s FR∗ 1 a 0.84 0.41 0.62 Dmean α a 0.82 0.48 0.62 Didf α a FRmean 0.72 0.72 0.62 a 0.67 0.81 0.61 FRidf a 0.92 0.26 0.73 Bas. M a 0.71 0.80 0.62 C|0.86 0.50 0.70 0.81 0.69 0.69 0.68 0.84 0.67 0.66 0.87 0.67 0.53 0.94 0.66 0.52 0.94 0.66|0.86 0.52 0.59 0.76 0.75 0.38 0.64 0.86 0.57 0.63 0.87 0.57 0.58 0.90 0.57 0.56 0.92 0.57|
||0.78 0.59 0.40 0.68 0.76 0.67|0.84 0.55 0.56 0.72 0.61 0.48|


Table 17: Summary of the results with additional finetuning of the models on the reference set. The
results are similar to the results without finetuning as expected since the models had been trained
initially on similar distributions.


-----


# CLOSING THE GAP BETWEEN SVRG AND TD-SVRG
## WITH GRADIENT SPLITTING

**Anonymous authors**
Paper under double-blind review

### ABSTRACT

Temporal difference (TD) learning is a simple algorithm for policy evaluation
in reinforcement learning. The performance of TD learning is affected by high
variance and it can be naturally enhanced with variance reduction techniques, such
as the Stochastic Variance Reduced Gradient (SVRG) method. Recently, multiple
works have sought to fuse TD learning with SVRG to obtain a policy evaluation
method with a geometric rate of convergence. However, the resulting convergence
rate is significantly weaker than what is achieved by SVRG in the setting of convex
optimization. In this work we utilize a recent interpretation of TD-learning as the
splitting of the gradient of an appropriately chosen function, thus simplifying the
algorithm and fusing TD with SVRG. We prove a geometric convergence bound
with predetermined learning rate of 1/8, that is identical to the convergence bound
available for SVRG in the convex setting.

### 1 INTRODUCTION

Reinforcement learning (RL) is a learning paradigm which addresses a class of problems in sequential decision making environments. Policy evaluation is one of those problems, which consists of
determining expected reward agent will achieve if it chooses actions according to stationary policy.
Temporal Difference learning (TD learning, Sutton (1988)) is popular algorithm, since it is simple
and might be performed online on single samples or small mini-batches. TD learning method uses
Bellman equation to bootstrap the estimation process update the value function from each incoming
sample or minibatch. As all methods in RL, TD learning from the “curse of dimensionality” when
number of states is large. To address this issue, in practice linear or nonlinear feature approximation
of state values is often used.

Despite its simple formulation, theoretical analysis of approximate TD learning is subtle. There are
few important milestones in this process, one of which is a work of Tsitsiklis & Van Roy (1997),
in which asymptotic convergence guarantees were established. More recently advances were made
by Bhandari et al. (2018), Srikant & Ying (2019) and Liu & Olshevsky (2020). In particular, the
last paper shows that TD learning might be viewed as an example of gradient splitting, a process
analogous to gradient descent.

TD-learning has inherent variance problem, which is that the variance of the update does not go to
zero as the method converges. This problem is also present in a class of convex optimization problems where target function is represented as a sum of functions and SGD-type methods are applied
Robbins & Monro (1951). Such methods proceed incrementally by sampling a single function,
or a minibatch of functions, to use for stochastic gradient evaluations. A few variance reduction
techniques were developed to address this problem and make convergence faster, including SAG
Schmidt et al. (2013), SVRG Johnson & Zhang (2013) and SAGA Defazio et al. (2014). These
methods are collectively known as variance-reduced gradient methods. The distinguishing feature
of these methods is that they converge geometrically.

The first attempt to adapt variance reduction to TD learning with online sampling was done by
Korda & La (2015). Their results were discussed by Dalal et al. (2018) and Narayanan & Szepesv´ari
(2017); Xu et al. (2020) performed reanalysis of their results and shown geometric convergence for
Variance Reduction Temporal Difference learning (VRTD) algorithm for both Markovian and i.i.d
sampling. The work of Du et al. (2017) directly apply SVRG and SAGA to a version of policy


-----

evaluations by transforming it into an equivalent convex-concave saddle-point problem. Since their
algorithm uses two sets of parameters, in this paper we call it Primal Dual SVRG or PD SVRG.

All these results obtained geometric convergence of the algorithms, improving the sub-geometric
convergence of the standard TD methods. However, the convergence rates obtained in these papers
are significantly worse than the convergence of SVRG in convex setting. In particular, the resulting convergence times for policy evaluations scaled with the square of the condition number, as
opposed to SVRG which retains the linear scaling with the condition number of SGD. Quadratic
scaling makes practical application of theoretically obtain values almost unfeasible, since number
of computations becomes very large even for simple problems. Moreover, the convergence time
bounds contained additional terms coming from the condition number of a matrix that diagonalizes
some of the matrices appearing the problem formulations, which can be arbitrarily large.

In this paper we analyze the convergence of the SVRG technique applied to TD (TD-SVRG) in two
settings: piq a pre-sampled trajectory of the Markov Decision Process (MDP) (finite sampling), and
piiq when states are sampled directly from the MDP (online sampling). Our contribution is threefold:

    - For the finite sample case we achieve significantly better results with simpler analysis. We
are first to show that TD-SVRG has the same convergence rate as SVRG in the convex
optimization setting with a pre-determined learning rate of 1/8.

    - For i.i.d. online sampling, we similarly achieve better results with simpler analysis. Similarly, we are first to show that TD-SVRG has the same convergence rate as SVRG in
the convex optimization setting with a predetermined learning rate of 1/8. In addition,
for Markovian online sampling, we provide convergence guarantees that in most cases are
better than state-of-the art results.

    - We are the first to develop theoretical guarantees for an algorithm that can be directly
applied to practice. In previous works, batch sizes required to guarantee convergence were
very large that made them impractical (see Subsection H.1) and grid search was needed to
optimize the learning rate and batch size values. We include experiments that show our
theoretically obtained batch size and learning rate can be applied in practice and achieve
geometric convergence.

### 2 PROBLEM FORMULATION

We consider a discounted reward Markov Decision Process (MDP) pS, A, P, r, γq, where S is a state
space, A is an action space, P “ Pps[1]|s, aqs,s1PS,aPA are the transition probabilities, r “ rps, s[1]q
are the rewards and γ P r0, 1q is a discount factor. In this MDP agent follows policy π, which is
a mapping π : S [Ś] _A Ñ r0, 1s. Given that policy is fixed, for the remainder of the paper we will_
consider transition matrix P, such that: P ps, s[1]q “ [ř]a _[π][p][s, a][q][P][p][s][1][|][s, a][q][.][ We assume, that Markov]_

process produced by transition matrix is irreducible and aperiodic with stationary distribution µπ.

The policy evaluation problem is to computeis the value function, formally defined to be the unique vector which satisfies the equality V _[π], defined as: V_ _[π]psq :“ E_ “ř8t“0 _[γ][t][r][t][`][1]‰_ _. T Here[π]V_ _[π] V“[π]_
_V_ _[π], where T_ _[π]_ is a Bellman operator, defined as: T _[π]V_ _[π]psq “_ [ř]s[1][ P] [p][s, s][1][q p][r][p][s, s][1][q `][ γV][ π][p][s][1][qq][ .]

The TD(0) method is defined as follows: one iteration performs a fixed point update on randomly
sampled pair of states s, s[1] with learning rate η: V psq Ð V psq` _ηprps, s[1]q`_ _γV ps[1]q´_ _V psqq. When_
the state space size |S| is large, tabular methods which update a value for every state V psq become
impractical. For this reason linear approximation is often used. Each state a represented as feature
vector ϕpsq P R[d] and state value V _[π]psq is approximated by V_ _[π]psq « ϕpsq[T]_ _θ, where θ is a tunable_
parameter vector. Now a single TD update on randomly sampled transition s, s[1] becomes:


_θ_ Ð _θ ` ηgs,s1pθq_

“ _θ ` ηpprps, s[1]q ` γϕps[1]q[T]_ _θ ´ ϕpsq[T]_ _θqϕpsqq,_

where the second equation should be viewed as a definition of gs,s1pθq.

Our goal is to find parameter vector θ[˚] such that average update vector is zero

Es,s1rgs,s1pθ[˚]qs “ 0.

This expectation is also called mean-path update ¯gpθq and can be written as:


-----

Table 1: Algorithms parameter comparison. PD SVRG and PD SAGA results reported from Du
et al. (2017), VRTD and TD results from Xu et al. (2020), GTD2 from Touati et al. (2018). λminpQq
and κpQq are used to define, respectively, minimum eigenvalue and condition number of matrix Q.
_λA in this table denotes minimum eigenvalue of matrix 1{2pA ` A[T]_ q. Other notation is taken from
original papers. For simplicity 1 ` γ is upper bounded by 2.

Setting Method Learning rate Batch size Total complexity

GTD2 8σ[2]p9k[2]`ˆ22q`σ 9[2]ζ 1 _Op_ _λ[κ]min[p][Q][q]p[2]G[H]q[d]ϵ_ [q]

Finite sample PD SVRG _λminpA[T]_ _C[´][1]Aq_ 51κ[2]pCqL[2]G _κ[2]pCqL[2]G_

48κpCqL[2]G _λminpA[T]_ _C[´][1]Aq[2]_ _Op_ _λminpA[T]_ _C[´][1]Aq[2][ log][p][ 1]ϵ_ [qq]

PD SAGA _λminpA[T]_ _C[´][1]Aq_ 1 _κ[2]pCqL[2]G_

3p8κC[2]L[2]G[`][nµ][ρ][q] _Op_ _λminpA[T]_ _C[´][1]Aq[2][ log][p][ 1]ϵ_ [qq]

This paper 1/8 16{λA _Op_ _λ[1]A_ [log][p][ 1]ϵ [qq]

TD minp _[λ]16[A]_ _[,]_ _λ2A_ [q] 1 _Op_ _ϵλ1[2]A_ [log][p][ 1]ϵ [qq]

i.i.d sampling

VRDT _λA{64_ 132λ[2]A _Opmaxp_ [1]ϵ _[,]_ _λ1[2]A_ [q][ log][p][ 1]ϵ [qq]

This paper 1/8 16{λA _Opmaxp_ [1]ϵ _[,]_ _λ1A_ [q][ log][p][ 1]ϵ [qq]

_g¯pθq_ “ Es,s1rgs,s1pθqs

“ Es,s1rpγϕps[1]q[T] _θ ´ ϕpsq[T]_ _θqϕpsqs ` Es,s1_ [“]rps, s[1]qϕpsq‰

:“ ´Aθ ` b,

|Setting|Method|Learning rate|Batch size|Total complexity|
|---|---|---|---|---|
|Finite sample|GTD2|92 ˆ2σ 8σ2pk`2q`92ζ|1|λκ mpQ inq p2 GH qd Op ϵq|
||PD SVRG|λminpAT C´1Aq 48κpCqL2 G|51κ2 pCqL2 G λminpAT C´1Aq2|Op λminκ p2 ApC T CqL ´2 G 1Aq2 logp 1 ϵ qq|
||PD SAGA|λminpAT C´1Aq 3p8κC2L2 G`nµρq|1|Op λminκ p2 ApC T CqL ´2 G 1Aq2 logp 1 ϵ qq|
||This paper|1/8|16{λ A|Op λ1 logp 1 qq A ϵ|
|i.i.d sampling|TD|minp λ 1A, λ2 q 6 A|1|Op ϵλ1 2 logp 1 ϵ qq A|
||VRDT|λ A{64|132 λ2 A|Opmaxp 1 ϵ, λ1 2 q logp 1 ϵ qq A|
||This paper|1/8|16{λ A|Opmaxp 1, λ1 q logp 1 qq ϵ A ϵ|


where the last line should be taken as the definition of A and b. Finally, the minimum eigenvalue of
matrix pA ` A[T] q{2 plays an important role in our analysis and will be denoted as λmin.

There are few possible setting of the problem: the samples s, s[1] might be drawn from the MDP
on-line (Markovian sampling) or independently (i.i.d. sampling): first state s is drawn from µπ,
then s[1] is drawn from correspondent row of P . The latter case analysis is covered in 6. Another possible setting for analysis is the ”finite sample set” setting, in which states a data set
_D “ tpst, at, rt, st`1qut[N]“1_ [of size][ N][ is drawn ahead of time following Markov sampling, and]
TD(0) proceeds by drawing samples from this data set. We analyze this case in Sections 4 and 5.

We make following standard assumptions:

**Assumption 1. The matrix A is non-singular.**

**Assumption 2. ||ϕpsq||2 ď 1 for all s P S.**

Assumption 1 needed to guarantee that A[´][1]b exists and the problem is solvable. Assumption 2 is
introduced for simplicity, it always might be fulfilled by rescaling feature matrix.

2.1 KEY IDEA OF THE ANALYSIS

In our analysis we often use function f pθq, defined as:

_f_ pθq “ pθ ´ θ[˚]q[T] _Apθ ´ θ[˚]q._ (1)


-----

Function f pθq is a key characteristic function of TD learning. In their paper Liu & Olshevsky
(2020) introduce function f pθq as p1 ´ γq||Vθ ´ Vθ˚||D[2] [`][ γ][||][V][θ][ ´][ V][θ][˚][||]Dir[2] [. Then, authors define]
gradient splitting (linear function hpxq “ Bpx ´ aq is gradient splitting of quadratic function
_jpxq “ px ´ aq[T]_ _Qpx ´ aq, where Q is symmetric positive semi-definite matrix, if B ` B[T]_ “ 2Q)
and show that negation to mean-path update ´g¯pθq is indeed a gradient splitting of function f pθq.
In this paper we do not use the fact that function f pθq might be represented as weighted sum of Dnorm and Dirichlet norm and, for convenience, define function f pθq based on its gradient splitting
properties.

We rely on interpretation of TD learning as splitting of the gradient descent in our analysis. In
Xu et al. (2020) authors note: ”In Johnson & Zhang (2013), the convergence proof relies on the
relationship between the gradient and the value of the objective function, but there is not such an
objective function in the TD learning problem.” Well, viewing on TD learning as gradient splitting
gives the relationship between the gradient and the value function, which allows implementation of
similar analysis as in Johnson & Zhang (2013) to achieve stronger results. It also gives the objective
function f pθq which is better measure of the distance to optimal solution, rather than ||θ ´ θ[˚]||[2],
and yields tighter convergence bounds.

### 3 THE TD-SVRG ALGORITHM

We next propose a modification of the TD(0) method (TD SVRG) which can attain a geometric
rate. This algorithm is given below as Algorithm 1. The algorithm works under the “fixed sample
set” setting which assumes there already exists a sampled data set . This is the same setting was
_D_
considered in Du et al. (2017). However, the method we propose does not add regularization and
does not use dual parameters, which makes it considerably simpler.

**Algorithm 1 TD-SVRG for finite sample case**

**Parameters update frequency M and learning rate η**
**Initialize** _θ[˜]0._
**Iterate: for m “ 1, 2, . . .**

_θ “_ _θ[˜]m´1,_
_g¯pθq “_ _N[1]_ řs,s[1]PD _[g][s,s][1][p][θ][q][,]_

where gs,s1pθq “ prps, s[1]q ` γϕps[1]q[T] _θ ´ ϕpsq[T]_ _θqϕpstq,_
_θ0 “ θ._
**Iterate: for t “ 1, 2, . . ., M**

Randomly sample s, s[1] from the dataset and compute update vector
_vt “ gs,s1pθt´1q ´ gs,s1pθq ` ¯gpθq._
Update parameters θt “ θt´1 ´ ηvt.

**end**
Set _θ[˜]m “ θt for randomly chosen t P p0, . . ., M ´ 1q._

**end**


Like the classic SVRG algorithm, our proposed TD-SVRG has two layers of loops. We refer one
step of the outer loop as an epoch and one step of inner loop as an iteration. TD-SVRG keeps
two parameter vectors: current parameter vector θt, which is being updated every iteration, and the
vector _θ[˜], which is updated the end of each epoch. In the beginning of outer loop, the mean-path TD_
update vector ¯gpθ[˜]q is computed with a pass through the entire data set. This vector is used in inner
loop to compute local updates vt “ gs,s1pθt´1q ´ gs,s1 pθ[˜]q ` ¯gpθq, where gs,s1pθt´1q is a TD update
computed on a uniformly randomly sampled data point from D with current parameter vector θt and
_gs,s1_ pθ[˜]q is a TD update computed on the same data point. Each iteration ends with an update of
epoch vector, which is randomly chosen from parameter vectors during the epoch.

### 4 CONVERGENCE ANALYSIS

In this section we show, that under simple assumption Algorithm 1 attain geometric convergence in
terms of specially chosen function f pθq with η is Op1q and M is Op1{λminq.


-----

4.1 PRELIMINARIES

In order to analyze the convergence of the presented algorithm we define expected square norm of
difference in current and optimal parameters as wpθq :

_wpθq_ “ _Es,s1||gs,s1pθq ´ gs,s1pθ[˚]q||[2]._ (2)

With this notation we provide an technical lemma. All of our proofs are based on variations of this
lemma.

**Lemma 1. If Assumptions 1, 2 hold, epoch parameters of two consecutive epochs m ´ 1 and m are**
_related by the following inequality:_

2ηM Ef pθ[˜]mq ´ 2Mη[2]Ewpθ[˜]mq ď E||θ[˜]m´1 ´ θ[˚]||[2] ` 2η[2]M Ewpθ[˜]m´1q, (3)

_where the expectation is taken with respect to all previous epochs and choices of states s, s[1]_ _during_
_the epoch m._

_Proof. The proof of the lemma generally follows the analysis logic in Johnson & Zhang (2013), it_
might be found in Appendix A.

Lemma 1 plays an auxiliary role in our analysis and significantly simplifies it. It introduces a new
approach to the convergence proof by carrying iteration to iteration and epoch to epoch bounds to
the earlier part of the analysis. In particular, deriving bounds in terms of some arbitrary function
_upθq is now reduced to deriving upper bounds on ||θ[˜]m´1||[2]_ and wpθq and a lower bound on f pθq in
terms of the function u. In fact, the function f pθq itself will play the role of upθq in our proof.

In addition, it is now easy to demonstrate the point we made in Subsection 2.1. The main problem
of direct application of the SVRG convergence analysis to TD learning is that it requires the target
function P pwq to be a sum of convex functions ϕipwq, where all functions ϕi are L-Lipschitz and γsmooth (notation here is from Johnson & Zhang (2013)). Of course, the TD learning problem cannot
be represented in this form. However, the main use of L-Lipschitz and smoothness properties in the
original is to derive a bound on the expected norm of the difference between current and optimal
parameters. As will be shown later, in TD learning this expected norm (wpθq in our notation) still
might be derived, even when a sum representation does not exist.

4.2 WARM-UP: CONVERGENCE IN TERMS OF SQUARED NORM

Firstly, we derive the convergence bound in terms of ||θ ´ θ[˚]||[2] and show that they are consistent
with previous results.

**Proposition 1. Suppose Assumptions 1, 2 hold. If we chose learning rate as η “ λmin{32 and**
_number of iteration as M “ 32{λ[2]min[, then Algorithm 1 has a convergence rate of:]_


ˆ 5
_Er||θ[˜]m ´ θ[˚]||[2]s ď_ 7


˙m
||θ[˜]0 ´ θ[˚]||[2].


_Proof. The proof is given in Appendix B_

Note that deriving a convergence rate in terms of squared norm ||θ[˜]m ´ θ[˚]||[2] leads to batch size
_m to be Op1{λ[2]min[q][, which is better than results in Du et al. (2017), since their results has com-]_
plexity Opκ[2]pCqκ[2]G[q][, where][ κ][p][C][q][ is condition number of matrix][ C][ “][ E][s][P][D][r][ϕ][p][s][q][ϕ][p][s][q][T][ s][ and]
_κG91{λminpA[T]_ _C_ [´][1]Aq. Experimental comparison of these values is provided in Subsection H.1
.


-----

4.3 FIRST MAIN RESULT: CONVERGENCE IN TERMS OF f pθq

In this section derive a bound in terms of f pθq. For analysis to be simpler and illustrative we
introduce one more assumption which we ease in section 4.5.
**Assumption 3 (Dataset Balance). In the dataset D first state of the first sample and the second state**
_of the last sample is the same state, i.e. s1 “ sN_ `1.

We need this assumption to omit dataset bias, so that states s and s[1] have the same distribution, as in
the original MDP. Having this assumption, we can proof theorem 1:
**Theorem 1. Suppose Assumptions 1, 2, 3 hold. If we choose learning rate η “ 1{8 and number of**
_inner loop iterations M “ 16{λmin, then Algorithm 1 will have a convergence rate of:_

ˆ 2 ˙m
_Erf_ pθ[˜]mqs ď _f_ pθ[˜]0q.

3

Note that _θ[˜]m refers to the iterate after m iterations of the outer loop. Because we choose the length_
_M of the inner loop to be 16{λmin, the total number of samples guaranteed by this theorem until_
_Erf_ pθ[˜]mqs ď ϵ is actually p16{λminq logp1{ϵq.

_Proof of Theorem 1. The proof is given in Appendix C._

Convergence analysis without Assumption 3 provided in Appendix Section D.

4.4 SIMILARITY OF SVRG AND TD-SVRG

Liu & Olshevsky (2020) show that negation to mean-path update ´g¯pθq is a gradient splitting of
_f_ pθq. In this work we show even greater importance of function f pθq for TD learning process.
Recall convergence rate obtained in Johnson & Zhang (2013) for sum of convex functions setting:

1 2Lη

_γηp1 ´ 2Lηqm_ [`] 1 ´ 2Lη [,]


where γ is a strong convexity parameter and L is Lipschitz smoothness parameter (employing notation from the original paper). Function f pθq “ pθ ´ θ[˚]q[T] _Apθ ´ θ[˚]q is 2λminpAq strongly convex_
and 2-Lipschitz smooth, which means that convergence rate obtained in this paper is identical to the
convergence rate of SVRG in convex setting (we have slightly better bound L instead of 2L due to
strong bound on wpθq we derived for this setting). This fact further extends the analogy between TD
learning and convex optimization earlier explored by Bhandari et al. (2018) and Liu & Olshevsky
(2020).

### 5 BATCHING SVRG CASE ANALYSIS

In this section we extend our results to inexact mean-path update computation, applying the results of
Babanezhad et al. (2015) to TD SVRG algorithm. We show that geometric convergence rate might
be achieved with smaller number of computations by estimating mean-path TD-update instead of
performing full computation. This approach is similar to Peng et al. (2019), but again doesn’t require
introduction of dual variables. In addition, we provide a particular way to compute nm, which might
be used in practice.

Since computation of mean-path error is not related to the dataset balance, in this section for simplicity we assume that dataset is balanced.
**Theorem 2. Suppose Assumptions 1, 2, 3 hold, then if learning rate is chosen as η “ 1{8 number of**
_inner loop iterations M “ 16{λmin and batch size nm “ minpN,_ _cρ[2][m]NpN_ ´1q [p][2][|][r][max][|][2] [`][8][||][θ][˜][m][||][2][qq][,]

_where c is a parameter, Algorithm 2 will have a convergence rate of:_

_Erf_ pθ[˜]mqs ď ρ[m]pf pθ[˜]0q ` Cq,
_where ρ P p0, 1q is convergence rate and C is some constant._


-----

**Algorithm 2 TD-SVRG with batching for finite sample case**

**Parameters update frequency M and learning rate η**
**Initialize** _θ[˜]0._
**Iterate: for m “ 1, 2, . . .**

_θ “_ _θ[˜]m´1,_
choose batch size nm,
sample batch D[m] of size nm from D without replacement,
compute µ “ _n1m_ řs,s[1]PD[m][ g][s,s][1][p][θ][q][,]

where gs,s1pθq “ prps, s[1]q ` γϕps[1]q[T] _θ ´ ϕpsq[T]_ _θqϕpstq,_
_θ0 “_ _θ[˜]._
**Iterate: for t “ 1, 2, . . ., M**

Randomly sample s, s[1] from D and compute update vector
_vt “ gs,s1pθt´1q ´ gs,s1pθ[˜]q ` µ,_
Update parameters θt “ θt´1 ´ ηvt.

**end**
set _θ[˜]m “ θt for randomly chosen t P p0, . . ., M ´ 1q._

**end**


_Proof. The proof is given in Appendix E._

This theorem shows that during early epochs approximation of mean-path update is good enough
to guarantee geometric convergence. However, the batch size used for approximation increases
geometrically with each epoch with rate ρ[2], where ρ is a desired convergence rate, until it reaches
size of the dataset N . The constant C depends on parameter c and upper bound Z “ maxθp|θ´θ[˚]|),
where the max is taken over all parameter vectors seen during the run of the algorithm.

### 6 SECOND MAIN RESULT: ONLINE IID SAMPLING FROM THE MDP

In this section we apply TD learning as gradient splitting analysis to the case of online i.i.d sampling
from the MDP each time we need to generate a new state s. We show that this view of TD learning
as gradient splitting might be applied in this case to derive tighter convergence bounds. One issue
of TD-SVRG in i.i.d. setting is that mean-path update may not be computed directly. However, this
issue might be addressed with sampling technique introduced in Section 5, which makes i.i.d. case
very similar to TD-SVRG with non-exact mean-path computation in finite samples case.

In this setting, geometric convergence is clearly not attainable with variance reduction, which always
relies on a pass through the entire dataset. Since here there is no data set, and one samples from the
MDP at every step, one clearly cannot make a pass through all states of the MDP (or, rather, this is
unrealistic to do in practice). To obtain convergence, one needs to take increasing batch sizes. Our
next theorem does so, while improving the scaling with condition number from quadratic to linear.

TD-SVRG algorithm for iid sampling case is very similar to Algorithm 2, with only difference that
states s, s[1] are being sampled from the MDP instead of the dataset D. Formal definition of Algorithm
3 might be found in section F.

**Theorem 3. Suppose Assumptions 1, 2 hold, then if learning rate is chosen as η “ 1{8, number of**
_inner loop iterations M “ 16{λmin and batch size nm “_ _cρ1[2][m][ p][2][|][r][max][|][2][ `]_ [8][||][θ][||][2][q][, where][ c][ is some]

_arbitrary chosen constant, Algorithm 3 will have a convergence rate of:_

_Erf_ pθ[˜]mqs ď ρ[m]pf pθ[˜]0q ` Cq,

_where ρ P p0, 1q is convergence rate and C is some constant._

_Proof. The proof is given in Appendix F._

To parse this, observe that as in 5, each epoch requires nm computations to estimate mean-path
update and 16{λmin to perform inner loop iterations. Thus, as epoch number m grows, nm will


-----

dominatewhich is better than 16{λmin, which results in the total computational complexity of Opmaxp [1]ϵ _[,]_ _λ[2]min1_ [q][ log][p][ 1]ϵ [qq][ shown by Xu et al. (2020) when] Opmax[ λ][min]p [1]ϵ[ ą][,] _λ[ ϵ]min1_ [.] [q][ log][p][ 1]ϵ [qq][,]

In practice, λmin is determined by the MDP and the feature matrix, while ϵ is a desired accuracy,
thus, the former is given and the latter might be chosen. In most scenarios, λmin is a small number
and ϵ is chosen such that ϵ ă λmin. Even if this is not a case, λ[2]min [is a very small number and most]
likely ϵ ă λ[2]min[. Thus, in the absolute majority of cases results shown in this paper are stronger.]

The same convergence result with predetermined constant learning rate cannot be derived for Markovian sampling case, in which update sampling strategy is different from one in classical SVRG
sampling. However, gradients splitting interpretation of TD learning still allows to achieve better
convergence guarantees than in previous works for the absolute majority of problems. Algorithm,
discussion and convergence proof is provided in Appendix Section G.

### 7 EXPERIMENTS

Figure 1: Average performance of different algorithms in finite sample case. Columns - dataset
source environments: MDP, Acrobot, CartPole and Mountain Car. Rows - performance measurements: logpf pθqq and logp|θ ´ θ[˚]|q.

7.1 ALGORITHMS COMPARISON

In this set of experiments we compare the performance of TD-SVRG with GTD2 Sutton et al. (2009),
Vanilla TD learning Sutton (1988) and PD SVRG Du et al. (2017) in finite sample setting. Generally,
experiment set-up is similar to Peng et al. (2019). Datasets of size 5000 are generated from 4
environments: Random MDP Dann et al. (2014) and Acrobot, CartPole and Mountain car OpenAI
Gym environments Brockman et al. (2016). For Random MDP, we construct MDP environment with
|S| “ 400, 21, features and 10 actions, with actions choice probabilities generated from U r0, 1q.
For OpenAI gym environments, agent select states uniformly at random. Features constructed by
applying RBF kernels to original states and then removing highly correlated features (correlation
coefficient ą 0.5). To produce datasets of similar sizes we resampled dataset if smallest eigen-value
of its matrix A was outside the interval r0.32, 0.54s ¨ 10[´][4], which corresponds to TD-SVRG batch
sizes between 30000 and 50000. Decay rate γ is set to 0.95.

Hyperparameters for algorithms selected as follows: for TD-SVRG theoretically justified parameters
are selected, learning rate η “ 1{8 and number of inner loop computations M “ 16{λmin; for
GTD2 we used parameters which are suggested for small problems? _α “ 1 and β “ 1. For vanilla_
TD decreasing learning rates are set to α “ 1{ _t and α “ 1{t. For PD-SVRG setting parameters_

to theoretically suggested is not feasible, since even for simple problems values of number of inner


-----

loop computations M is too large (see Appendix Subsection H.1). Following original paper Du
et al. (2017) we run a simple grid search are pick best performing values, which are σθ “ σw “
0.1{λmaxpC[ˆ]q. Results presented on Figure 1. Each algorithm for each setting was run 10 times,
average result is presented. As theory predicts, TD-SVRG and PD-SVRG converge geometrically,
while GTD and vanilla TD converge sub linearly.

7.2 ONLINE IID SAMPLING FROM THE MDP

Figure 2: Average performance of TD-SVRG, VRTD and vanilla TD in i.i.d. sampling case. ”TDdecr” refers to vanilla TD with decrasing learning rate, ”TD-const” - to vanilla TD with constant
learning rate. Left figure - performance in terms of logpf pθqq, right in terms of logp|θ ´ θ[˚]|q.

In this set of experiments we compare the performance of TD-SVRG, VRTD and three Vanilla TD
with fixed and decreasing learning rates in i.i.d. sampling case. States and rewards are sampled from
the same MDP as in Section 7.1. Hyperparameters are chosen as follows: for TD-SVRG - learning
rate η “ 1{8, number of inner loop computations M “ 16{λmin. VRTD - learning rate α “ 0.1
and batch size M “ 2000. For vanilla TD with constant learning rate its value set to 0.1 and for
decreasing learning rate it is 1{t, where t is number of performed update. Average results over 10
runs presented on Figure 2.

7.3 REPRODUCIBILITY

[Authors provide a link to anonynous github repository with code and instruction how to reproduce](https://anonymous.4open.science/r/SVRG_for_TD_learning-C688)
the experiments.

### 8 CONCLUSION

In the paper we utilize a view on TD learning as splitting of gradient descent to show that SVRG
technique applied to TD updates attain similar convergence rate as SVRG in convex function setting. Our analysis addresses both finite sample and i.i.d. sampling cases, which previously were
analyzed separately, and improves state of the art bounds in both cases. In addition we show that
gradient splitting interpretation helps to improve convergence guarantees in Markovian sampling
case. The algorithms based on our analysis have fixed learning rate and small number of inner loop
computation, easy to implement and demonstrates good performance during experiments.

### REFERENCES

Reza Babanezhad, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub Koneˇcn´y, and
Scott Sallinen. Stop wasting my gradients: Practical svrg, 2015.

Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. CoRR, abs/1806.02450, 2018.


-----

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

Gal Dalal, Bal´azs Sz¨or´enyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td(0) with
function approximation. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1),
Apr. 2018.

Christoph Dann, Gerhard Neumann, Jan Peters, et al. Policy evaluation with temporal differences:
A survey and comparison. Journal of Machine Learning Research, 15:809–883, 2014.

Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives, 2014.

Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction methods for policy evaluation. CoRR, abs/1702.07944, 2017.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger (eds.),
_Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013._

Nathaniel Korda and Prashanth La. On td(0) with function approximation: Concentration bounds
and a centered variant with exponential convergence. In Proceedings of the 32nd International
_Conference on Machine Learning (ICML), pp. 626–634, 2015._

Rui Liu and Alex Olshevsky. Temporal difference learning as gradient splitting. _CoRR,_
abs/2010.14657, 2020.

C Narayanan and Csaba Szepesv´ari. Finite time bounds for temporal difference learning with function approximation: Problems with some “state-of-the-art” results. Technical report, Technical
report, 2017.

Zilun Peng, Ahmed Touati, Pascal Vincent, and Doina Precup. SVRG for policy evaluation with
fewer gradient evaluations. CoRR, abs/1906.03704, 2019.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati_cal statistics, pp. 400–407, 1951._

Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient, 2013.

R. Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation and TD learning. CoRR, abs/1902.00923, 2019.

Richard Sutton. Learning to predict by the methods of temporal differences. Mach Learn, 3, 1988.

Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesv´ari, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning
with linear function approximation. In Proceedings of the 26th annual international conference
_on machine learning, pp. 993–1000, 2009._

Ahmed Touati, Pierre-Luc Bacon, Doina Precup, and Pascal Vincent. Convergent tree backup and retrace with function approximation. In International Conference on Machine Learning, pp. 4955–
4964. PMLR, 2018.

J.N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997. doi: 10.1109/9.580874.

Tengyu Xu, Zhe Wang, Yi Zhou, and Yingbin Liang. Reanalysis of variance reduced temporal
difference learning. CoRR, abs/2001.01898, 2020.


-----

### A PROOF OF LEMMA 1

The proof follows the same logic as in Johnson & Zhang (2013) and organized in four steps.
**Step A.1. In the original paper proof starts with deriving a bound on the squared norm of the**
_difference between current and optimal sets of parameters. Since the introduction of wpθq this step_
_in our proof is trivial._
_Es,s1||gs,s1pθq ´ gs,s1pθ[˚]q||[2]_ “ wpθq

**Step A.2. During Step 2 we derive a bound on the norm of an single iteration t update vt “**
_gs,s1_ pθt´1q ´ gs,s1pθ[˜]q ` ¯gpθ[˜]q, assuming that states s, s[1] _were sampled randomly during step t:_

_Es,s1r||vt||[2]s “ E||gs,s1pθt´1q ´ gs,s1pθ[˜]q ` ¯gpθ[˜]q||[2]_

“ Es,s1||pgs,s1pθt´1q ´ gs,s1 pθ[˚]qq ` pgs,s1pθ[˚]q ´ gs,s1 pθ[˜]q ` ¯gpθ[˜]q||[2]

ď 2Es,s1||pgs,s1pθt´1q ´ gs,s1pθ[˚]qq||[2]

` 2Es,s1||gs,s1pθ[˜]q ´ gs,s1pθ[˚]q ´ pg¯pθ[˜]q ´ ¯gpθ[˚]qq||[2]

“ 2Es,s[1]||pgs,s[1]pθt´1q ´ gs,s[1]pθ[˚]qq||[2] ` 2Es,s[1] ||gs,s[1] pθ[˜]q ´ gs,s1pθ[˚]q

´ Es,s1rgs,s1pθ[˜]q ´ gs,s1pθ[˚]qs||[2]

ď 2Es,s[1]||pgs,s[1]pθt´1q ´ gs,s[1]pθ[˚]qq||[2] ` 2Es,s[1] ||gs,s[1] pθ[˜]q ´ gs,s[1]pθ[˚]q||[2]

“ 2wpθt´1q ` 2wpθ[˜]q

_The first inequality uses E||a ` b||[2]_ ď 2E||a||[2] ` 2E||b||[2]. The second inequality uses the face that
_second central moment is smaller than second moment. The last equality uses the equality from step_
_1._
**Step A.3. During this step we derive a bound on the expected squared norm of a distance to optimal**
_parameter vector after a single update t:_

Es,s1 ||θt ´ θ[˚]||[2] “ Es,s1 ||θt´1 ´ θ[˚] ` ηvt||[2]

“ ||θt´1 ´ θ[˚]||[2] ` 2ηpθt´1 ´ θ[˚]qEvt ` η[2]E||vt||[2]

ď ||θt´1 ´ θ[˚]||[2] ` 2ηpθt´1 ´ θ[˚]qg¯pθt´1q ` 2η[2]wpθt´1q ` 2η[2]wpθ[˜]q

“ ||θt´1 ´ θ[˚]||[2] ´ 2ηf pθt´1q ` 2η[2]wpθt´1q ` 2η[2]wpθ[˜]q


_The inequality uses the bound obtained in step 2. After rearranging terms it becomes:_

E||θt ´ θ[˚]||[2] ` 2ηf pθt´1q ´ 4η[2]wpθt´1q ď ||θt´1 ´ θ[˚]||[2] ` 4η[2]wpθ[˜]q

**Step A.4. During this step we sum the inequality obtained in Step 3 over the epoch and take expec-**
_tations with respect to all choices of pair of states s, s[1]_ _and all previous history and use the random_
_choice property to obtain Equation 1 which relates parameter vectors of two consecutive epochs:_


_M_
ÿ

E||θt ´ θ[˚]||[2] `
_t“1_


_M_
ÿ

2ηEf pθt´1q ´
_t“1_


_M_
ÿ

2η[2]Ewpθt´1q ď
_t“1_


_M_
ÿ

E||θt´1 ´ θ[˚]||[2] `
_t“1_


_M_
ÿ

2η[2]Ewpθ[˜]q
_t“1_


_We analyze this expression term-wise._

řM
_t“1_ [E][||][θ][t][´][1][ ´][ θ][˚][||][2][ and][ ř]t[M]“1 [E][||][θ][t][ ´][ θ][˚][||][2][ consist of same terms, except the first term in the first]
_sum and the last term in the last sum, which are E||θ0_ ´θ[˚]||[2] _and E||θM ´θ[˚]||[2]. Since E||θM ´θ[˚]||[2]_
_is always positive and it is on the left hand side of the inequality, we could drop it._

_We denote the parameter vector θ chosen for epoch parameters in the end of the epoch_ _θ[˜]m. Since this_
_vector is chosen uniformly at random among all iteration vectors θt,_ [ř]t[M]“1 [E][f] [p][θ][t][´][1][q “][ M] [E][f] [p][θ][˜][m][q]
_and_ [ř]t[M]“1 [E][w][p][θ][t][´][1][q “][ M] [E][w][p][θ][˜][m][q][.]


-----

_At the same time,_ _θ[˜], which was chosen in the end of the previous epoch remains the same throughout_
_the epoch, therefore,_ [ř]t[M]“1 [E][w][p][θ][˜][q “][ M] [E][w][p][θ][˜][q][. Note, that current epoch starts with setting][ θ][0][ “][ ˜][θ][.]
_Also, to underline it is previous epoch, we denote it as_ _θ[˜]m´1._

_Plugging this values in we have 3:_

2ηM Ef pθ[˜]mq ´ 2Mη[2]Ewpθ[˜]mq ď E||θ[˜]m´1 ´ θ[˚]||[2] ` 2η[2]M Ewpθ[˜]m´1q

### B PROOF OF PROPOSITION 1

To transform inequality 3 from Lemma 1 into convergence rate guarantee, we need to bound wpθq
and f pθq in terms of ||θ ´ θ[˚]||[2]. Both bounds are easy to show:

_wpθq “ Es,s1||gs,s1pθq ´ gs,s1pθ[˚]q||[2]_

“ pθ ´ θ[˚]q[T] _Es,s1rpγϕps[1]q ´ ϕpsqqϕpsq[T]_ _ϕpsqpγϕps[1]q ´ ϕpsqq[T]_ spθ ´ θ[˚]q

ď pθ ´ θ[˚]q[T] _Es,s[1]r||pγϕps[1]q ´ ϕpsqq|| ¨ ||ϕpsq|| ¨ ||ϕpsq|| ¨ ||pγϕps[1]q ´ ϕpsqq||spθ ´ θ[˚]q_

ď 4||θ ´ θ[˚]||[2],

_f_ pθq “ pθ ´ θ[˚]q[T] _Es,s1rϕpsqpϕpsq ´ γϕps[1]qq[T]_ spθ ´ θ[˚]q ě λmin||θ ´ θ[˚]||[2].


Plugging these bounds into Equation 3 we have:

p2ηMλmin ´ 8Mη[2]q||θ[˜]m ´ θ[˚]||[2] ď p1 ` 8Mη[2]q||θ[˜]m´1 ´ θ[˚]||[2].

Which yields epoch to epoch convergence rate of:

1 ` 8Mη[2]

2ηMλmin ´ 8Mη[2][ .]

For this expression to be ă 1, we need that ηM to be Op1{λminq, which means that η needs to be
_Opλminq for Mη[2]_ to be Op1q. Therefore, M need to be Op1{λ[2]min[q][. Setting][ η][ “][ λ][min][{][32][ and]
_m “ 32{λ[2]min_ [yields convergence rate of][ 5][{][7][.]

### C PROOF OF THEOREM 1

The same as in the previous section, we start with deriving bounds, but this time we bound ||θ´θ[˚]||[2]
and wpθq in terms of f pθq. First bound is straightforward:


1
_f_ pθq “ pθ ´ θ[˚]q[T] _Eϕ,ϕ1rϕpϕ ´ γϕ[1]q[T]_ spθ ´ θ[˚]q ùñ ||θ ´ θ[˚]||[2] ď _f_ pθq.

_λmin_

For wpθq we have:


-----

_wpθq “ pθ ´ θ[˚]q[T]_ _Es,s1rpγϕps[1]q ´ ϕpsqqϕpsq[T]_ _ϕpsqpγϕps[1]q ´ ϕpsqq[T]_ spθ ´ θ[˚]q

ÿ

“ pθ ´ θ[˚]q[T][ “][ 1] pγϕps[1]q ´ ϕpsqqϕ[T] psqϕpsqpγϕps[1]q ´ ϕpsqq[T][ ‰]pθ ´ θ[˚]q

_N_

_s,s[1]PD_

ÿ

ď pθ ´ θ[˚]q[T][ “][ 1] pγϕps[1]q ´ ϕpsqqpγϕps[1]q ´ ϕpsqq[T][ ‰]pθ ´ θ[˚]q

_N_

_s,s[1]PD_

ÿ

“ pθ ´ θ[˚]q[T][ “][ 1] _γ[2]ϕps[1]qϕps[1]q[T]_ ´ γϕps[1]qϕpsq[T][ ‰]pθ ´ θ[˚]q ` f pθq

_N_

_s,s[1]PD_

ÿ

“ pθ ´ θ[˚]q[T][ “][ 1] _γ[2]ϕpsqϕpsq[T]_ ´ γϕpsqϕps[1]q[T][ ‰]pθ ´ θ[˚]q ` f pθq

_N_

_s,s[1]PD_


(4)


ď 2f pθq,

first inequality uses Assumption 2, third equality uses Assumption 3, ([ř]s[1][ γ][2][ϕ][p][s][1][q][ϕ][p][s][1][q][T] “

ř

_s_ _[γ][2][ϕ][p][s][q][ϕ][p][s][q][T][, since][ s][ and][ s][1][ are the same set of states). The last inequality uses the fact that]_
_γ ă 1._


Plugging these bound into Equation 3, we have:

1
2ηM Ef pθ[˜]mq ´ 4Mη[2]Ef pθ[˜]mq ď Ef pθ[˜]m´1q ` 4η[2]M Ef pθ[˜]m´1q,

_λmin_

which yields epoch to epoch convergence rate of:


” 1 2η
Ef pθ[˜]mq ď

2λminηM p1 ´ 2ηq [`] 1 ´ 2η

Setting η “ 8[1] [and][ M][ “] _λmin16_ [we have the desired inequality.]


ı
Ef pθ[˜]m´1q.


### D CONVERGENCE ANALYSIS WITHOUT DATASET BALANCE

In this section we show the convergence bound for the problem without Assumption 3. In this case,
the problem is that after the sampling, s and s[1] in the dataset does not have the same distribution, i.e.,
the first element of the tuples pst, at, rt, st`1q in our dataset D need not have the same distribution
as the last element of this tuple. Indeed, it could happen that a particular state occurs a different
numbers of times as the first element of the tuples in as compared to the last element, which would
_D_
not happen under Assumption 3. When this happens, we will say that the data set is unbalanced. In
that case, ¯gpθq need not be a gradient splitting of function f pθq.

One might hope that, when the size of the data set N is large, this effect has an impact which decays
to zero with N . Our second main result shows something even stronger: we show that the effect of
unbalacedness disappears completely for large N . Thus our next theorem completely recovers the
performance attained by Theorem 1 for large N . The catch is the size of the dataset has to be at least
as big as λ[´]min[1] [for this to happen.]

**Theorem 4. Suppose Assumptions 1, 2 hold and dataset is unbalanced. Define error term J “**
4γ[2]

_Nλmin_ _[. Then, if we choose learning rate][ η][ “][ 1][{p][8][ `][ J][q][ and number of inner loop iterations]_
_M “ 2{pλminηq, Algorithm 1 will have a convergence rate of:_

ˆ 2 ˙m
_Erf_ pθ[˜]mqs ď _f_ pθ[˜]0q.

3

_Proof. The proof is given in Appendix D.1._


Note, that in this case η P Op maxp1,1{p1Nλminqq [q][ and][ M][ P][ O][p] _λmin1_ _η_ [q][, which always better than]

parameters required to guarantee convergence of ||θ´θ[˚]||[2] (Proposition 1). Note that the guarantees
_of this theorem are identical to the guarantees of Theorem 1 in the unbalanced case when N ě λ[´]min[1]_ _[.]_


-----

D.1 PROOF OF THEOREM 4

To proof the theorem we follow the same strategy as in C. For the f pθq we can use the same bound:

1
_f_ pθq “ pθ ´ θ[˚]q[T] _Eϕ,ϕ1rϕpϕ ´ γϕ[1]q[T]_ spθ ´ θ[˚]q ùñ ||θ ´ θ[˚]||[2] ď _f_ pθq

_λmin_

Bound for wpθq is a little bit more difficult:


_wpθq “ pθ ´ θ[˚]q[T][ “][ 1]_

_N_

ď pθ ´ θ[˚]q[T][ “][ 1]

_N_

“ pθ ´ θ[˚]q[T][ “][ 1]

_N_

“ pθ ´ θ[˚]q[T][ “][ 1]

_N_

“ pθ ´ θ[˚]q[T][ “][ 1]

_N_


ÿ

_γ[2]ϕpsqϕpsq[T]_ ´ γϕpsqϕps[1]q[T][ ‰]pθ ´ θ[˚]q ` f pθq
_s,s[1]PD_


ÿ

pγϕps[1]q ´ ϕpsqqϕ[T] psqϕpsqpγϕps[1]q ´ ϕpsqq[T][ ‰]pθ ´ θ[˚]q
_s,s[1]PD_

ÿ

pγϕps[1]q ´ ϕpsqqpγϕps[1]q ´ ϕpsqq[T][ ‰]pθ ´ θ[˚]q
_s,s[1]PD_


ÿ

_γϕps[1]qpγϕps[1]q ´ ϕpsqq[T]_ ´ ϕpsqpγϕps[1]q ´ ϕpsqq[T][ ‰]pθ ´ θ[˚]q
_s,s[1]PD_

ÿ

_γ[2]ϕps[1]qϕps[1]q[T]_ ´ γϕps[1]qϕpsq[T][ ‰]pθ ´ θ[˚]q ` f pθq
_s,s[1]PD_


` _[γ][2]_

_N_ [p][θ][ ´][ θ][˚][q][T][ p][ϕ][p][s][N] [`][1][q][ϕ][p][s][N] [`][1][q][T][ ´][ ϕ][p][s][1][q][ϕ][p][s][1][q][T][ qp][θ][ ´][ θ][˚][q][T]

ď 2f pθq ` _[γ][2]_

_N_ [p][θ][ ´][ θ][˚][q][T][ p][ϕ][p][s][N] [`][1][q][ϕ][p][s][N] [`][1][q][T][ ´][ ϕ][p][s][1][q][ϕ][p][s][1][q][T][ qp][θ][ ´][ θ][˚][q][T][ .]


The first inequality follows from the assumption about norms of feature vectors. The third equality
is obtained by adding and subtracting _[γ]N[2]_

[p][θ] [´] _[θ][˚][q][T][ ϕ][p][s][1][q][ϕ][p][s][1][q][T][ p][θ]_ [´] _[θ][˚][q][. Second inequality uses the]_
fact that γ[2] ă 1. We denote maximum eigen-value of matrix ϕpsN `1qϕpsN `1q[T] ´ ϕps1qϕps1q[T] as
_K (note that K ď 1). Thus,_

_γ[2]_ _γ[2]_
_K_

_wpθq ď 2f_ pθq ` _[γ][2][K]_ q ď

_N_ [||][θ][ ´][ θ][˚][||][2][ ď][ f] [p][θ][qp][2][ `] _Nλmin_ _Nλmin_


Plugging these bounds into Equation 3 we have:

_γ[2]_ 1 _γ[2]_
p2ηM ´ 2Mη[2]p2 ` qqEf pθ[˜]mq ď p ` 2η[2]M p2 ` qqf pθ[˜]q

_Nλmin_ _λmin_ _Nλmin_


Which yields convergence rate of:

_γ[2]_

1 _ηp2 `_ _Nλmin_

[q]

_λmin2ηM_ p1 ´ ηp2 ` _Nλγmin[2]_ ` 1 ´ ηp2 ` _Nλγmin[2]_

[qq] [q]


To achieve constant convergence rate, for example [2]3 [, we set up][ η][ such that][ η][p][2][ `] _Nλγmin[2]_

thus the second term is equal to 1/3 and η “ 8` _Nλmin14γ[2]_ . Then, to make the first term equal to 1/3,[q “][ 0][.][25][,]

we need to set

2 2
_M “_ _λminη_ [“] _λmin_ 14γ[2]

8` _Nλmin_


Thus, η is a scale of maxp1,1{p1 _Nλminq_ [and][ M][ is a scale of] _λmin minp11,Nλminq_ [.]


-----

### E PROOF OF THEOREM 3

In the first part of the proof we derive an inequality which relates model parameters of two consecutive epochs similar to what we achieved in previous proofs, but with error term. In this part of the
proof we follow the same 4 steps logic as while proof of Lemma 1. In the second part of the proof
we show that there are conditions under which error term converges to 0.
**Step E.1. During the first step we use the bound obtained in inequality 4:**

_wpθq ď 2f_ pθq

**Step E.2. During this step we derive a bound on the squared norm of a single update Er||vt||[2]s.**
_But now, compared to previous case, we do not compute the exact mean-path updated ¯gpθq, but its_
_estimate, and assume our computation has error µ “ ¯gpθq ` e. Thus the single update vector will_
_be_

_vt “ gpθt´1q ´ gpθ[˜]q ` ¯gpθ[˜]q ` e_

_Thus, the bound on the single update might be derived as:_

Er||vt||[2]s “ E||gpθt´1q ´ gpθ[˜]q ` ¯gpθ[˜]q ` e||[2]

“ E||pgpθt´1q ´ ¯gpθ[˚]qq ` pg¯pθ[˚]q ´ gpθ[˜]q ` ¯gpθ[˜]q ` eq||[2]

ď 2E||pgpθt´1q ´ gpθ[˚]qq||[2] ` 2E||gpθ[˜]q ´ gpθ[˚]q ´ pg¯pθ[˜]q ´ ¯gpθ[˚]qq ´ e||[2]

“ 2E||pgpθt´1q ´ gpθ[˚]qq||[2] ` 2E||gpθ[˜]q ´ gpθ[˚]q ´ Ergpθ[˜]q ´ gpθ[˚]qs ´ e||[2]

“ 2E||pgpθt´1q ´ gpθ[˚]qq||[2] ` 2E||gpθ[˜]q ´ gpθ[˚]q ´ Ergpθ[˜]q ´ gpθ[˚]qs||[2]

´ 2Exgpθ[˜]q ´ gpθ[˚]q ´ 4Ergpθ[˜]q ´ gpθ[˚]qs, ey ` 2E||e||[2]

ď 2E||pgpθt´1q ´ gpθ[˚]qq||[2] ` 2E||gpθ[˜]q ´ gpθ[˚]q||[2] ` 2E||e||[2]

“ 2wpθt´1q ` 2wpθ[˜]q ` 2E||e||[2]

ď 4f pθt´1q ` 4f pθ[˜]q ` 2E||e||[2],

_where first inequality uses E||A ` B||[2]_ ď 2E||A||[2] ` 2E||B||[2], second inequality uses E||A ´
ErAs||[2] ď E||A||[2] _and the third inequality uses the result of Step E.1._
**Step E.3. During this step, we derive a bound on a vector norm after a single update:**

E||θt ´ θ[˚]||[2] “ E||θt´1 ´ θ[˚] ` p´ηvtq||[2]

“ ||θt´1 ´ θ[˚]||[2] ´ 2ηpθt´1 ´ θ[˚]q[T] Evt ` η[2]E||vt||[2]

ď ||θt´1 ´ θ[˚]||[2] ´ 2ηpθt´1 ´ θ[˚]q[T] _g¯pθt´1q ` 4η[2]f_ pθt´1q ` 4η[2]f pθ[˜]q ` 2η[2]E||e||[2]

“ ||θt´1 ´ θ[˚]||[2] ´ 2ηpθt´1 ´ θ[˚]q[T] _∇f_ pθt´1q ´ 2ηpθt´1 ´ θ[˚]q[T] _e_

` 4η[2]f pθt´1q ` 4η[2]f pθ[˜]q ` 2η[2]E||e||[2]


_Rearranging terms we obtain:_

E||θt ´ θ[˚]||[2] ` 2ηf pθt´1q ´ 4η[2]f pθt´1q

ď ||θt´1 ´ θ[˚]||[2] ` 4η[2]f pθ[˜]q ´ 2ηpθt´1 ´ θ[˚]q[T] _e ` 2η[2]E||e||[2]_

ď ||θt´1 ´ θ[˚]||[2] ` 4η[2]f pθ[˜]q ` 2η||θt´1 ´ θ[˚]|| ¨ ||e|| ` 2η[2]E||e||[2]


**Step E.4. Now derive a bound on epoch update. We assume that quantity ||θt´1 ´ θ[˚]|| might be**
_bounded by constant Z. Similarly, we denote an error term from previous epoch as e[m][´][1]. We use_
_the similar logic as during the proof of theorem 1. Since error term doesn’t change over the epoch,_
_thus, summing over the epoch we have:_


-----

E||θm ´ θ[˚]||[2] ` 2ηM Ef pθ[˜]mq ´ 8η[2]M Ef pθ[˜]mq ď

E||θ0 ´ θ[˚]||[2] ` 8η[2]M Ef pθ[˜]q ` 2MηZE||e[m][´][1]|| ` 2η[2]M E||e[m][´][1]||[2]

_Rearranging terms we have the bound:_

1 4η 1
Ef pθ[˜]mq ď p

_λmin2ηM_ p1 ´ 4ηq [`] 1 ´ 4η [q][E][f] [p][θ][˜][m][´][1][q `] 1 ´ 4η [p][Z][E][||][e][m][´][1][|| `][ η][E][||][e][m][´][1][||][2][q]


To obtain convergence, we need to guarantee geometric convergence of first and second term in the
sum separately. The first term is dependent on inner loop updates, its convergence is analyzed in
Theorem 1. Here we show how to achieve a similar geometric convergence rate of the second term.
Since error term has 0 mean and it is finite sample case with replacement, expected squared norm
might be bounded by:

E||e[m]||[2] ď _[N][ ´][ n][m]_ _S[2]_ ď p1 ´ _[n][m]_ ď _[S][2]_

_Nnm_ _N_ [q][ S]nm[2] _nm_


where S[2] is a bound on update vector norm variance. If we want the error to be bounded by cρ[2][m],
we need the number of batch computations nm to satisfy the condition:

_S[2]_
_nm ě_ _cρ[2][m]_

Satisfying this condition guarantees that the second term has geometric convergence:


1 2

1 ´ 4η [p][Z][E][||][e][m][´][1][|| `][ η][E][||][e][m][´][1][||][2][q ď] 1 ´ 4η [max][p][Z][?][c, ηcρ][q][ρ][m]

It is only left to derive a bound S[2] for on update vector norm sample variance:


1

_N ´ 1_


ÿ

||gs,s1pθq||[2] ´ ||g¯pθq||[2] ď
_s,s[1]_


_N_ 1

_N ´ 1_ _N_

_N_ 1

_N ´ 1_ _N_


ÿ _N_ 1

||gs,s1pθq||[2] “
_s,s[1]_ _N ´ 1_ _N_


ÿ

2||rϕpsq||[2] ` 4||γϕps[1]q[T] _θϕpsq||[2]_ ` 4||ϕpsq[T] _θϕpsq||[2]_ ď
_s,s[1]_


ÿ

||prps, s[1]q ` γϕps[1]q[T] _θ ´ ϕpsq[T]_ _θqϕpsq||[2]_ ď
_s,s[1]_


_N_ _N_

_N ´ 1_ [p][2][|][r][max][|][2][ `][ 4][γ][2][||][θ][||][2][ `][ 4][||][θ][||][2][q “] _N ´ 1_ [p][2][|][r][max][|][2][ `][ 8][||][θ][||][2][q “][ S][2]

### F PROOF OF THEOREM 4

TD-SVRG algorithm for iid sampling case is described as Algorithm 3:

The proof of its convergence is very similar to E, the only difference is that now we derive expectation with respect to MDP instead of fixed dataset.


-----

**Algorithm 3 TD-SVRG for iid sampling case**

**Parameters update frequency M and learning rate η**
**Initialize** _θ[˜]0._
**Iterate: for m “ 1, 2, . . .**

_θ “_ _θ[˜]m´1,_
choose batch size nm,
sample batch D[m] of size nm,
compute µ “ _n1m_ řs,s[1]PD[m][ g][s,s][1][p][θ][q][,]

where gs,s1pθq “ prps, s[1]q ` γϕps[1]q[T] _θ ´ ϕpsq[T]_ _θqϕpstq,_
_θ0 “_ _θ[˜]._
**Iterate: for t “ 1, 2, . . ., M**

Randomly sample s, s[1] and compute update vector
_vt “ gs,s1pθt´1q ´ gs,s1pθ[˜]q ` µ,_
Update parameters θt “ θt´1 ´ ηvt.

**end**
set _θ[˜]m “ θt for randomly chosen t P p0, . . ., M ´ 1q._

**end**


**Step F.1. During the first step we use the bound obtained during the proof of theorem 1:**

_wpθq “ pθ ´ θ[˚]q[T]_ _Es,s1rpγϕps[1]q ´ ϕpsqqϕpsq[T]_ _ϕpsqpγϕps[1]q ´ ϕpsqq[T]_ spθ ´ θ[˚]q

“ pθ ´ θ[˚]q[T][ “ ÿ] _µπpsqP_ ps, s[1]qpγϕps[1]q ´ ϕpsqqϕ[T] psqϕpsqpγϕps[1]q ´ ϕpsqq[T][ ‰]pθ ´ θ[˚]q

_s,s[1]_

ď pθ ´ θ[˚]q[T][ “ ÿ] _µπpsqP_ ps, s[1]qpγϕps[1]q ´ ϕpsqqpγϕps[1]q ´ ϕpsqq[T][ ‰]pθ ´ θ[˚]q

_s,s[1]_


(5)


“ pθ ´ θ[˚]q[T][ “ ÿ] _µπpsqP_ ps, s[1]qpγ[2]ϕps[1]qϕps[1]q[T] ´ γϕps[1]qϕpsq[T] q‰pθ ´ θ[˚]q ` f pθq

_s,s[1]_


“ pθ ´ θ[˚]q[T][ ÿ] _µπpsqP_ ps, s[1]qpγ[2]ϕpsqϕpsq[T] ´ γϕpsqϕps[1]q[T] q‰pθ ´ θ[˚]q ` f pθq

_s,s[1]_

ď 2f pθq,

_first inequality uses Assumption 2, third equality uses the fact that µπ is a stationary distribution of_
_P ([ř]s[1][ γ][2][µ][π][p][s][q][P]_ [p][s, s][1][q][ϕ][p][s][1][q][ϕ][p][s][1][q][T][ “][ ř]s[1][ γ][2][µ][π][p][s][1][q][ϕ][p][s][1][q][ϕ][p][s][1][q][T][ “][ ř]s _[µ][π][p][s][q][γ][2][ϕ][p][s][q][ϕ][p][s][q][T][ ). The]_

_last inequality uses the fact that γ ă 1._

**Step F.2. During this step we derive a bound on the squared norm of a single update Er||vt||[2]s.**
_Similarly with E we assume inexact computation of mean-path update µ “ ¯gpθq`_ _e. Thus the single_
_update vector becomes:_

_vt “ gpθt´1q ´ gpθ[˜]q ` ¯gpθ[˜]q ` e_

_Norm of this vector is bounded by:_


-----

Er||vt||[2]s “ E||gpθt´1q ´ gpθ[˜]q ` ¯gpθ[˜]q ` e||[2]

“ E||pgpθt´1q ´ ¯gpθ[˚]qq ` pg¯pθ[˚]q ´ gpθ[˜]q ` ¯gpθ[˜]q ` eq||[2]

ď 2E||pgpθt´1q ´ gpθ[˚]qq||[2] ` 2E||gpθ[˜]q ´ gpθ[˚]q ´ pg¯pθ[˜]q ´ ¯gpθ[˚]qq ´ e||[2]

“ 2E||pgpθt´1q ´ gpθ[˚]qq||[2] ` 2E||gpθ[˜]q ´ gpθ[˚]q ´ Ergpθ[˜]q ´ gpθ[˚]qs ´ e||[2]

“ 2E||pgpθt´1q ´ gpθ[˚]qq||[2] ` 2E||gpθ[˜]q ´ gpθ[˚]q ´ Ergpθ[˜]q ´ gpθ[˚]qs||[2]

´ 2Exgpθ[˜]q ´ gpθ[˚]q ´ 4Ergpθ[˜]q ´ gpθ[˚]qs, ey ` 2E||e||[2]

ď 2E||pgpθt´1q ´ gpθ[˚]qq||[2] ` 2E||gpθ[˜]q ´ gpθ[˚]q||[2] ` 2E||e||[2]

“ 2wpθt´1q ` 2wpθ[˜]q ` 2E||e||[2]

ď 4f pθt´1q ` 4f pθ[˜]q ` 2E||e||[2]

_where first inequality uses E||A ` B||[2]_ ď 2E||A||[2] ` 2E||B||[2], second inequality uses E||A ´
ErAs||[2] ď E||A||[2] _and the third inequality uses the result of Step F.1._
**Step F.3. Bound on a vector norm after a single update:**

E||θt ´ θ[˚]||[2] “ E||θt´1 ´ θ[˚] ` p´ηvtq||[2]

“ ||θt´1 ´ θ[˚]||[2] ´ 2ηpθt´1 ´ θ[˚]q[T] Evt ` η[2]E||vt||[2]

ď ||θt´1 ´ θ[˚]||[2] ´ 2ηpθt´1 ´ θ[˚]q[T] _g¯pθt´1q ` 4η[2]f_ pθt´1q ` 4η[2]f pθ[˜]q ` 2η[2]E||e||[2]

“ ||θt´1 ´ θ[˚]||[2] ´ 2ηpθt´1 ´ θ[˚]q[T] _∇f_ pθt´1q ´ 2ηpθt´1 ´ θ[˚]q[T] _e_

` 4η[2]f pθt´1q ` 4η[2]f pθ[˜]q ` 2η[2]E||e||[2]


_Rearranging terms we obtain:_

E||θt ´ θ[˚]||[2] ` 2ηf pθt´1q ´ 4η[2]f pθt´1q

ď ||θt´1 ´ θ[˚]||[2] ` 4η[2]f pθ[˜]q ´ 2ηpθt´1 ´ θ[˚]q[T] _e ` 2η[2]E||e||[2]_

ď ||θt´1 ´ θ[˚]||[2] ` 4η[2]f pθ[˜]q ` 2η||θt´1 ´ θ[˚]|| ¨ ||e|| ` 2η[2]E||e||[2]


**Step F.4. Now derive a bound on epoch update. We assume that quantity ||θt´1 ´ θ[˚]|| might be**
_bounded by constant Z. Similarly, we denote an error term from previous epoch as e[m][´][1]. We use_
_the similar logic as during the proof of Theorem 1. Since error term doesn’t change over the epoch,_
_thus, summing over the epoch we have:_

E||θm ´ θ[˚]||[2] ` 2ηM Ef pθ[˜]mq ´ 8η[2]M Ef pθ[˜]mq ď

E||θ0 ´ θ[˚]||[2] ` 8η[2]M Ef pθ[˜]q ` 2MηZE||e[m][´][1]|| ` 2η[2]M E||e[m][´][1]||[2]

_Rearranging terms we have the bound:_


1 4η 1
Ef pθ[˜]mq ď p

_λmin2ηM_ p1 ´ 4ηq [`] 1 ´ 4η [q][E][f] [p][θ][˜][m][´][1][q `] 1 ´ 4η [p][Z][E][||][e][m][´][1][|| `][ η][E][||][e][m][´][1][||][2][q]

Similarly to C convergence for the first term might be obtained by setting learning rate η “ 1{8 and
number of inner loop iterations M “ 16{λmin. To guarantee convergence of the second term, we
need to bound E||e[m]||[2]. In the infinite population with replacement case norm of the error vector is
bounded by:

E||e[m]||[2] ď _[S][2]_

_nm_


-----

where S[2] is a bound update vector norm variance. If we want the error to be bounded by cρ[2][m], we
need the number of batch computations nm to satisfy the condition:

_S[2]_
_nm ě_ _cρ[2][m]_

Satisfying this condition guarantees that the second term has geometric convergence:


1 2

1 ´ 4η [p][Z][E][||][e][m][´][1][|| `][ η][E][||][e][m][´][1][||][2][q ď] 1 ´ 4η [max][p][Z][?][c, ηcρ][q][ρ][m]

Similarly to E, bound on sample variance S[2] might be derived as follows:

ÿ

_µπpsqP_ ps, s[1]q||gs,s[1]pθq||[2] ´ ||g¯pθq||[2] ď
_s,s[1]_


ÿ ÿ

_µπpsqP_ ps, s[1]q||gs,s1pθq||[2] “ _µπpsqP_ ps, s[1]qp||prps, s[1]q ` γϕps[1]q[T] _θ ´ ϕpsq[T]_ _θqϕpsq||[2]q ď_
_s,s[1]_ _s,s[1]_

ÿ

_µπpsqP_ ps, s[1]qp2||rϕpsq||[2] ` 4||γϕps[1]q[T] _θϕpsq||[2]_ ` 4||ϕpsq[T] _θϕpsq||[2]q ď_
_s,s[1]_

p2|rmax|[2] ` 4γ[2]||θ||[2] ` 4||θ||[2]q “ p2|rmax|[2] ` 8||θ||[2]q “ S[2]


### G MARKOVIAN SAMPLING CASE ALGORITHM AND ANALYSIS.

Markovian sampling case is the hardest to analyse due to its dependence on MDP properties, which
makes establishing bounds on various quantities used during the proof much harder. Applying
gradient splitting view helps to improve over existing bounds but derived algorithm does not have
a nice property of constant learning rate. To deal with sample-to-sample dependencies with utilize
one more assumption often used in the literature:

**Assumption 4. The considered MDP is irreducible and aperiodic and there exist constant m ą 0**
_and ρ P p0, 1q such that_

sup _dT V pPpst P ¨|s0 “ sq, πq ď mρ[t], @t ě 0,_
_sPS_

_where dT V pP, Qq denotes the total-variation distance between the probability measures P and Q._

Another thing we need to employ is projection, which will help to set a bound on update vector v.
Following Bhandari et al. (2018) and Xu et al. (2020) after each iteration we project parameter vector
on a ball or radius R (denoted as ΠRpθq “ arg minθ1:|θ1|ďR |θ ´ θ[1]|[2]. We assume that |θ ˚ | ď R,
choice of R which guarantees it might be found in Bhandari et al. (2018), Section 8.2. Adding
projection results in Algorithm 4.

Guarantees of convergence of Algorithm 4 are given in Theorem 5.

**Theorem 5. Suppose Assumptions 1, 2, 4 hold, then output of Algorithm 4 will satisfy:**

8C

_Erf_ pθ[˜]sqs ď p [3] ` 4ηp2G[2]p4 ` 6τ _[mix]pηqq ` 9R[2]q,_

4 [q][s][f] [p][θ][0][q `] _λminnm_


_where C “_ [4][p][1][`p]p1[m]´ρ[´]q[1][q][ρ][q] r4R[2] ` rmax[2] [s][.]

_Proof. The proof is given in Appendix G.1._


Theorem 5 implies that if we choose s “ Oplogp1{ϵqq, nm “ Op1{pλminϵqq and η “ Opϵ{ logp1{ϵq
and M “ Op [log]ϵλ[p]min[1][{][ϵ][q]

[q][, total sample complexity to achieve accuracy of][ ϵ][ is:]


-----

**Algorithm 4 TD-SVRG with batching for Markovian sampling case**

**Parameters update frequency M**, learning rate η, projection radius R and batch size nm
**Initialize** _θ[˜]0._
**Iterate: for m “ 1, 2, . . .**

_θ “_ _θ[˜]m´1,_
sample trajectory D[m] of length nm,
compute µ “ _n1m_ řs,s[1]PD[m][ g][s,s][1][p][θ][q][,]

where gs,s1pθq “ prps, s[1]q ` γϕps[1]q[T] _θ ´ ϕpsq[T]_ _θqϕpstq,_
_θ0 “_ _θ[˜]._
**Iterate: for t “ 1, 2, . . ., M**

Randomly sample s, s[1] and compute update vector
_vt “ gs,s1pθt´1q ´ gs,s1pθ[˜]q ` µ,_
Update parameters θt “ ΠRpθt´1 ´ ηvtq.

**end**
set _θ[˜]m “ θt for randomly chosen t P p0, . . ., M ´ 1q._

**end**


_Op_ [log][2][p][1][{][ϵ][q] q

_ϵλmin_

In the most practical application his result is better than Op _ϵλ1[2]min_ [log][p][1][{][ϵ][qq][, since][ log][p][1][{][ϵ][q{][λ][min][ ą][ 1]

for practical values of ϵ and λmin.


G.1 PROOF OF THEOREM 5

In the Markovian sampling case, we cannot simply apply Lemma 1; due to high estimation bias
the bounds on f pθq and wpθq will not be derived based on current value of θ, but based on global
constraints on the updates guaranteed by applying projection.

First, we analyse a single iteration on step t of epoch m, during which we apply the update vector
_vt “ gtpθq ´ gtpθ[˜]q ` µpθ[˜]q. The update takes the form:_

_E||θt ´ θ[˚]||2[2]_ [“][ E][||][Π][R][p][θ][t][´][1] [`][ ηv][t][q ´][ Π][R][p][θ][˚][q||]2[2] [ď][ E][||][θ][t][´][1] [´][ θ][˚][ ` p´][ηv][t][q||]2[2] [“]

||θt´1 ´ θ[˚]||2[2] [`][ 2][η][p][θ][t][´][1] [´][ θ][˚][q][T][ E][r][v][t][s `][ η][2][E][||][v][t][||]2[2] [“] (6)

||θt´1 ´ θ[˚]||2[2] [`][ 2][η][p][θ][t][´][1] [´][ θ][˚][q][T][ p][E][r][g][t][p][θ][t][´][1][qs ´][ E][r][g][t][p][θ][˜][qs `][ µ][p][θ][˜][qq`]

_η[2]E||vt||2[2][,]_

where the expectation is taken with respect to s, s[1] sampled during iteration t. Recall that under
Markovian sampling, Ergtpθt´1qs ‰ ¯gpθt´1q and that for the expectation of the estimated meanpath update we have Erµpθ[˜]q|sm´1s ‰ ¯gpθ[˜]q, where sm´1 is the last state of epoch m ´ 1. To
tackle this issue, we follow the approach introduced in a previous works (Bhandari et al. (2018), Xu
et al. (2020)) and rewrite the expectation as a sum of mean-path update and error terms. Similar to
Bhandari et al. (2018), we denote the error term on a single update as ζ:

_ζtpθq “ pθ ´ θ[˚]q[T]_ pgtpθq ´ ¯gpθqq.

For an error term on the trajectory we follow Xu et al. (2020) and denote it as ξ:


_ξmpθq “ pθ ´ θ[˚]q[T]_ pµpθq ´ ¯gpθqq.
Applying this notation, 6 can be rewritten as:
_E||θt ´ θ[˚]||2[2]_ [ď||][θ][t][´][1] [´][ θ][˚][||]2[2][`]

2ηpθt´1 ´ θ[˚]q[T] pErgt´1pθt´1qs ´ Ergtpθ[˜]qs ` µpθ[˜]qq ` η[2]E||vt||2[2] [“]

||θt´1 ´ θ[˚]||2[2] [`][ 2][η]“pErζtpθt´1qs ` pθt´1 ´ θ[˚]q[T] _g¯pθt´1qq´_

pErζtpθ[˜]qs ´ pθt´1 ´ θ[˚]q[T] _g¯pθ[˜]qq`_

pErξpθ[˜]qs ´ pθt´1 ´ θ[˚]q[T] _g¯pθ[˜]qq‰_ ` η[2]E||vt||2[2][.]


(7)


-----

Error terms can be bounded by slightly modified lemmas from the original papers. For ζpθq, we
apply a bound from Lemma 11 in Bhandari et al. (2018):

|Erζtpθqs| ď G[2]p4 ` 6τ _[mix]pηqqη._ (8)

In the original lemma, a bound on Erζtpθqs is stated, however, in the proof a bound on absolute
value of the expectation is also derived.

For mean-path estimation error term, we use a modified version of Lemma 1 in Xu et al. (2020).
The proof of this lemma in the original paper starts by applying the inequality

_a[T]_ _b ď_ _[k]_

2 [||][a][||][2][ `][ 1]2k [||][b][||][2]

to the expression pθ ´ θ[˚]q[T] pµpθq ´ ¯gpθqq, with k “ λA{2 (using the notation in Xu et al. (2020)).
For the purposes of our proof we use k “ λmin. Thus, we will have the expression:

Erξmpθqs ď _[λ][min]_ Er||θ ´ θ[˚]||2[2][|][s][m][´][1][s `][ 4][p][1][ ` p][m][ ´][ 1][q][ρ][q] r4R[2] ` rmax[2] [s “]

2 _λminp1 ´ ρqnm_

(9)

_λmin_ _C_

Er||θ ´ θ[˚]||2[2][|][s][m][´][1][s `] _._
2 _λminnm_


Also, note, that the term E||vt||2[2] [might be bounded as][ E][||][v][t][||]2[2] [ď][ 18][R][2][. Plugging 8 and 9 bounds]
into 7 we obtain:

_E||θt ´ θ[˚]||2[2]_ [ď||][θ][t][´][1] [´][ θ][˚][||]2[2] [´][ 2][ηf] [p][θ][t][´][1][q `][ 4][η][2][G][2][p][4][ `][ 6][τ][ mix][p][η][qq`]

_C_

2ηp _[λ][min]2_ ||θ[˜] ´ θ[˚]||2[2] [`] _λminnm_ q ` 18η[2]R[2].


Summing the inequality over the epoch and taking expectation with respect to all previous history,
we have:

_C_

2ηMErf pθ[˜]sqs ď||θ[˜]s´1 ´ θ[˚]||2[2] [`][ 2][ηM] [p] _[λ][min]2_ ||θ[˜]s´1 ´ θ[˚]||2[2] [`] _λminnm_ q`

_η[2]M_ p4G[2]p4 ` 6τ _[mix]pηqq ` 18R[2]q._

Then we divide both sides by 2ηM and use ||θ[˜]s´1 ´ θ[˚]||2[2] [ď][ 1][{][λ][min][f] [p][θ][˜][s][´][1][q][ to obtain:]

1 _C_
_Erf_ pθ[˜]sqs ďp `

2λminηM [`][ 1]2 [q][f] [p][θ][˜][s][´][1][q `] _λminnm_

_ηp2G[2]p4 ` 6τ_ _[mix]pηqq ` 9R[2]q._

We choose η and M such that ηMλmin “ 2. We then apply this inequality to the value of the
function f in the first term in the right-hand side recursively, which yields the desired result:

_Erf_ pθ[˜]sqs ď p [3] 8C0 ` 4ηp2G[2]p4 ` 6τ _[mix]pηqq ` 9R[2]q_

4 [q][s][f] [p][θ][0][q `] _λminnm_


### H DETAILS ON ALGORITHM IMPLEMENTATION

H.1 COMPARISON OF THEORETIC BATCH SIZES

In this subsection we compare the values of batch sizes which are theoretically required to guarantee
convergence. We compare batch sizes of three algorithms: TD-SVRG, PDSVRG (Du et al. (2017))
and VRTD (Xu et al. (2020)). Note that PDSVRG and VRTD are algorithms for different settings,
but for TD-SVRG the batch size value is the same: 16{λmin, thus, we compare two algorithms in
the same table. We compare the batch sizes required by algorithm for three MDPs: first with 50
state, 20 action and γ “ 0.8, second with 400 state, 10 actions and γ “ 0.95, third with 1000
states, 20 actions and γ “ 0.99, with actions choice probabilities generated from U r0, 1q. (similar
to one used for experiments in Subsections 7.1 and 7.2). Since a batch size is dependent on the
smallest eigenvalue of the matrix A, which is, in turn, is dependent on the dimensionality of the
feature vector, we do the comparison for different feature vector sizes: 5, 10, 20 and 40 randomly


-----

generated features + 1 constant feature for each state. We generate 10 datasets and environments for
each feature size. Our results are summarized in tables 2, 3 and 4

Table 2: Comparison of theory suggested batch sizes for MDP with 50 states, 20 actions and γ “ 0.8.
Values in the first row are feature vectors dimensionality. Value in other rows: bitch size of a
corresponded method (row 1). Values are average over 10 generated datasets and environments.

Method/Features 6 11 21 41

TD-SVRG 2339 6808 21553 4.51 ¨ 10[5]

PD SVRG 1.52 ¨ 10[16] 3.09 ¨ 10[19] 1.85 ¨ 10[23] 1.41 ¨ 10[36]

VRTD 3.07 ¨ 10[6] 2.13 ¨ 10[7] 3.79 ¨ 10[8] 165 ¨ 10[11]

Table 3: Comparison of theory suggested batch sizes for MDP with 400 states, 10 actions and
_γ “ 0.95. Values in the first row are feature vectors dimensionality. Value in other rows: bitch size_
of a corresponded method (row 1). Values are average over 10 generated datasets and environments.

Method/Features 6 11 21 41

TD-SVRG 3176 6942 18100 54688

PD SVRG 1.72 ¨ 10[16] 3.83 ¨ 10[18] 3.06 ¨ 10[21] 5.77 ¨ 10[24]

VRTD 5.41 ¨ 10[6] 2.53 ¨ 10[7] 1.63 ¨ 10[8] 1.58 ¨ 10[9]

Table 4: Comparison of theory suggested batch sizes for MDP with 1000 states, 20 actions and
_γ “ 0.99. Values in the first row are feature vectors dimensionality. Value in other rows: bitch size_
of a corresponded method (row 1). Values are average over 10 generated datasets and environments.

Method/Features 6 11 21 41

TD-SVRG 9206 16096 32723 79401

PD SVRG 7.38 ¨ 10[18] 9.64 ¨ 10[20] 5.14 ¨ 10[23] 4.97 ¨ 10[26]

VRTD 4.35 ¨ 10[7] 1.34 ¨ 10[8] 5.44 ¨ 10[8] 1.45 ¨ 10[9]

H.2 BATCHED SVRG PERFORMANCE

In this set of experiments we compare the performance of TD-SVRG and batched TD-SVRG in
finite-sample case. We generate 10 datasets of size 50000 from the similar MDP as in Section 7.1.
Algorithms also run with the same hyperparameters. Average results over 10 runs presented on
Figure 3 and show, that batched TD-SVRG saves a lot of computations during the earlier epochs,
which provides faster convergence.

|Method/Features|6|11|21|41|
|---|---|---|---|---|
|TD-SVRG|2339|6808|21553|4.51 ¨ 105|
|PD SVRG|1.52 ¨ 1016|3.09 ¨ 1019|1.85 ¨ 1023|1.41 ¨ 1036|
|VRTD|3.07 ¨ 106|2.13 ¨ 107|3.79 ¨ 108|165 ¨ 1011|

|Method/Features|6|11|21|41|
|---|---|---|---|---|
|TD-SVRG|3176|6942|18100|54688|
|PD SVRG|1.72 ¨ 1016|3.83 ¨ 1018|3.06 ¨ 1021|5.77 ¨ 1024|
|VRTD|5.41 ¨ 106|2.53 ¨ 107|1.63 ¨ 108|1.58 ¨ 109|

|Method/Features|6|11|21|41|
|---|---|---|---|---|
|TD-SVRG|9206|16096|32723|79401|
|PD SVRG|7.38 ¨ 1018|9.64 ¨ 1020|5.14 ¨ 1023|4.97 ¨ 1026|
|VRTD|4.35 ¨ 107|1.34 ¨ 108|5.44 ¨ 108|1.45 ¨ 109|


-----

Figure 3: Average performance of TD-SVRG and batching TD-SVRG in finite sample case.
Datasets sampled from MDP environments. Left figure - performance in terms of logpf pθqq. Right
figure - performance in terms of logp|θ ´ θ[˚]|q.


-----


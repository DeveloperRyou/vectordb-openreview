# OFF-POLICY AVERAGE REWARD ACTOR-CRITIC WITH DETERMINISTIC POLICY SEARCH

**Anonymous authors**
Paper under double-blind review

### ABSTRACT

The average reward criterion is relatively less explored as most existing works in
the Reinforcement Learning literature consider the discounted reward criterion.
There are few recent works that present on-policy average reward actor-critic
algorithms, but average reward off-policy actor-critic is relatively less explored. In
this paper, we present both on-policy and off-policy deterministic policy gradient
theorems for the average reward performance criterion. Using these theorems, we
also present an Average Reward Off-Policy Deep Deterministic Policy Gradient
(ARO-DDPG) Algorithm. We show a finite time analysis of the resulting threetimescale stochastic approximation scheme with linear function approximator and
obtain an ϵ-optimal stationary policy with a sample complexity of Ω(ϵ[−][2][.][5]). We
compare the average reward performance of our proposed algorithm and observe
better empirical performance compared to state-of-the-art on-policy average reward
actor-critic algorithms over MuJoCo based environments.

### 1 INTRODUCTION

The reinforcement learning (RL) paradigm has shown significant promise for finding solutions to
decision making problems that rely on a reward-based feedback from the environment. Here one
is mostly concerned with the long-term reward acquired by the algorithm. In the case of infinite
horizon problems, the discounted reward criterion has largely been studied because of its simplicity.
Major recent development in the context of RL in continuous state-action spaces has considered the
discounted reward criterion (Schulman et al., 2015; 2017; Lillicrap et al., 2016; Haarnoja et al., 2018).
However, there are very few works which focus on the average reward performance criterion in the
continuous state-action setting (Zhang & Ross, 2021; Ma et al., 2021).

The average reward criterion has started receiving attention in recent times and there are papers that
discuss the benefits of using this criterion over the discounted reward (Dewanto & Gallagher, 2021;
Naik et al., 2019). One of the reasons being, average reward criteria only considers recurrent states
and it happens to be the most selective optimization criterion in recurrent Markov Decision Processes
(MDPs) according to n-discount optimality criterion. Please refer Mahadevan (1996) for more details
on n-discount optimality criterion. Further, optimization in average reward setting is not dependent
on the initial state distribution. Moreover, the discrepancy between the objective function and the
evaluation metric, that exists for discounted reward setting, is resolved by opting for the average
reward criterion. We encourage the readers to go through Dewanto & Gallagher (2021); Naik et al.
(2019) for better understanding of the benefits mentioned.

There are very few algorithms in literature that optimize the average reward and all of them happen
to be on-policy algorithms (Zhang & Ross, 2021; Ma et al., 2021). It has been demonstrated several
times that on-policy algorithms are less sample efficient than off-policy algorithms Lillicrap et al.
(2016); Haarnoja et al. (2018); Fujimoto et al. (2018) for the discounted reward criterion. In this
paper we try to find whether the same is true for the average reward criterion. We try to overcome
the research gap in development of off-policy average reward algorithms for continuous state and
action spaces by proposing an Average Reward Off-Policy Deep Deterministic Policy Gradient
(ARO-DDPG) Algorithm.

The policy evaluation step in the case of the average reward algorithm is equivalent to finding the
solution to the Poisson equation (i.e., the Bellman equation for a given policy). Poisson equation,
because of its form, does not admit a unique solution but only solutions that are unique up to a constant


-----

term. Further, the policy evaluation step in this case consists of finding not just the Differential
Q-value function but also the average reward. Thus, because of the required estimation of two
quantities instead of one, the role of the optimization algorithm and the target network increases
here. Therefore we implement the proposed ARO-DDPG algorithm by using target network and by
carefully selecting the optimization algorithm.

The following are the broad contributions of our paper:

    - We provide both on-policy and off-policy deterministic policy gradient theorems for the
average reward performance metric.

   - We present our Average Reward Off-Policy Deep Deterministic Policy Gradient (ARODDPG) algorithm.

    - We perform non-asymptotic convergence analysis and provide a finite time analysis of our
three timescale stochastic approximation based actor-critic algorithm using a linear function
approximator.

    - We show the results of implementations using our algorithm with other state-of-the-art
algorithms in the literature.

The rest of the paper is structured as follows: In Section 2, we present the preliminaries on the
MDP framework, the basic setting as well as the policy gradient algorithm. Section 3 presents the
deterministic policy gradient theorem and our algorithm. Section 4 then presents the main theoretical
results related to the finite time analysis. Section 5 presents the experimental results. In Section 6, we
discuss other related work and Section 7 presents the conclusions. The detailed proofs for the finite
time analysis are available in the Appendix.

### 2 PRELIMINARIES

Consider a Markov Decision Process (MDP) M = {S, A, R, P, π} where S ⊂ R[n] is the (continuous)
state space, A ⊂ R[m] is the (continuous) action space, R : S × A �→ R denotes the reward function
with R(s, a) being the reward obtained under state s and action a. Further, P ( _s, a) denotes the_

_·|_
state transition function defined as P : S _A_ _µ(_ ), where µ : (S) [0, 1] is a probability
_×_ _�→_ _·_ _B_ _�→_
measure. Deterministic policy π is defined as π : S _A.In the above,_ (S) represents the Borel
_�→_ _B_
sigma algebra on S. Stochastic policy πr is defined as πr : S �→ _µ[′](·), where µ[′]_ : B(A) �→ [0, 1] and
(A) is the Borel sigma algebra on A.
_B_
**Assumption 1. The Markov process obtained under any policy π is ergodic.**

Assumption 1 is necessary to ensure existence of steady state distribution of Markov process.

2.1 DISCOUNTED REWARD MDPS

In discounted reward MDPs, discounting is controlled by γ (0, 1). The following performance
_∈_
metric is optimized with respect to the policy:


_η(π) = E[π][_


_∞_ �
�

_γ[t]R(st, at)] =_
_t=0_


_ρ0(s)V_ _[π](s) ds._ (1)
_S_


Here, ρ0 is the initial state distribution and V _[π]_ is the value function. Vπ(s) denotes the long term
reward acquired when starting in the state s.

_V_ _[π](st) = E[π][R(st, at) + γV_ _[π](st+1)|st]._ (2)

2.2 AVERAGE REWARD MDPS

The performance metric in the case of average reward MDPs is the long-run average reward ρ(π)
defined as follows:


1
_ρ(π) = lim_
_N_ _→∞_ _N_ [E][π][[]


_N_ _−1_ �
�

_R(st, at)] =_
_t=0_


_d[π](s)R[π](s) ds,_ (3)
_S_


-----

where R[π](s) =△ R(s, π(s)). The limit in the first equality in equation 3 exists because of Assumption
1. The quantity d[π](s) in the second equality in equation 3 corresponds to the steady state probability
of the Markov process being in state s _S and it exists and is unique given π from Assumption 1 as_
_∈_
well.

_Vdiff[π]_ [is the differential value function corresponding to the policy][ π][ and is defined in (4). Further,]
the differential Q-value or action-value function Q[π]diff [is defined in (5).]


_∞_
�

_R(sk, ak) −_ _ρ(π)|st]._ (4)
_k=t_

_∞_
�

_R(sk, ak) −_ _ρ(π)|st, at]._ (5)
_k=t_


_Vdiff[π]_ [(][s][t][) =][ E][π][[]


_Q[π]diff_ [(][s][t][, a][t][) =][ E][π][[]


**Lemma 1. There exists a unique constant k(= ρ(π)) which satisfies the following equation for**
_differential value function Vdiff :_

_Vdiff[π]_ [(][s][t][) =][ E][π][[][R][(][s][t][, a][t][)][ −] _[k][ +][ V][ π]diff_ [(][s][t][+1][)][|][s][t][]] (6)

_Proof. See appendix for the proof._

2.3 POLICY GRADIENT THEOREM

Unlike in Q-learning where we try to find the optimal Q-value function and then infer the policy from
it, the policy gradient theorem (Sutton et al., 1999; Silver et al., 2014; Degris et al., 2012) allows us
to directly optimize the performance metric via its gradient with respect to the policy parameters.
Q-learning can be visualized to be a value iteration scheme while an algorithm based on the policy
gradient theorem can be seen as mimicking policy iteration. Sutton et al. (1999) provided the policy
gradient theorem for on-policy optimization of both the discounted reward and the average reward
algorithms, see (7)-(8), respectively.


�
_∇θη(π) =_


�
_ω[π](s)_
_S_


_∇θπr(a|s, θ)Q[π](s, a) da ds._ (7)
_A_


� �
_∇θρ(π) =_ _d[π](s)_ _∇θπr(a|s, θ)Q[π]diff_ [(][s, a][)][ da ds.] (8)

_S_ _A_

In (7) ω[π] denotes the long term discounted state visitation probability density which is defined
in equation 9 while d[π](s) = limt→∞ _Pt[π][(][s][)][ is the steady state probability density on states.][ P][ π]_
denotes the transition probability kernel for the Markov chain induced by policy π and Pt[π] [is the state]
distribution at instant t given by (10).


_ω[π](s) = (1_ _γ)_
_−_


_∞_
� _γ[t]Pt[π][(][s][)][.]_ (9)

_t=0_


�
_Pt[π][(][s][) =]_


_t−1_

_ρ0(s0)_ � _P_ _[π](sk+1|sk) ds0 . . . dst−1._ (10)
_S×S..._ _k=0_


The policy gradient theorem in Sutton et al. (1999) is only valid for on-policy algorithms. Degris
et al. (2012) proposed an approximate off-policy policy gradient theorem for stochastic policies, see
(11), where d[µ] stands for the steady state density function corresponding to the policy µ.


�
_∇θη(π) ≈_


�
_d[µ](s)_
_S_


_∇θπr(a|s, θ)Q[π](s, a) da ds._ (11)
_A_


Silver et al. (2014) came up with the deterministic policy gradient theorem, see (12), which eventually
led to the development of very successful Deep Deterministic Policy Gradient (DDPG) (Lillicrap
et al., 2016) algorithm and Twin Delayed DDPG (TD3) algorithm (Fujimoto et al., 2018).


�
_∇θη(π) =_ _ω[π](s)∇aQ[π](s, a)|a=π(s)∇θπ(s, θ) ds._ (12)

_S_


-----

### 3 PROPOSED AVERAGE REWARD ALGORITHM

We now propose the deterministic policy gradient theorem for the average reward criterion. The
policy gradient estimator has to be derived separately for both the on-policy and off-policy settings.
Obtaining the on-policy deterministic policy gradient estimator is straight forward but dealing with
the off-policy gradient estimates involves an approximate gradient (Degris et al., 2012).

3.1 ON-POLICY POLICY GRADIENT THEOREM

We cannot directly use the second equality of (3) to derive the policy gradient theorem because of
the inability to take the derivative of steady state density function. Therefore one needs to use (6) to
obtain the average reward deterministic policy gradient theorem.

**Theorem 1. The gradient of ρ(π) with respect to policy parameter θ is given as follows:**


�
_∇θρ(π) =_


_d[π](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds.] (13)
_S_


_Proof. See appendix for the proof._

3.2 COMPATIBLE FUNCTION APPROXIMATION

The result in this section is mostly inspired from Silver et al. (2014). Recall that Q[π]diff [(][s, a][)][ is]
the ‘true’ differential Q-value of the state-action tuple (s, a) under the parameterized policy π.
Now let Q[w]diff [(][s, a][)][ denote the approximate differential][ Q][-value of the][ (][s, a][)][-tuple when function]
approximation with parameter w is used. Lemma 2 says that when the function approximator satisfies
a compatibility condition (cf. (14,15)), then the gradient expression in (13,) is also satisfied by Q[w]diff
in place of Q[π]diff [.]

**Lemma 2. Assume that the differential Q-value function (5) satisfies the following:**

1.∇w∇aQ[w]diff [(][s, a][) =][ ∇][θ][π][(][s, θ][)][.] (14)

_2. Differential Q-value function parameter w = wϵ[∗]_ _[optimizes the following error function:]_


_d[π](s)∥∇aQ[π]diff_ [(][s, a][)][|]a=π(s) _[−∇][a][Q]diff[w]_ [(][s, a][)][|]a=π(s)[∥][2][ ds.] (15)
_S_


_ζ(θ, w) = [1]_

2


�


_Then,_

� �

_d[π](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds][ =]
_S_


_d[π](s)∇aQ[w]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds.][ (16)]
_S_


_Further, in the case when a linear function approximator is used, we obtain_

_∇aQ[w]diff_ [(][s, a][) =][ ∇][θ][π][(][s, θ][)][⊺][w.] (17)

_Proof. See the appendix for a proof._

An important implication of lemma 2 also is that the dimension of the matrix on the left hand side
and the right hand side of (14) should be the same. Hence the dimensions of the parameters θ (used
in the parameterized policy) and w (used to approximate the differential Q-value function) are the
same. Lemma 2 shows that the compatible function approximation theorem has the same form in the
average reward setting as the discounted reward setting.

3.3 OFF-POLICY POLICY GRADIENT THEOREM

In order to derive off-policy policy gradient theorem it is not possible to use the direction adopted
by Degris et al. (2012) for off-policy stochastic policy gradient theorem for the discounted reward
setting. We first mention our proposed approximate off-policy deterministic policy gradient theorem
and then explain why some alternatives would not have worked.


-----

**Assumption 2. For the Markov chain obtained from the policy π, let K(·|·) be the transition kernel**
_and S[π]_ _the steady state measure. Then there exists a > 0 and κ_ (0, 1) such that
_∈_

_DT V (K_ _[t](·|s), S[π](·)) ≤_ _aκ[t], ∀t, ∀s ∈_ _S._

Assumption 2 states that Markov chain generated by a policy π follows uniform ergodicity property.
This assumption is necessary to get an upper bound on the total variation norm of steady state
probability distribution of two policies. This assumption is used in Lemma 12, which in turn is used
for Theorem 2.

**Theorem 2. The approximate gradient of the average reward ρ(π) with respect to the policy**
_parameter θ is given by the following expression:_


�
_∇�θρ(π) =_


_d[µ](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds.] (18)
_S_


_Further, the approximation error is E(π, µ) = ∥∇θρ(π)−∇[�]θρ(π)∥, where µ represents the behaviour_
_policy._ _satisfies_
_E_
(π, µ) _Z_ _θ[π]_ _θ[µ]_ _._ (19)
_E_ _≤_ _∥_ _−_ _∥_

_Here, Z = 2[m][+1]C(⌈logκ a[−][1]⌉_ + 1/κ)Lt with Lt being the Lipchitz constant for the transition
_probability density function (Assumption 9). Constants a and κ are from Assumption 2, m is the_
_dimension of action space, and C = maxs ∥∇aQ[π]diff_ [(][s, a][)][|][a][=][π][(][s][)][∇][θ][π][(][s, θ][)][∥][.]

_Proof. See the appendix for a proof._

Theorem 2 suggests that the approximation error in the gradient increases as the difference between
the target policy π and the behaviour policy µ increases.

3.4 OFF-POLICY ALTERNATIVES

In this section we will talk about what alternatives could be thought of in place of what is suggested
in section 3.3 and why those alternatives would not work.

1. One can possibly take inspiration from Degris et al. (2012) and define an objective function,
_ρ¯(π), as in (20), which is a naive off-policy version of (3)._


�
_ρnew(π) =_


_d[µ](s)R[π](s) ds._ (20)
_S_


If, however, we take the derivative of ρnew(π) defined above, we get the policy update rule
as in (21).

�
_∇θρnew(π) =_ _d[µ](s)∇aR(s, a)|a=π(s)∇θπ(s, θ) ds._ (21)

_S_

The update rule (21) only considers the reward function and not the transition dynamics of
the MDP. In (18), the derivative of the objective function includes the differential Q-value
function which encapsulates both the information of the reward function and the transition
dynamics of the MDP and hence is valid derivative.

2. A lot of work in the off-policy setting relies on importance sampling ratios. Recently a few
works devised a method to estimate the steady state probability density ratio of the target
and behavior policies (Zhang et al., 2020a;b; Liu et al., 2018; Nachum et al., 2019). The
ratio of steady state densities could be used for deterministic policy optimization but there
are certain issues which prohibit its usage, see (22).


�
_∇θρ(π) =_


_d[µ](s)τ_ (s)∇aQ[π]diff [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds.] (22)
_S_


Here, τ (s) is the steady state probability density ratio defined as d[π](s)/d[µ](s). In order to
calculate τ (s) we need information about (π(a _s), µ(a_ _s) and P_ (s[′] _s, a)). We need the ratio_
_|_ _|_ _|_


-----

_π(a_ _s)/µ(a_ _s) and for deterministic policies the ratio would be δ(a_ _π(s)/δ(a_ _µ(s)),_
_|_ _|_ _−_ _−_
where δ( ) is the Dirac-Delta function:

_·_


0 if a = µ(s)
if a = π(s) (23)
_∞_
00 otherwise.


_δ(a_ _π(s))_
_−_
_δ(a_ _µ(s)) [=]_
_−_








From (23), it is clear that the ratio δ(a _π(s)/δ(a_ _µ(s)) will be undefined for almost all_
_−_ _−_
actions a _A. Thus, we cannot use this ratio for deterministic policies. Otherwise, we need_
_∈_
_P_ (s[′] _s, π(a)) and P_ (s[′] _s, µ(a)). It is possible to get the information about P_ (s[′] _s, µ(a)) by_
_|_ _|_ _|_
sampling from the Markov process generated by the policy µ but obtaining this information
about P (s[′] _s, π(a)) is impossible as in the off-policy setting data from π is assumed to be_
_|_
simply unavailable.

3.5 ACTOR-CRITIC UPDATE RULE

**Assumption 3. αt, βt, and γt are the step sizes for critic, target estimator, and actor parameter**
_updates respectively._

_Cα_ _Cβ_ _Cγ_
_αt =_ _βt =_ _γt =_

(1 + t)[σ] (1 + t)[u] (1 + t)[v]


_Here, Cα, Cβ, Cγ > 0 and 0 < σ < u < v < 1. αt is at the fastest timescale, βt is at slower_
_timescale and γt is at the slowest timescale._

The critic and average reward parameters are estimated using the TD(0) update rule but use target
estimators. We are using target estimators to ensure stability of the iterates of the algorithm. Let
_{si, ai, s[′]i[}][n]i=0[−][1]_ [denote the batch of sampled data from the replay buffer.]


� �2
_R(si, ai) −_ _ρt −_ _Q[w]diff[i]_ [(][s][i][, a][i][) + min(][Q][w]diff[1] _[, Q][w]diff[2]_ [)(][s]i[′] _[, π][(][s]i[′]_ _[, θ][t][))]_ _j ∈{1, 2}_

(24)


_ξt[j]_ [= 1]

2


_n−1_
�

_i=0_


_n�−1�_ � �2

_ξt[3]_ [= 1]2 _R(si, ai) −_ _ρt −_ min(Q[w]diff[1] _[, Q][w]diff[2]_ [)(][s][i][, a][i][)] + min(Q[w]diff[1] _[, Q][w]diff[2]_ [)(][s]i[′] _[, π][(][s]i[′]_ _[, θ][t][))]_

_i=0_

(25)
Equation 24 and 25 are the bellman error for differential Q-value function approximator and average
reward estimator respectively. Note here we are using double Q-value function approximator.

_wt[i]+1_ [=][ w]t[i] _[−]_ _[α][t][∇][w]i_ _[ξ]t[i]_ _i ∈{1, 2}_ (26)

_ρt+1 = ρt −_ _αt∇pξt[3]_ (27)

The bellman errors 24 is used to update Q-value function approximator parameters wt[i] [using 26 and]
the bellman average in 25 is used to update average reward estimator ρt using 27.

_νi = ∇amin(Q[w]diff[1]_ _[, Q][w]diff[2]_ [)(][s][i][, a][)][|][a][=][π][(][s]i[)][∇][θ][π][(][s][i][, θ][t][)] (28)


_θt+1 = θt + γt_


�n−1
�

_νi_
_i=0_


�
(29)


Actor update is performed using theorem 2. Actor parameter, θt, is updated using empirical estimate
(28) of the gradient in 18.

_wt[i]+1_ [=][ w]t[i] [+][ β][t][(][w]t[i]+1 _[−]_ _[w]t[i][)]_ _i ∈{1, 2}_ (30)

_ρt+1 = ρt + βt(ρt+1_ _ρt)_ (31)
_−_

_θt+1 = θt + βt(θt+1_ _θt)_ (32)
_−_

Equation 30-32 are used to update the target Q-value function approximator wt[i][, target average reward]
estimator ρt and target actor parameter θt.


-----

### 4 FINITE TIME ANALYSIS

In this section we present the finite time analysis of the on-policy and off-policy average reward
actor critic algorithm with linear function approximators. First we mention the assumptions taken to
perform the finite time analysis followed by the main results.

**Assumption 4. ϕ[π](s)�** = ϕ(s, π(s)� _denotes the feature vector of state s and satisfies ∥ϕ[π](s)∥≤_ 1.

The assumption above is just taken for the sake of convenience.
**Assumption 5. The reward function is uniformly bounded, viz., |R[π](s)| ≤** _Cr < ∞._

Assumption 5 is required to make sure that the average reward objective function is bounded from
above.
**Assumption 6. Q[w]diff** [(][s, a][)][ is Lipchitz continuous w.r.t to][ a][.] _Thus, ∀w_ _∥Q[w]diff_ [(][s, a][1][)][ −]
_Q[w]diff_ [(][s, a][2][)][∥≤] _[L][a][∥][a][1][ −]_ _[a][2][∥][.]_

Continuity of approximate Q-value function w.r.t action is enforced using Assumption 6. Without the
continuity property approximate differential Q-values will not generalize for unseen action values.
**Assumption 7. Parameterised policy π(s, θ) is Lipchitz continuous w.r.t θ. Thus, ∥π(s, θ1) −**
_π(s, θ2)∥≤_ _Lπ∥θ1 −_ _θ2∥._

Assumption 7 is a common regularity assumption for convergence of actor. It can be found in Wu
et al. (2020), Xiong et al. (2022) and Zou et al. (2019).

**Assumption 8. The state feature mapping (ϕ[π](s) = ϕ(s, π(s)) defined for a policy π with parameter**
_θ is Lipschitz continuous w.r.t θ. Thus, maxs ∥ϕ[π][1]_ (s) − _ϕ[π][2]_ (s)∥≤ _Lϕ∥θ1 −_ _θ2∥._

Continuity of state action feature w.r.t action is required to ensure generalisation of Q-values to
unseen action values. Using this continuity of state action feature with Assumption 7 we can satisfy
Assumption 8.

4.1 ON-POLICY ANALYSIS

In this section we present the theorem for finite time analysis of the on-policy version of the algorithm
with linear function approximator and target estimator for the critic and average reward.

**Theorem 3. The on-policy average reward actor critic algorithm (Algorithm 2) obtains an ϵ-accurate**
_optimal point with sample complexity of Ω(ϵ[−][2][.][5]). We obtain_


� 1
min
0≤t≤T −1 _[E][||∇][θ][ρ][(][θ][t][)][||][2][ =][ O]_ _T_ [0][.][4]


�
+ (1),
_O_


_ϵ +_ (1).
_≤_ _O_

_Proof. See the appendix for a proof._

We want to reach as close as possible to a value of θ such that ∥∇θρ(θ)∥ = 0, which indicates we
have found a local maxima. (1) term is present in the bound because of using linear function
_O_
approximation and will not reduce as time increases. However, if the (1) term is small enough, the
_O_
bound in Theorem 3 shows that as T is increases, the algorithm will get close to the local maxima of
the objective function(3). A similar (1) term is present in (Xiong et al., 2022). Xiong et al. claims
_O_
the term will be small upon using neural network for critic.

4.2 OFF-POLICY ANALYSIS

In this section we present the theorem for finite time analysis of off-policy version of the algorithm
with linear function approximator and target estimator for the critic and average reward.

**Theorem 4. The off-policy average reward actor critic algorithm (Algorithm 3) with behavior policy**
_µ obtains an ϵ-accurate optimal point with sample complexity of Ω(ϵ[−][2][.][5]). Here θµ refers to the_
_behavior policy parameter and θt refers to the target or current policy parameter. We obtain_


-----

� 1
0≤mint≤T −1 _[E][∥]∇[�]θρ(θt)∥[2]_ = O _T_ [0][.][4]


�
+ O(1) + O(Wθ[2][)]


_≤_ _ϵ + O(1) + O(Wθ[2][)]_
_where Wθ := maxt_ _∥θµ −_ _θt∥._

_Proof. See the appendix for a proof._

The significance of finding a bound on ∥∇[�]θρ(θt)∥ is same as explained above for Theorem 3. The
error bound in the off-policy algorithm has an extra term O(Wθ[2][)][. The extra term denotes the error]
induced because of not using the samples from the current policy for performing updates. Wθ[2] [will]
be small when replay buffer is used because replay buffer contains data from policies similar to the
current policy. This explains why Theorem 2 can be used with replay buffer.

### 5 EXPERIMENTAL RESULTS

We conducted experiments on six different environments using the DeepMind control suite (Tassa
et al., 2018) and found the performance of ARO-DDPG to be superior than the other algorithms. All
the environments selected are infinite horizon tasks. Maximum reward per time step is 1.None of
the tasks have a goal reaching nature. We performed all the experiments using 10 different seeds.
We show here performance comparisons with two state-of-the-art algorithms: the Average Reward
TRPO (ATRPO) (Zhang & Ross, 2021) and the Average Policy Optimization (APO) (Ma et al., 2021)
respectively. In general for the average reward performance, not many algorithms are available in
the literature. We implemented the ATRPO algorithm using the instructions available in the original
paper. We used the original hyper-parameters suggested by the author for ATRPO.

Figure 1: Comparison of performance of different average reward algorithms

For our proposed algorithm we trained the agent for 1 million time steps and evaluated the agent after
every 5,000 time steps in the concerned environment. The length of each episode for the training
phase was taken to be 1,000 and for the evaluation phase it was taken to be 10,000. The reason
for taking longer episode length for evaluation phase was to compare the long term average reward
performance of the algorithms. We also tried using episode length of 10,000 for training phase and
found that to be giving poor average reward performance. We do not reset the agent if it lands in a


-----

state before completing 10,000 steps from where it is unable to escape of its own, while continuing
to give a penalty for the remaining length of the episode. That way the cost of failure is very high.
While training we updated the actor after performing a fixed number of environment steps. We
updated the critic neural network with more frequency as compared to the actor neural network. We
used target actor and critic networks along with target estimator of the average reward parameter for
stability while using bootstrapping updates. We updated the target network using polyak averaging.
We tried to enforce multiple timescales in our algorithm by using different update frequency for actor,
critic and polyak averaging for target networks. We also borrowed the double Q-network trick from
Fujimoto et al. (2018). Complete information regarding the set of hyper-parameters used is provided
in the appendix.

### 6 RELATED WORK

Actor-Critic algorithms for average reward performance criterion is much less studied compared to
discounted reward performance criterion. One of the earliest works on the average reward criterion is
Mahadevan (1996). In this paper, Mahadevan compares the performance of R-learning with that of
Q-learning and concludes that fine tuning is required to get better results from R-learning. R-learning
is the average reward version of Q-learning. Later in 1999, Sutton et al. derived the policy gradient
theorem for both discounted and average reward criteria (Sutton et al., 1999), which formed the
bedrock for development of the average reward actor-critic algorithms. The first proof of asymptotic
convergence of average reward actor-critic algorithms with function approximation appeared in
Konda & Tsitsiklis (2003). In 2007, Bhatnagar et al. proposed incremental natural policy gradient
algorithms for the average reward setting and provided the asymptotic convergence proof of these.

Recently, Wan et al. presented a Differential Q-learning algorithm and claimed that their algorithm is
able to find the exact differential value function without an offset. Further, Wan et al. provided an
extension of the options framework from the discounted setting to the average reward setting and
demonstrated the performance of the algorithm in the Four-Room domain task. One of the major
contributions in off-policy policy evaluation is made by Zhang et al. (2021a). Here Zhang et al. gave
a convergent off-policy evaluation scheme inspired from the gradient temporal difference learning
algorithms but involving a primal-dual formulation making the policy evaluation step feasible for
a neural network implementation. Zhang et al. (2021b) provided another convergent off-policy
evaluation algorithm using target network and l2-regularisation. In our work we use the same policy
evaluation update.

Our work in this paper is actually an extension of the work of Silver et al. (2014) from the discounted
to the average reward setting. In Xiong et al. (2022), a finite time analysis for deterministic policy
gradient algorithm was done for the discounted reward setting. We performed the finite time analysis
for the average reward deterministic policy gradient algorithm and in particular obtain the same
sample complexity for our algorithm as reported by Wu et al. (2020) for stochastic policies.

### 7 CONCLUSION AND FUTURE WORK

In this paper we presented a deterministic policy gradient theorem for both on-policy and off-policy
settings. We then proposed the Average Reward Off-policy Deep Deterministic Policy Gradient(ARODDPG) algorithm using neural network and replay buffer for high dimensional MuJoCo based
environments. We observed superior performance of ARO-DDPG over existing average reward
algorithms (ATRPO and PPO). At the end we provided a finite time analysis for the on-policy and
off-policy algorithms obtained from the proposed policy gradient theorem and obtained a sample
complexity of Ω(ϵ[−][2][.][5]). Lastly to extend the current line of work, one could try using natural gradient
descent based update rule for deterministic policy. Further in the current work we tried optimizing
the average reward performance (gain optimality). In the literature, optimizing the differential value
function for all the states is mentioned as part of achieving Blackwell optimality. Hence actor-critic
algorithms could be designed that not only optimize average reward performance but also differential
value function (bias optimality).


-----

### REFERENCES

Dimitri Bertsekas. Convergence of discretization procedures in dynamic programming. IEEE
_Transactions on Automatic Control, 20(3):415–419, 1975._

Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton. Incremental natural
actor-critic algorithms. Advances in neural information processing systems, 20, 2007.

V. S. Borkar and S. P. Meyn. The o.d.e. method for convergence of stochastic approximation and
reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469, 2000. doi:
[10.1137/S0363012997331639. URL https://doi.org/10.1137/S0363012997331639.](https://doi.org/10.1137/S0363012997331639)

Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer,
2009.

CHEE-S Chow and John N Tsitsiklis. An optimal one-way multigrid algorithm for discrete-time
stochastic control. IEEE transactions on automatic control, 36(8):898–914, 1991.

Thomas Degris, Martha White, and Richard S. Sutton. Linear off-policy actor-critic. In Proceedings
_of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK,_
_[June 26 - July 1, 2012. icml.cc / Omnipress, 2012. URL http://icml.cc/2012/papers/](http://icml.cc/2012/papers/268.pdf)_
[268.pdf.](http://icml.cc/2012/papers/268.pdf)

Vektor Dewanto and Marcus Gallagher. Examining average and discounted reward optimality criteria
[in reinforcement learning. CoRR, abs/2107.01348, 2021. URL https://arxiv.org/abs/](https://arxiv.org/abs/2107.01348)
[2107.01348.](https://arxiv.org/abs/2107.01348)

Francois Dufour and Tomas Prieto-Rumeau. Approximation of average cost markov decision
processes using empirical distributions and concentration inequalities. Stochastics An International
_Journal of Probability and Stochastic Processes, 87(2):273–307, 2015._

Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International
_Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,_
_2018, volume 80 of Proceedings of Machine Learning Research, pp. 1582–1591. PMLR, 2018._
[URL http://proceedings.mlr.press/v80/fujimoto18a.html.](http://proceedings.mlr.press/v80/fujimoto18a.html)

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer G.
Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Ma_chine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, vol-_
ume 80 of Proceedings of Machine Learning Research, pp. 1856–1865. PMLR, 2018. URL
[http://proceedings.mlr.press/v80/haarnoja18b.html.](http://proceedings.mlr.press/v80/haarnoja18b.html)

Vijay R Konda and John N Tsitsiklis. Onactor-critic algorithms. SIAM journal on Control and
_Optimization, 42(4):1143–1166, 2003._

Chandrashekar Lakshminarayanan and Shalabh Bhatnagar. A stability criterion for two timescale
stochastic approximation schemes. Automatica, 79:108–114, 2017.

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations,
_ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL_
[http://arxiv.org/abs/1509.02971.](http://arxiv.org/abs/1509.02971)

Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-horizon off-policy estimation. In Samy Bengio, Hanna M. Wallach, Hugo
Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances
_in Neural Information Processing Systems 31: Annual Conference on Neural Information_
_Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp._
5361–5371, 2018. [URL https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/dda04f9d634145a9c68d5dfe53b21272-Abstract.html)
[dda04f9d634145a9c68d5dfe53b21272-Abstract.html.](https://proceedings.neurips.cc/paper/2018/hash/dda04f9d634145a9c68d5dfe53b21272-Abstract.html)


-----

Xiaoteng Ma, Xiaohang Tang, Li Xia, Jun Yang, and Qianchuan Zhao. Average-reward reinforcement learning with trust region methods. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth
_International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal,_
_Canada, 19-27 August 2021, pp. 2797–2803. ijcai.org, 2021. doi: 10.24963/ijcai.2021/385. URL_
[https://doi.org/10.24963/ijcai.2021/385.](https://doi.org/10.24963/ijcai.2021/385)

Sridhar Mahadevan. Average reward reinforcement learning: Foundations, algorithms, and empirical
[results. Mach. Learn., 22(1-3):159–195, 1996. doi: 10.1023/A:1018064306595. URL https:](https://doi.org/10.1023/A:1018064306595)
[//doi.org/10.1023/A:1018064306595.](https://doi.org/10.1023/A:1018064306595)

A Yu Mitrophanov. Sensitivity and convergence of uniformly ergodic markov chains. Journal of
_Applied Probability, 42(4):1003–1014, 2005._

Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation
of discounted stationary distribution corrections. In Hanna M. Wallach, Hugo Larochelle,
Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Ad_vances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-_
_tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,_
[pp. 2315–2325, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/](https://proceedings.neurips.cc/paper/2019/hash/cf9a242b70f45317ffd281241fa66502-Abstract.html)
[cf9a242b70f45317ffd281241fa66502-Abstract.html.](https://proceedings.neurips.cc/paper/2019/hash/cf9a242b70f45317ffd281241fa66502-Abstract.html)

Abhishek Naik, Roshan Shariff, Niko Yasui, and Richard S. Sutton. Discounted reinforce[ment learning is not an optimization problem. CoRR, abs/1910.02140, 2019. URL http:](http://arxiv.org/abs/1910.02140)
[//arxiv.org/abs/1910.02140.](http://arxiv.org/abs/1910.02140)

John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust region
policy optimization. In Proceedings of the 32nd International Conference on International
_Conference on Machine Learning - Volume 37, ICML’15, pp. 1889–1897. JMLR.org, 2015._

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
[optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347.](https://arxiv.org/abs/1707.06347)

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller. Deterministic policy gradient algorithms. In Proceedings of the 31th International
_Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, volume 32_
[of JMLR Workshop and Conference Proceedings, pp. 387–395. JMLR.org, 2014. URL http:](http://proceedings.mlr.press/v32/silver14.html)
[//proceedings.mlr.press/v32/silver14.html.](http://proceedings.mlr.press/v32/silver14.html)

Richard S. Sutton, David A. McAllester, Satinder Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Sara A. Solla, Todd K. Leen,
and Klaus-Robert Müller (eds.), Advances in Neural Information Processing Systems 12, [NIPS
_Conference, Denver, Colorado, USA, November 29 - December 4, 1999], pp. 1057–1063. The_
[MIT Press, 1999. URL http://papers.nips.cc/paper/1713-policy-gradient-](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation)
[methods-for-reinforcement-learning-with-function-approximation.](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation)

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David
Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A.
[Riedmiller. Deepmind control suite. CoRR, abs/1801.00690, 2018. URL http://arxiv.org/](http://arxiv.org/abs/1801.00690)
[abs/1801.00690.](http://arxiv.org/abs/1801.00690)

Yi Wan, Abhishek Naik, and Rich Sutton. Average-reward learning and planning with options.
In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Ad_vances in Neural Information Processing Systems, volume 34, pp. 22758–22769. Curran As-_
[sociates, Inc., 2021a. URL https://proceedings.neurips.cc/paper/2021/file/](https://proceedings.neurips.cc/paper/2021/file/c058f544c737782deacefa532d9add4c-Paper.pdf)
[c058f544c737782deacefa532d9add4c-Paper.pdf.](https://proceedings.neurips.cc/paper/2021/file/c058f544c737782deacefa532d9add4c-Paper.pdf)

Yi Wan, Abhishek Naik, and Richard S Sutton. Learning and planning in average-reward markov
decision processes. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
_Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp._
[10653–10662. PMLR, 18–24 Jul 2021b. URL https://proceedings.mlr.press/v139/](https://proceedings.mlr.press/v139/wan21a.html)
[wan21a.html.](https://proceedings.mlr.press/v139/wan21a.html)


-----

Yue Frank Wu, Weitong ZHANG, Pan Xu, and Quanquan Gu. A finite-time analysis of two timescale actor-critic methods. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 17617–17628. Curran
[Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/cc9b3c69b56df284846bf2432f1cba90-Paper.pdf)
[cc9b3c69b56df284846bf2432f1cba90-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/cc9b3c69b56df284846bf2432f1cba90-Paper.pdf)

Huaqing Xiong, Tengyu Xu, Lin Zhao, Yingbin Liang, and Wei Zhang. Deterministic policy gradient:
Convergence analysis. In The 38th Conference on Uncertainty in Artificial Intelligence, 2022.

Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of stationary values. In 8th International Conference on Learning Representations,
_[ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020a. URL https:](https://openreview.net/forum?id=HkxlcnVFwB)_
[//openreview.net/forum?id=HkxlcnVFwB.](https://openreview.net/forum?id=HkxlcnVFwB)

Shangtong Zhang, Bo Liu, and Shimon Whiteson. Gradientdice: Rethinking generalized offline estimation of stationary values. In Proceedings of the 37th International Conference on Machine Learn_ing, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learn-_
_[ing Research, pp. 11194–11203. PMLR, 2020b. URL http://proceedings.mlr.press/](http://proceedings.mlr.press/v119/zhang20r.html)_
[v119/zhang20r.html.](http://proceedings.mlr.press/v119/zhang20r.html)

Shangtong Zhang, Yi Wan, Richard S Sutton, and Shimon Whiteson. Average-reward off-policy
policy evaluation with function approximation. In International Conference on Machine Learning,
pp. 12578–12588. PMLR, 2021a.

Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson. Breaking the deadly triad with a target
network. In International Conference on Machine Learning, pp. 12621–12631. PMLR, 2021b.

Yiming Zhang and Keith W. Ross. On-policy deep reinforcement learning for the averagereward criterion. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th Interna_tional Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume_
139 of Proceedings of Machine Learning Research, pp. 12535–12545. PMLR, 2021. URL
[http://proceedings.mlr.press/v139/zhang21q.html.](http://proceedings.mlr.press/v139/zhang21q.html)

Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for SARSA with
linear function approximation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu_ral Information Processing Systems 32:_ _Annual Conference on Neural Information Pro-_
_cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp._
8665–8675, 2019. [URL https://proceedings.neurips.cc/paper/2019/hash/](https://proceedings.neurips.cc/paper/2019/hash/9f9e8cba3700df6a947a8cf91035ab84-Abstract.html)
[9f9e8cba3700df6a947a8cf91035ab84-Abstract.html.](https://proceedings.neurips.cc/paper/2019/hash/9f9e8cba3700df6a947a8cf91035ab84-Abstract.html)

### A APPENDIX

A.1 ADDITIONAL ASSUMPTIONS, PROOFS OF LEMMAS AND THEOREMS

We make the following additional assumptions.

**Assumption 9. The transition probability density function for a policy π with parameter θ is Lipschitz**
_continuous w.r.t θ. Thus, maxs′,s|P_ _[π][1]_ (s[′]|s) − _P_ _[π][2]_ (s[′]|s)| ≤ _Lt∥θ1 −_ _θ2∥._

The above assumption is a standard assumption in theoretical studies in literature. Reference for
those assumptions can be found in Xiong et al. (2022); Bertsekas (1975); Chow & Tsitsiklis (1991)
and Dufour & Prieto-Rumeau (2015).

**Assumption 10. The reward function for a policy π with parameter θ is Lipschitz continuous w.r.t θ.**
_Thus, maxs|R[π][1]_ (s) − _R[π][2]_ (s)| ≤ _Lr∥θ1 −_ _θ2∥._

The above assumption can be satified by using a well defined reward function to ensure Lipchitz
continuity of reward function w.r.t action and then evoking Assumption 7.
**Assumption 11. The initial value of target estimators is bounded. Thus, ∥w¯0∥≤** _Cw and ∥ρ¯0∥≤_
(Cr + 2Cw).


-----

Assumption 11 is used to enforce the stability of the iterates of target estimators.
**Assumption 12. Let A(θ) =** � _d[π](s)(ϕ[π](s)(�_ _P_ _[π](s[′]|s)ϕ[π](s[′]) ds[′]_ _−_ _ϕ[π](s))[⊺]_ _−_ _ηI) ds. λmin is the_
_lower bound on the minimum eigenvalue of A(θ) for all values of θ._

The assumption above is used in Lemma 6 to prove the Lipchitz continuity of optimal critic parameter
_w[∗]_ for a particular value of policy parameter θ with respect to θ.
**Assumption 13. Let A[′](θ) =** � _d[π](s)(ϕ[π](s)(�_ _P_ _[π](s[′]|s)ϕ[π](s[′]) ds[′]_ _−_ _ϕ[π](s))[⊺]) ds. λ[all]max_ _[is the]_
_upper bound on maximum eigenvalue of (A[′](θ) + A[′](θ)[⊺])/2 for all values of θ._

Assumption 13 is used to prove the negative definiteness of the matrix Aθ (defined in Assumption 12)
in Lemma 11.
**Assumption 14. Let Hθ =** �S _[d][π][(][s][)][∇][θ][π][(][s, θ][)][∇][θ][π][(][s, θ][)][⊺]_ _[ds][.][ λ]min[ϵ]_ _[>][ 0][ is the lower bound on the]_

_minimum eigenvalues of Hθ for all values of θ._

The above assumption is used in Lemma 13 to make sure Hθ is invertible and optimal critic parameter
_wϵ[∗]_ [according to compatible function approximation lemma (Lemma 2) can be obtained. Similar]
assumption is present in (Xiong et al., 2022).
**Assumption 15. Let A[µ]off** _′(θ) =_ � _d[µ](s)(ϕ[π](s)(�_ _P_ _[π](s[′]|s)ϕ[π](s[′]) ds[′]_ _−_ _ϕ[π](s))[⊺]) ds. χ[all]max_ _[is the]_
_upper bound on maximum eigenvalue of (A[µ]off_ _′(θ) + A[µ]off_ _′(θ)⊺)/2 for behaviour policy µ and all_
_values of θ._

Assumption 15 is used to prove the negative definiteness of the matrix Aθ (defined in Lemma 15) in
Lemma 16.

**Lemma 1. There exists a unique constant k(= ρ(π)) which satisfies the following equation for**
_differential value function Vdiff :_

_Vdiff[π]_ [(][s][t][) =][ E][π][[][R][(][s][t][, a][t][)][ −] _[k][ +][ V][ π]diff_ [(][s][t][+1][)][|][s][t][]][.]

_Proof._


�
_Vdiff[π]_ [(][s][t][) =][ R][(][s][t][, π][(][s][t][))][ −] _[k][ +]_ _P_ _[π](st+1|st)Vdiff[π]_ [(][s][t][+1][)][ ds][t][+1]

_S_


�
=⇒ _Vdiff[π]_ [(][s][t][)][ −]


_P_ _[π](st+1|st)Vdiff[π]_ [(][s][t][+1][)][ ds][t][+1] [=][ R][(][s][t][, π][(][s][t][))][ −] _[k]_
_S_


�
_P_ _[π](st+1|st)Vdiff[π]_ [(][s][t][+1][)][ ds][t][+1] =
_S_


_T −1_
�

_R(st, π(st)) −_ _kT_
_t=0_


=
_⇒_


_T −1_
�

_t=0_


� �
_Vdiff[π]_ [(][s][t][)][ −]


Integrating w.r.t the stationary distribution d[π] of policy π :


_T −1_
�

_t=0_


� � �

_d[π](st)_ _Vdiff[π]_ [(][s][t][)][ −]
_S_


�
_P_ _[π](st+1|st)Vdiff[π]_ [(][s][t][+1][)][ ds][t][+1] _dst_
_S_


=


_T −1_
�

_t=0_


�


_d[π](st)R(st, π(st)) ds −_ _kT_
_S_


_T −1_
�

_t=0_


�[�]


�
_d[π](st)Vdiff[π]_ [(][s][t][)][ ds][t] _[−]_
_S_


�
_d[π](st+1)Vdiff[π]_ [(][s][t][+1][)][ ds][t][+1]
_S_


=


_T −1_
�

_t=0_


�


_d[π](st)R(st, π(st)) dst −_ _kT_
_S_


Note: ��


�
_S_ _[d][π][(][s][t][)][V][ π]diff_ [(][s][t][)][ ds][t][ −]


�
_S_ _[d][π][(][s][t][+1][)][V][ π]diff_ [(][s][t][+1][)][ ds][t][+1] = 0.


-----

_d[π](st)R(st, π(st)) dst_
_S_


= _k = [1]_
_⇒_

_T_


_T −1_
�

_t=0_


�


_d[π](st)R(st, π(st)) dst_
_S_


1
= _k = lim_
_⇒_ _T →∞_ _T_


_T −1_
�

_t=0_


�


= _k = ρ(π)_ (using (3)).
_⇒_

**Theorem 1. The gradient of ρ(π) with respect to the policy parameter θ is given as follows:**


�
_∇θρ(π) =_

_Proof. Using Lemma 1:_


_d[π](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds.]
_S_


�
_Vdiff[π]_ [(][s][t][) =][ R][(][s][t][, π][(][s][t][))][ −] _[ρ][(][π][) +]_ _P_ _[π](st+1|st)Vdiff[π]_ [(][s][t][+1][)][ ds][t][+1]

_S_

�
=⇒ _Q[π]diff_ [(][s][t][, π][(][s][t][)) =][ R][(][s][t][, π][(][s][t][))][ −] _[ρ][(][π][) +]_ _P_ _[π](st+1|st)Q[π]diff_ [(][s][t][+1][, π][(][s][t][+1][))][ ds][t][+1]

_S_

Differentiating w.r.t θ, we obtain

_∇θQ[π]diff_ [(][s][t][, π][(][s][t][)) =][ ∇][θ][R][(][s][t][, π][(][s][t][))][ −∇][θ][ρ][(][π][)]

�[�] �
+ ∇θ _P_ _[π](st+1|st)Q[π]diff_ [(][s][t][+1][, π][(][s][t][+1][))][ ds][t][+1]

_S_


Note: ∇aρ(π) = ∇a��


= ∇aR(st, a)|a=π(st)∇θπ(st) −∇θρ(π)

�
+ _∇aP_ _[π](st+1|st, a)|a=π(st)∇θπ(st)Q[π]diff_ [(][s][t][+1][, π][(][s][t][+1][))][ ds][t][+1]

_S_

�
+ _P_ _[π](st+1|st)∇θQ[π]diff_ [(][s][t][+1][, π][(][s][t][+1][))][ ds][t][+1][.]

_S_

�
_S_ _[d][π][(][s][)][R][π][(][s][)][ ds]_ = 0.


=⇒∇θQ[π]diff [(][s][t][, π][(][s][t][)) =][ ∇][a][Q]diff[π] [(][s][t][, a][)][|]a=π(st)[∇][θ][π][(][s][t][)][ −∇][θ][ρ][(][π][)]

�
+ _P_ _[π](st+1|st)∇θQ[π]diff_ [(][s][t][+1][, π][(][s][t][+1][))][ ds][t][+1][.]

_S_

Integrating w.r.t stationary distribution d[π]( ) of policy π:

_·_


�


�
_d[π](st)∇θQ[π]diff_ [(][s][t][, π][(][s][t][))][ds][t] [=] _d[π](st)∇aQ[π]diff_ [(][s][t][, a][)][|]a=π(st)[∇][θ][π][(][s][t][)][ds][t] _[−∇][θ][ρ][(][π][)]_
_S_ _S_

� �
+ _d[π](st)_ _P_ _[π](st+1|st)∇θQ[π]diff_ [(][s][t][+1][, π][(][s][t][+1][))][ ds][t][+1] _[ds][t][.]_

_S_ _S_


Note: �


_S_ _[d][π][(][s][)][P][ π][(][s][′][|][s][)][ ds][ =][ d][π][(][s][′][)][. Thus,]_

�
_∇θρ(π) =_ _d[π](st)∇aQ[π]diff_ [(][s][t][, a][)][|]a=π(st)[∇][θ][π][(][s][t][)][ds][t]

_S_

�
+ _d[π](st+1)∇θQ[π]diff_ [(][s][t][+1][, π][(][s][t][+1][))][ ds][t][+1]

_S_

�
_−_ _d[π](st)∇θQ[π]diff_ [(][s][t][, π][(][s][t][))][ds][t][.]

_S_


-----

�
_∇θρ(π) =_


_d[π](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s][)][ ds.]
_S_


**Lemma 2. Assume that the differential Q-value function (5) satisfies the following:**


_1._


_∇w∇aQ[w]diff_ [(][s, a][) =][ ∇][θ][π][(][s, θ][)][.]


_2. The differential Q-value function parameter w = wϵ[∗]_ _[optimizes the following error function:]_


_d[π](s)∥∇aQ[π]diff_ [(][s, a][)][|]a=π(s) _[−∇][a][Q]diff[w]_ [(][s, a][)][|]a=π(s)[∥][2][ ds.]
_S_


_ζ(θ, w) = [1]_

2


�


_Then,_
� �

_d[π](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds][ =]
_S_


_d[π](s)∇aQ[w]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds.]
_S_


_Further,_
_∇aQ[w]diff_ [(][s, a][) =][ ∇][θ][π][(][s, θ][)][⊺][w] (for linear function approximator).

_Proof. Let E(θ, w, s) = ∇aQ[π]diff_ [(][s, a][)][|][a][=][π][(][s][)][ −∇][a][Q][w]diff [(][s, a][)][|][a][=][π][(][s][)][,]


_d[π](s)_ (θ, w, s)[⊺] (θ, w, s) ds.
_E_ _E_
_S_


_ζ(θ, w) = [1]_

2


�


Differentiating w.r.t the critic parameter w, we obtain:


�
_∇wζ(θ, w) =_ _d[π](s)∇wE(θ, w, s)E(θ, w, s) ds_

_S_

� �
= − _d[π](s)∇w∇aQ[w]diff_ [(][s, a][)][|]a=π(s) _∇aQ[π]diff_ [(][s, a][)][|]a=π(s)

_S_

�
_−∇aQ[w]diff_ [(][s, a][)][|]a=π(s) _ds = 0._

Letting ∇w∇aQ[w]diff [(][s, a][)][|][a][=][π][(][s][)][ =][ ∇][θ][π][(][s][)][, we obtain]


�


�
_d[π](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds][ =]
_S_


_d[π](s)∇aQ[w]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds.]
_S_


Let us consider the case of linear function approximator with parameter w, i.e., Q[w]diff [(][s, π][(][s][)) =]
_ϕ[π](s, π(s))[⊺]w._

We know from above,


Thus,


_∇w∇aQ[w]diff_ [(][s, a][)][|]a=π(s) [=][ ∇][θ][π][(][s][)]

(A.1)
=⇒∇aϕ[π](s, a)|a=π(s) = ∇θπ(s).

_Q[w]diff_ [(][s, π][(][s][)) =][ ϕ][π][(][s, π][(][s][))][⊺][w]

=⇒∇aQ[w]diff [(][s, a][)][|]a=π(s) [=][ ∇][a][ϕ][π][(][s, a][)][|][⊺]a=π(s)[w]

=⇒∇aQ[w]diff [(][s, a][)][|]a=π(s) [=][ ∇][θ][π][(][s][)][⊺][w][ (][using][ (][A.][1))][.]


-----

**Theorem 2. The approximate gradient of the average reward ρ(π) with respect to the policy**
_parameter θ is given by the following expression:_


�
_∇�θρ(π) =_


_d[µ](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds.]
_S_


_Further, the approximation error is E(π, µ) = ∥∇θρ(π)−∇[�]θρ(π)∥, where µ represents the behaviour_
_policy._ _satisfies_
_E_
(π, µ) _Z_ _θ[π]_ _θ[µ]_ _._
_E_ _≤_ _∥_ _−_ _∥_

_Here, Z = 2[m][+1]C(⌈logκ a[−][1]⌉_ + 1/κ)Lt with Lt being the Lipchitz constant for the transition
_probability density function (Assumption 9). Constants a and κ are from Assumption 2, m is the_
_dimension of action space, and C = maxs ∥∇aQ[π]diff_ [(][s, a][)][|][a][=][π][(][s][)][∇][θ][π][(][s, θ][)][∥][.]

_Proof._

_E(π, µ) = ∥∇θρ(π) −_ _∇[�]θρ(π)∥_

�
= ∥ _d[π](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds]

_S_

�
_−_ _d[µ](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds][∥]

_S_

�
_≤_ _|d[π](s) −_ _d[µ](s)|∥∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][∥] _[ds]_

_S_

�
_C_ _d[π](s)_ _d[µ](s)_ _ds._
_≤_ _|_ _−_ _|_

_S_


Here, C = maxs ∥∇aQ[π]diff [(][s, a][)][|][a][=][π][(][s][)][∇][θ][π][(][s, θ][)][∥][.][ Thus,]

_E(π, µ) ≤_ _CLd∥θ[π]_ _−_ _θ[µ]∥_ = Z∥θ[π] _−_ _θ[µ]∥_ (using Lemma12).

Here, Z = 2[m][+1]C(⌈logκ a[−][1]⌉ + 1/κ)Lt.

**Lemma 3. Let the cumulative error of on-policy actor be** [�][T]t=0[ −][1] _[E][||∇][θ][ρ][(][θ][t][)][||][2][ and cumulative error]_
_of critic be_ [�]t[T]=0[ −][1] _[E][||][∆][w][t][||][2][.][ θ][t][ and][ w][t][ are the actor and linear critic parameter at time t.Bound on]_
_the cumulative error of on-policy actor is proven using cumulative error of critic as follows:_


_T −1_
�

_E||∆wt||[2]) + 3Cπ[4][(][τ][ 2][ + 4]M [C]w[2]_ _ϵ[∗]_ [)][,]
_t=0_


1

_T_


_T −1_
� _E||∇θρ(θt)||[2]_ _≤_ 2 _[C][r]_ _T_ _[v][−][1]_ + 3Cπ[4][( 1]

_Cγ_ _T_

_t=0_


+ _[C][γ][L][J]_ _[G]θ[2]_ _T_ _[−][v]_

1 _v_
_−_

_Here, Cr is the upper bound on rewards (Assumption 5), Cγ, v are constants used for step size γt_
_(Assumption 3, ∥∇θπ(s)∥≤_ _Cπ (Assumption 7), ∆wt = wt −_ _wt[∗][, τ][ = max][t]_ _[∥][w]t[∗]_ _[−]_ _[w]ϵ,t[∗]_ _[∥][,][ w]ϵ[∗]_ _[is]_
_the optimal critic parameter according to Lemma 2. wt[∗]_ _[is the optimal parameters given by TD(0)]_
_algorithm corresponding to policy parameter θt. Constant Cwϵ∗_ _[is defined in Lemma 13.][ L][J][ is the]_
_coefficient used in smoothness condition of the non convex function ρ(θ). Constant Gθ is defined in_
_Lemma 7. M is the size of batch of samples used to update parameters._

_Proof. By [−LJ_ _, LJ_ ]-smoothness of non-convex function we have:


_E[ρ(θt+1)] ≥_ _E[ρ(θt)] + E⟨∇θρ(θt), θt+1 −_ _θt⟩−_ _[L]2[J]_ _[E][∥][θ][t][+1][ −]_ _[θ][t][∥][2][.]_ (A.2)


-----

Now,


_h(Bt, wt, θt) = [1]_

_M_


�

_∇aQ[π](st,i, a)|a=π(st,i)∇θπ(st,i)._
_i_


_E⟨∇θρ(θt), θt+1 −_ _θt⟩_ = γtE⟨∇θρ(θt), h(Bt, wt, θt)⟩

(A.3)
= γtE⟨∇θρ(θt), h(Bt, wt, θt) −∇θρ(θt)⟩ + γtE∥∇θρ(θt)∥[2].

From (A.3), we have


_E⟨∇θρ(θt), h(Bt, wt, θt) −∇θρ(θt)⟩≥−_ [1]

2 _[E][∥∇][θ][ρ][(][θ][t][)][∥][2][ −]_ [1]2 _[E][∥][h][(][B][t][, w][t][, θ][t][)][ −∇][θ][ρ][(][θ][t][)][∥][2]_

(∵ _x[⊺]y ≥−∥x∥[2]/2 −∥y∥[2]/2)._
(A.4)

From (A.4):

_E∥h(Bt, wt, θt) −∇θρ(θt)∥[2]_

= E∥h(Bt, wt, θt) − _h(Bt, wt[∗][, θ][t][) +][ h][(][B][t][, w]t[∗][, θ][t][)][ −]_ _[h][(][B][t][, w]ϵ,t[∗]_ _[, θ][t][) +][ h][(][B][t][, w]ϵ,t[∗]_ _[, θ][t][)][ −∇][θ][ρ][(][θ][t][)][∥][2]_

_≤_ 3(E∥h(Bt, wt, θt) − _h(Bt, wt[∗][, θ][t][)][∥][2]_ _⃝1_

+ E∥h(Bt, wt[∗][, θ][t][)][ −] _[h][(][B][t][, w]ϵ,t[∗]_ _[, θ][t][)][∥][2]_ _⃝2_

+ E∥h(Bt, wϵ,t[∗] _[, θ][t][)][ −∇][θ][ρ][(][θ][t][)][∥][2]_ _⃝3_
(A.5)

From (A.5):

### ⃝1 :

_E||h(Bt, wt, θt) −_ _h(Bt, wt[∗][, θ][t][)][||][2]_

= [1] � _∇aQ[w][t]_ (st,i, a)|a=π(st,i)∇θπ(st,i) − � _∇aQ[w]t[∗] (st,i, a)|a=π(st,i)∇θπ(st,i)||[2]._

_M_

_[||]_ _i=0_ _i=0_

Here, by compatible function approximation lemma 2: ∇aQ[w]t[∗] (si, a)|a=π(si) = ∇θπ(s)[⊺]w.


_E||h(Bt, wt, θt) −_ _h(Bt, wt[∗][, θ][t][)][||][2][ =][ E][||][ 1]_

_M_


�

_∇θπ(st,i)∇θπ(st,i)[⊺](wt −_ _wt[∗][)][||][2]_
_i=0_


### ⃝2 is similar as 1⃝:

 ⃝3 :


_≤_ _Cπ[4][E][||][w][t]_ _[−]_ _[w]t[∗][||][2][.]_

_E||h(Bt, wt[∗][, θ][t][)][ −]_ _[h][(][B][t][, w]ϵ,t[∗]_ _[, θ][t][)][||][2][ ≤]_ _[C]π[4][E][||][w]t[∗]_ _[−]_ _[w]ϵ,t[∗]_ _[||][2]_

_≤_ _Cπ[4][τ][ 2][.]_



- By compatible function approximation lemma 2: _∇θρ(θt)_ =
�

_S_ _[d][(][s, π][(][θ][t][))][∇][θ][π][(][s][)][∇][θ][π][(][s][)][⊺][w]ϵ,t[∗]_ _[ds][ =][ E][[][h][(][B][t][, w]ϵ,t[∗]_ _[, θ][t][)]]_

- By lemma 4 (Xiong et al., 2022), if E[ Y[ˆ] ] = Y,[¯] _||Y[ˆ] ||, ||Y[¯] || ≤_ _CY then,_


_E_ [1]
_||_

_M_


_M_ _−1_
� _Yˆi −_ _Y¯ || ≤_ 4 _[C]M Y[2]_ _[.]_

_i=0_


-----

Using above two bullet points:

_E||h(Bt, wϵ,t[∗]_ _[, θ][t][)][ −∇][θ][ρ][(][θ][t][)][||][2][ ≤]_ [4] _ϵ,t[||][2]_

_M_

_[||∇][θ][π][(][s][)][∇][θ][π][(][s][)][⊺][w][∗]_

_π[C]w[2]_ _ϵ_ _._
_≤_ [4][C] [4]

_M_


Combining 1⃝, 2⃝ and 3⃝ and using in (A.5):

_E||h(Bt, wt, θt) −∇θρ(θt)||[2]_ _≤_ 3Cπ[4][(][E][||][w][t] _[−]_ _[w]t[∗][||][2][ +][ τ][ 2][ + 4][C]Mw[2]_ _ϵ_ [)][.] (A.6)

Using (A.6) in (A.4):


_E⟨∇θρ(θt), h(Bt, wt, θt) −∇θρ(θt)⟩≥−_ [1]

2 _[E][||∇][θ][ρ][(][θ][t][)][||][2]_

(A.7)

_wϵ_

_−_ [3]2 _[C]π[4][(][E][||][w][t]_ _[−]_ _[w]t[∗][||][2][ +][ τ][ 2][ + 4][C]M[2]_ [)][.]


Using (A.7) in (A.3):

_E⟨∇θρ(θt), θt+1 −_ _θt⟩≥_ _[γ]2[t]_ _[E][||∇][θ][ρ][(][θ][t][)][||][2]_

(A.8)

_wϵ_

_−_ [3]2[γ][t] _[C]π[4][(][E][||][w][t]_ _[−]_ _[w]t[∗][||][2][ +][ τ][ 2][ + 4][C]M[2]_ [)][.]


Using (A.8) in (A.2):

_E[ρ(θt+1)] −_ _E[ρ(θt)] ≥_ _[γ][t]_

2 _[E][||∇][θ][ρ][(][θ][t][)][||][2][ −]_ _[L]2[J]_ _[E][||][θ][t][+1][ −]_ _[θ][t][||][2]_

_wϵ_

_−_ [3]2[γ][t] _[C]π[4][(][E][||][w][t]_ _[−]_ _[w]t[∗][||][2][ +][ τ][ 2][ + 4][C]M[2]_ [)]


=⇒ _E||∇θρ(θt)||[2]_ _≥_ _γ[2]t_ _E[ρ(θt+1)] −_ _E[ρ(θt)] + 3Cπ[4][(][E][||][w][t]_ _[−]_ _[w]t[∗][||][2][)]_

+ 3Cπ[4][(][τ][ 2][ + 4][C]Mw[2] _ϵ_ [) +][ L][J] _[γ][t][G]θ[2]_ (using lemma 7)


=
_⇒_

From equation A.9


_T −1_ _T −1_
� �

_E||∇θρ(θt)||[2]_ _≥_
_t=0_ _t=0_


2

_γt_ _E[ρ(θt+1)] −_ _E[ρ(θt)]_ _⃝1_


_T −1_
�

3Cπ[4][(][E][||][w][t] _[−]_ _[w]t[∗][||][2][)]_ _⃝2_
_t=0_


(A.9)


+

+

+


_T −1_
� _wϵ_

3Cπ[4][(][τ][ 2][ + 4][C]M[2] [)] _⃝3_
_t=0_


_T −1_
�

_LJ_ _γtG[2]θ_ _⃝4_ (using lemma 7)
_t=0_


-----

### ⃝1 :

 ⃝2 :

 ⃝4 :


_T −1_
�

_t=0_


2 �T� −1

_E[ρ(θt+1)] −_ _E[ρ(θt)] = 2_
_γt_

_t=0_


1

� 1 1

_−_
_γt_ _γt−1_


�
_E[ρ(θt)] +_ _[E][[][ρ][(][θ][0][)]]_ _−_ _[E][[][ρ][(][θ][T][ )]]_

_γ0_ _γT −1_


�


�T� −1� 1 1
2
_≤_ _−_

_t=0_ _γt_ _γt−1_

�T� −1� 1 1
2
_≤_ _−_

_t=0_ _γt_ _γt−1_

= [2][C][r][T][ v]

_≤_ [2][C][r]

_γT −1_ _Cγ_


�


�
_E[ρ(θt)] +_ _[E][[][ρ][(][θ][0][)]]_

_γ0_


�
+ γ0


�
_Cr_


_T −1_
�

3Cπ[4][(][E][||][w][t] _[−]_ _[w]t[∗][||][2][) =]_
_t=0_


_T −1_
�

3Cπ[4][(][E][||][∆][w][t][||][2][)]
_t=0_


_T� −1_ _T_ [1][−][v]

_LJ_ _γtG[2]θ_ _[≤]_ _[L][J]_ _[G]θ[2][C][γ]_ 1 _v_
_t=0_ _−_

Using 1⃝- 4⃝ and dividing equation A.9 by T:


1

_t[v][ dt][ =][ T]1_ [ 1][−]v[v]

_−_


�
∵


_T −1_
�

_t=0_


1 � _T_

1 + t[v][ ≤] 0


�


_T −1_
�

_E||∆wt||[2]) + 3Cπ[4][(][τ][ 2][ + 4]_ _wϵ_ [)]

_M [C]_ [2]

_t=0_


1

_T_


_T −1_
� _E||∇θρ(θt)||[2]_ _≤_ 2 _[C][r]_ _T_ _[v][−][1]_ + 3Cπ[4][( 1]

_Cγ_ _T_

_t=0_


+ _[C][γ][L][J]_ _[G]θ[2]_ _T_ _[−][v]_

1 _v_
_−_

**Lemma 4. Let the cumulative error of linear critic be** [�][T]t=0[ −][1] [E][||][∆][w][t][||][2][ and cumulative error of]
_average reward estimator be_ [�]t[T]=0[ −][1] [E][||][∆][ρ][t][||][2][.][ w][t][ and][ ρ][t][ are linear critic parameter and average]
_reward estimator at time t respectively. Bound on the cumulative error of critic is proven using_
_cumulative error of average reward estimator as follows:_


2Cw[2] _T_ _[σ][−][1]_ + _[C][g][C][α]_
_λCα_ 1 − _σ [T][ −][σ][+]_


�[�]


1

_T_


_T −1_
�

E||∆wt||[2] _≤_ 2
_t=0_


+ [2(][C][r][ + 3][C][w][)]

_λ_


1

+ [2]

_λ[2]_ _T_


� [�] _γt_

_αt_


_LwGθ_

_λ_


�
1

_T_


�2�1/2


�2


_T −1_
�

E||∆ρt||[2]
_t=0_


_Here, ∆wt = wt −_ _wt[∗][,][ ∆][ρ][t]_ [=][ ρ][t] _[−]_ _[ρ]t[∗][.][ w]t[∗]_ _[and][ ρ]t[∗]_ _[are the optimal parameters given by TD(0)]_
_algorithm corresponding to policy parameter θt. Cα, σ are constants and γt, αt are step-sizes defined_
_in Assumption 3, ∥wt∥≤_ _Cw (Algorithm 2, step 8), Cr is the upper bound on rewards (Assumption_
_w_ _γt[2]_ _δ_
_5), Constant Gθ is defined in Lemma 7, Cg =_ _[L]λ[2]_ [max][t] _α[2]t_ _[G]θ[2]_ [+][ C]λ[2] _[,][ C][δ][ = 2][C][r][ + (4 +][ η][)][C][w][.][ η][ is]_

_the l2-regularisation coefficient from Algorithm 2 and η > λ[all]max[, where][ λ][all]max_ _[is defined in Lemma]_
_11. λ is defined in Lemma 11. Lw is defined in Lemma 6._


-----

_Proof._


1
_wt+1 = wt + αt_

_M_


_M_ _−1_
�

_i=0_


� �
_R[π](st,i) −_ _ρ¯t + ϕ[π](s[′]t,i[)][⊺][w][¯][t]_ _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][t]_ _ϕ[π](st,i) −_ _αtηwt_


=⇒ _wt+1 −_ _wt[∗]+1_ [=][ w][t] _[−]_ _[w]t[∗]_ [+][ w]t[∗] _[−]_ _[w]t[∗]+1_ _⃝1_

1 _M�−1�_ �
+ αt _M_ _R[π](st,i) −_ _ρ[∗]t_ [+][ ϕ][π][(][s]t,i[′] [)][⊺][w][¯][t] _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][t]_ _ϕ[π](st,i) −_ _αtηwt_ _⃝2_

_i=0_

1 _M�−1�_ �
+ αt _M_ _ρ[∗]t_ _[−]_ _[ρ][t]_ _ϕ[π](st,i)_ _⃝3_

_i=0_

1 _M�−1�_ �
+ αt _M_ _ρt −_ _ρ¯t_ _ϕ[π](st,i)_ _⃝4_

_i=0_

(A.10)

From equation A.10:
### ⃝2 :


1

_M_


_M_ _−1_
�

_i=0_


� �
_R[π](st,i) −_ _ρ[∗]t_ [+][ ϕ][π][(][s]t,i[′] [)][⊺][w][¯][t] _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][t]_ _ϕ[π](st,i) −_ _ηwt_


= [1]

_M_


_M�−1�_ �

_R[π](st,i) −_ _ρ[∗]t_ [+][ ϕ][π][(][s]t,i[′] [)][⊺][w][t] _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][t]_ _ϕ[π](st,i) −_ _ηwt_

_i=0_


+ [1]

_M_


_M_ _−1_
�

_ϕ[π](st,i)ϕ[π](s[′]t,i[)][⊺][( ¯][w][t]_ _[−]_ _[w][t][)]_
_i=0_


= [1]

_M_


_M_ _−1_
�

_ϕ[π](st,i)ϕ[π](s[′]t,i[)][⊺][( ¯][w][t]_ _[−]_ _[w][t][) +][ g][(][B][t][, w][t][, θ][t][)][ −]_ _[g][¯][(][w][t][, θ][t][)]_
_i=0_


+ ¯g(wt, θt) − _g¯(wt[∗][, θ][t][)]_
(A.11)

Let g(Bt, wt, θt) := 1 �Mi=0−1�R[π](st,i) − _ρ[∗]t_ �ϕ[π](st,i) + 1 �Mi=0−1�ϕ[π](st,i)(ϕ[π](s[′]t,i[)][ −]

_M_ _M_

�
_ϕ[π](st,i))[⊺]_ _−_ _ηI_ _wt_


� �
Let ¯g(wt, θt) := � _d(s, π(θt))ϕ[π](s)_ _r[π](s) −_ _ρ[∗]t_ [+] � _ρ[π](s[′]|s)ϕ[π](s[′])[⊺]wt ds[′]_ _−_ _ϕ[π](s)[⊺]wt_ _ds_

Using equation A.11 in equation A.10:

_wt+1 −_ _wt[∗]+1_ [=][w][t] _[−]_ _[w]t[∗]_ [+][ w]t[∗] _[−]_ _[w]t[∗]+1[+]_

1 _M�−1_
+ αt _M_ (ρ[∗]t _[−]_ _[ρ][t][)][ϕ][π][(][s][t,i][)]_

_i=0_


1
+ αt

_M_

1
+ αt

_M_


_M_ _−1_
�

(ρt − _ρ¯t)ϕ[π](st,i)_
_i=0_

_M_ _−1_
�

_ϕ[π](st,i)ϕ[π](s[′]t,i[)][⊺][( ¯][w][t]_ _[−]_ _[w][t][)]_
_i=0_


+ αt(g(Bt, wt, θt) − _g¯(wt, θt))_
+ αt(¯g(wt, θt) − _g¯(wt[∗][, θ][t][))]_


-----

Let, _f_ (Bt, wt, θt) := [1]

_M_


_M_ _−1_
�

(ρ[∗]t _[−]_ _[ρ][t][)][ϕ][π][(][s][t,i][)]_
_i=0_


+ [1]

_M_

+ [1]

_M_


_M_ _−1_
�

(ρt − _ρ¯t)ϕ[π](st,i)_
_i=0_

_M_ _−1_
�

_ϕ[π](st,i)ϕ[π](s[′]t,i[)][⊺][( ¯][w][t]_ _[−]_ _[w][t][)]_
_i=0_


+ g(Bt, wt, θt) − _g¯(wt, θt)_
+ ¯g(wt, θt) − _g¯(wt[∗][, θ][t][)]_


_||wt+1 −_ _wt[∗]+1[||][2][ =][ ||][(][w][t]_ _[−]_ _[w]t[∗][) + (][w]t[∗]_ _[−]_ _[w]t[∗]+1[) +][ α][t][f]_ [(][B][t][, w][t][, θ][t][)][||][2]

= ||wt − _wt[∗][||][2][ +][ ||][w]t[∗]_ _[−]_ _[w]t[∗]+1[||][2]_

+ αt[2][||][f] [(][B][t][, w][t][, θ][t][)][||][2]

+ 2⟨∆wt, wt[∗] _[−]_ _[w]t[∗]+1[⟩]_ [+ 2][α][t][⟨][∆][w][t][, f] [(][B][t][, w][t][, θ][t][)][⟩]

+ 2αt⟨wt[∗] _[−]_ _[w]t[∗]+1[, f]_ [(][B][t][, w][t][, θ][t][)][⟩]

E||wt+1 − _wt[∗]+1[||][2][ ≤]_ [E][||][∆][w][t][||][2][ + 2][E][||][w]t[∗] _[−]_ _[w]t[∗]+1[||][2]_

+ 2αt[2][E][||][f] [(][B][t][, w][t][, θ][t][)][||][2]

+ 2E⟨∆wt, wt[∗] _[−]_ _[w]t[∗]+1[⟩]_

+ 2αtE⟨∆wt, f (Bt, wt, θt)⟩

= E||∆wt||[2] + 2E||wt[∗] _[−]_ _[w]t[∗]+1[||][2]_ _⃝1_

+ 2αt[2][E][||][f] [(][B][t][, w][t][, θ][t][)][||][2] _⃝2_
+ 2E⟨∆wt, wt[∗] _[−]_ _[w]t[∗]+1[⟩]_ _⃝3_

_M_ _−1_
�

+ 2αtE⟨∆wt, _M[1]_ (ρ[∗]t _[−]_ _[ρ][t][)][ϕ][π][(][s][t,i][)][⟩]_ _⃝4_

_i=0_


(A.12)


+ 2αtE⟨∆wt, [1]

_M_

+ 2αtE⟨∆wt, [1]

_M_


_M_ _−1_
�

(ρt − _ρ¯t)ϕ[π](st,i)⟩_ _⃝5_
_i=0_

_M_ _−1_
�

_ϕ[π](st,i)ϕ[π](s[′]t,i[)][⊺][( ¯][w][t]_ _[−]_ _[w][t][)][⟩]_ _⃝6_
_i=0_


+ 2αtE⟨∆wt, g(Bt, wt, θt) − _g¯(wt, θt)⟩_ _⃝7_
+ 2αtE⟨∆wt, ¯g(wt, θt) − _g¯(wt[∗][, θ][t][)][⟩]_ _⃝8_

From equation A.12:
### ⃝1 :

E||wt[∗] _[−]_ _[w]t[∗]+1[||][2][ ≤]_ _[L][2]w[E][||][θ][t][+1]_ _[−]_ _[θ][t][||][2]_ (using lemma 6)

_≤_ _L[2]w[γ]t[2][G]θ[2]_ (using lemma 7)

### ⃝2 :

E||f (Bt, wt, θt)||[2]


= E|| [1]

_M_


_M_ _−1_
�

(R[π](st,i) − _ρ¯t + ϕ[π](s[′]t,i[)][⊺][w][¯][t]_ _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][t][)][ϕ][π][(][s][t,i][)][ −]_ _[ηw][t][||][2]_
_i=0_


_≤_ E�|| [1]

_M_


_M_ _−1_
� (R[π](st,i) − _ρ¯t + ϕ[π](s[′]t,i[)][⊺][w][¯][t]_ _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][t][)][ϕ][π][(][s][t,i][)][||][ +][ η][||][w][t][||]�2_

_i=0_


-----

Here,
_ϕ[π](s)_ _< 1_ (Assumption 4)
_||_ _||_
_|R[π](s)| ≤_ _Cr_ (Assumption 5)
_||wt|| ≤_ _Cw_ (Algorithm 2, step 8)
_|ρt| ≤_ _Cr + 2Cw_ (lemma 8)
_|| ¯wt|| ≤_ _Cw_ (lemma 9)
_|| ¯ρt|| ≤_ _Cr + 2Cw_ (lemma 10)


� 1
_≤_ E

_M_


_M�−1_ �2

_||(R[π](st,i) −_ _ρ¯t + ϕ[π](s[′]t,i[)][⊺][w][¯][t]_ _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][t][)][ϕ][π][(][s][t,i][)][||][ +][ η][||][w][t][||]_
_i=0_


### ⃝3 :

 ⃝4 :

 ⃝5 :

 ⃝6 :


_≤_ E(Cr + Cr + 2Cw + 2Cw + ηCw)[2]

_≤_ E(Cδ)[2] (Cδ = 2Cr + (4 + η)Cw)

_≤_ _Cδ[2]_

E⟨∆wt, wt[∗] _[−]_ _[w]t[∗]+1[⟩≤]_ [E][||][∆][w][t][|| ||][w]t[∗] _[−]_ _[w]t[∗]+1[||]_

_≤_ _LwE||∆wt|| ||θt+1 −_ _θt||_ (using Lemma 6)


_M_ _−1_
�

_⟨∆wt, ϕ[π](st,i)⟩(ρ[∗]t_ _[−]_ _[ρ][t][)]]_
_i=0_

_M_ _−1_
�

_||∆wt||||ϕ[π](st,i)|||(ρ[∗]t_ _[−]_ _[ρ][t][)][|][]]_
_i=0_


E[⟨∆wt, [1]

_M_


_M_ _−1_
�

(ρ[∗]t _[−]_ _[ρ][t][)][ϕ][π][(][s][t,i][)][⟩][] =][ E][[ 1]M_
_i=0_

_≤_ E[ [1]

_M_


_≤_ E||∆wt|||ρ[∗]t _[−]_ _[ρ][t][|]_
= E||∆wt|||∆ρt|


_M_ _−1_
�

_⟨∆wt, ϕ[π](st,i)⟩(ρt −_ _ρ¯t)]_
_i=0_

_M_ _−1_
�

_||∆wt|| ||ϕ[π](st,i)|||ρt −_ _ρ¯t|]_
_i=0_


E[⟨∆wt, [1]

_M_


_M_ _−1_
�

(ρt − _ρ¯t)ϕ[π](st,i)⟩] = E[ M[1]_
_i=0_

_≤_ E[ [1]

_M_


_≤_ E[||∆wt|||ρt − _ρ¯t|]_
_≤_ E[||∆wt||(|ρt| + |ρ¯t|)]
_≤_ E[||∆wt||]2(Cr + 2Cw) (using Lemma 8, 10)


_M_ _−1_
�

_ϕ[π](st,i)ϕ[π](s[′]t,i[)][⊺][( ¯][w][t]_ _[−]_ _[w][t][)][||]_
_i=0_


E⟨∆wt, [1]

_M_


_M_ _−1_
�

_ϕ[π](st,i)ϕ[π](s[′]t,i[)][⊺][( ¯][w][t]_ _[−]_ _[w][t][)][⟩≤]_ [E][||][∆][w][t][|| ||][ 1]M
_i=0_


### ⃝7 :


_≤_ E||∆wt|| ||w ¯t − _wt||_
_≤_ 2CwE||∆wt|| (using algorithm 2)

E[⟨∆wt, g(Bt, wt, θt) − _g¯(wt, θt)⟩] = E[⟨∆wt, E[g(Bt, wt, θt) −_ _g¯(wt, θt)|∆wt]⟩]_
Note: E[g(Bt, wt, θt) − _g¯(wt, θt)] = 0_
Hence, E[⟨∆wt, g(Bt, wt, θt) − _g¯(wt, θt)⟩] = 0_


-----

### ⃝8 :


E[⟨∆wt, ¯g(wt, θt) − _g¯(wt[∗][, θ][t][)][⟩][]]_

�
_A(θt) =_

�
_b(θt) =_


_d[π](s, θt)(ϕ[π](s)(E[ϕ[π](s[′])] −_ _ϕ[π](s)[⊺]_ _−_ _ηI)ds_
_S_

_d[π](s, θt)r[π](s)ϕ[π](s)ds_
_S_


_g¯(wt, θt) −_ _g¯(wt[∗][, θ][t][) =][ b][(][θ][t][) +][ A][(][θ][t][)][w][t]_ _[−]_ _[b][(][θ][t][)][ −]_ _[A][(][θ][t][)][w]t[∗]_
= A(θt)(wt − _wt[∗][)]_
Now, E[⟨∆wt, ¯g(wt, θt) − _g¯(wt[∗][, θ][t][)][⟩][] =][ E][[][⟨][∆][w][t][, A][(][θ][t][)∆][w][t][⟩][]]_

= E[∆wt[⊺][A][(][θ][t][)∆][w][t][]]

_≤−λE||∆wt||[2]_ (Lemma 11)

Combining 1⃝ - 8⃝ into equation A.12:

E||wt+1 − _wt[∗]+1[||][2][ ≤]_ [(1][ −] [2][λα][t][)][E][||][∆][w][t][||][2][ + 2][L]w[2] _[γ]t[2][G]θ[2]_ [+ 2][α]t[2][C]δ[2]
+ 2LwE||∆wt||||θt+1 − _θt|| + 2αtE||∆wt|||∆ρt|_
+ 4αtE||∆wt||(2Cw + Cr) + 4αtCwE||∆wt||

=⇒ 2λαtE||∆wt||[2] _≤_ E[||∆wt||[2]] − E||∆wt+1||[2] + 2L[2]w[γ]t[2][G]θ[2] [+ 2][α]t[2][C]δ[2]
+ 2LwγtGθE||∆wt|| + 2αtE||∆wt|||∆ρt|
+ 4αt(Cr + 3Cw)E||∆wt||

1
=⇒ E||∆wt||[2] _≤_ (E||∆wt||[2] _−_ E||wt+1||[2])

2λαt

+ � _L2w[γ]t[2]_ _G[2]θ_ [+][ α][t] _δ_ �

_λαt_ _λ [C]_ [2]

_γt_

+ _[L][w]_ _GθE||∆wt||_

_λ_ _αt_


+ [E][||][∆][w][t][|||][∆][ρ][t][|]

_λ_

+ [2]

_λ_ [(][C][r][ + 3][C][w][)][E][||][∆][w][t][||]


_T −1_
�

_t=0_


=
_⇒_


_T −1_
�

E||∆wt||[2] _≤_
_t=0_


1

2λαt (E||∆wt||[2] _−_ E||∆wt+1||[2]) _⃝1_


+

+

+

+


_T −1_
�

_t=0_

_T −1_
�

_t=0_

_T −1_
�

_t=0_

_T −1_
�

_t=0_


� _Lλw_ _γαt[2]t_ _G[2]θ_ [+][ α]λ [t] _[C]δ[2]�_ _⃝2_

_Lw_ _γt_

_λ_ _αt_ _GθE||∆wt||_ _⃝3_


E||∆wλt|||∆ρt| _⃝4_

2
_λ_ [(][C][r][ + 3][C][w][)][E][||][∆][w][t][||] _⃝5_


(A.13)


-----

From equation A.13:
### ⃝1 :


� _[T]�[ −][1]_

_t=1_

� _T −1_
�

_t=1_


� 1 1

_−_
_αt_ _αt−1_

� 1 1

_−_
_αt_ _αt−1_


�
+ [1]

_α0_


1

2λ

### ⃝2 :

 ⃝3 :

 ⃝4 :

 ⃝5 :


_T −1_
�

(E||∆wt||[2] _−_ E||∆wt+1||[2]) [1] = [1]

_αt_ 2λ

_t=0_

_≤_ [1]

2λ


� 1
E||∆wt||[2] + α[1]0 E||∆w0||[2] _−_ _αT −1_ E||∆wT ||[2][�]


�


4Cw[2]


_≤_ 4Cw[2] = _[C]w[2]_ _T_ _[σ]_ (∵ _αt =_ _Cα_

2λαT −1 _λCα_ (1 + t)[α][ )]


_T −1_
�

_t=0_


� _L2w_ _γt[2]_ _G[2]θ_ [+][ α][t] _δ_ � =

_λ_ _αt_ _λ [C]_ [2]

_≤_

=


_T −1_
�

_t=0_

_T −1_
�

_t=0_


_T� −1_ � _w_ _γt[2]_ _δ_

_t=0_ _Cgαt =_ _[C]1 −[g][C]σ [α]_ _[T][ 1][−][σ]_ _Cg =_ _[L]λ[2]_ [max]t _αt[2]_ _G[2]θ_ [+][ C]λ[2]


� _L2w_ _γt[2]_ _G[2]θ_ [+][ C]δ[2]

_λ_ _αt[2]_ _λ_


�
_αt_


� _Lλ2w_ [max]t _αγt[2]t[2]_ _G[2]θ_ [+][ C]λδ[2]


�
_αt_


�


_T −1_
�

_t=0_

1
_λ_


_T −1_
�

E||∆wt|||∆ρt| ≤ [1]

_λ_

_t=0_

_≤_ [1]

_λ_


_Lw_ _γt_

_GθE||∆wt|| =_ _[L][w]_

_λ_ _αt_ _λ [G][θ]_

_≤_ _[L][w]_

_λ [G][θ]_


_T� −1_ _γt_

E||∆wt||
_αt_

_t=0_


� _T −1_
�

_t=0_


� _γt_

_αt_


�2� 2[1] � _T� −1_ 2

(E||∆wt||)[2][�] [1]
_t=0_


(Using Cauchy Schwartz inequality)


2� [1]2 � _T� −1_ 2

E||∆wt||[2][�] [1]
_t=0_


_≤_ _[L][w]_

_λ [G][θ]_


� _T −1_
�

_t=0_


� _γt_

_αt_


(Using Jensen’s inequality)


� _[T]�[ −][1](E||∆wt||)[2][�]_ [1]2 [�] _[T]�[ −][1](E|∆ρt|)[2][�]_ 2[1]

_t=0_ _t=0_

� _[T]�[ −][1]_ E||∆wt||[2][�] [1]2 [�] _[T]�[ −][1]_ E|∆ρt|[2][�] 2[1]

_t=0_ _t=0_


_T −1_
�

_t=0_


2(Cr + 3Cw)

E||∆wt|| ≤ [2(][C][r][ + 3][C][w][)]
_λ_ _λ_


� _[T]�[ −][1]_ E||∆wt||[2][�] 2[1] [�] _[T]�[ −][1]_ 1� 2[1]

_t=0_ _t=0_


_≤_ [2(][C][r][ + 3][C][w][)] _T_ 21 � _[T]�[ −][1]_ E||∆wt||[2][�] 2[1]

_λ_

_t=0_


-----

Combining 1⃝ - 5⃝ into equation A.13:


1

_T_


_T −1_
� E||∆wt||[2] _≤_ [2][C]w[2] _T_ _[σ][−][1]_ + _[C][g][C]_ _[α]_

_t=0_ _λCα_ 1 − _σ [T][ −][σ]_


_T −1_
� 2

E||∆wt||[2][�] [1]
_t=0_


�
1

+ _[L][w][G][θ]_

_λ_ _T_


_T −1_
�

_t=0_


� _γt_

_αt_


�2� 2[1] � 1

_T_


_T −1_
� 2

E||∆ρt||[2][�] [1]
_t=0_


_T −1_
� E||∆wt||[2][�] 2[1] [�] 1

_T_

_t=0_


+ [1]

_λ_


� 1

_T_


_T −1_
� 2

E||∆wt||[2][�] [1]
_t=0_


+ [2(][C][r][ + 3][C][w][)]

_λ_


� 1

_T_


Let,


_M_ (T ) = [1]

_T_

_N_ (T ) = [1]

_T_


_T −1_
�

E||∆wt||[2]
_t=0_

_T −1_
�

E|∆ρt|[2]
_t=0_


� � �
_M_ (T ) ≤ _K1 + K2_ _M_ (T ) + K3 _M_ (T ) _N_ (T )

_K1 := [2][C]w[2]_ _T_ _[σ][−][1]_ + _[C][g][C][α]_

_λCα_ 1 − _σ [T][ −][σ]_

� 1 _T� −1_ � _γt_ �2� 2[1]

_K2 :=_ _[L][w][G][θ]_ + [2(][C][r][ + 3][C][w][)]

_λ_ _T_ _αt_ _λ_

_t=0_

_K3 := [1]_

_λ_


� � � _K3_ �

_M_ (T ) 2 _[K][2]_ _M_ (T ) 2 _[K][3]_ _M_ (T ) _N_ (T ) + 2 _[K][2]_ _N_ (T )
_−_ _−_

2 2 2 2

� _K2_ �2 � _K3_ � �2 � _K2_ �2 � _K3_ � �2 _K3_ �
+ + _N_ (T ) _≤_ _K1 +_ + _N_ (T ) + 2 _[K][2]_

2 2 2 2 2 2


_N_ (T )


�� � �2 � _K2_ � �2
=⇒ _M_ (T ) − _[K][2]_ _N_ (T ) _≤_ _K1 +_ _N_ (T )

2 2 2 [+][ K]2[3]

_[−]_ _[K][3]_

� � � �
=⇒ _M_ (T ) − _[K][2]_ _N_ (T ) ≤ _K1 +_ _[K][2]_ _N_ (T )

2 2 2 [+][ K]2[3]

_[−]_ _[K][3]_

� � �
=⇒ _M_ (T ) ≤ _K1 + K2 + K3_ _N_ (T )

�
=⇒ _M_ (T ) ≤ 2( _K1 + K2)[2]_ + 2K3[2][N] [(][T] [)]


_T −1_
�

_t=0_


� _γt_

_αt_


�
1

_T_


�2� 2[1]

+ [2(][C][r][ + 3][C][w][)]

_λ_


�2


1

_T_


_T −1_ �[�]
�

E||∆wt||[2] _≤_ 2
_t=0_


2Cw[2] _T_ _[σ][−][1]_ + _[C][g][C][α]_
_λCα_ 1 − _σ [T][ −][σ][ +][ L][w]λ[G][θ]_


1

+ [2]

_λ[2]_ _T_


_T −1_
�

E||∆ρt||[2]
_t=0_


-----

**Lemma 5. Let the cumulative error of linear critic be** [�][T]t=0[ −][1] [E][||][∆][w][t][||][2][ and cumulative error of]
_average reward estimator be_ [�]t[T]=0[ −][1] [E][||][∆][ρ][t][||][2][.][ w][t][ and][ ρ][t][ are linear critic parameter and average]
_reward estimator at time t respectively. Bound on the cumulative error of average reward estimator is_
_proven using cumulative error of critic as follows:_


1

_T_


_T −1_ �[�]
�

E|∆ρt|[2] _≤_ 2
_t=0_


2(Cr + 2Cw)[2]T _[σ][−][1]_

+ _[C][s][C][α][T][ −][σ]_
_Cα_ 1 − _σ_


�2


� 1
+ LpGθ

_T_


_T −1_
�

_t=0_


� _γt_

_αt_


�2� 2[1] + 4Cw


+ [8]

_T_


_T −1_
�

E||∆wt||[2]
_t=0_


_Here, ∆ρt = ρt −_ _ρ[∗]t_ _[,][ ∆][w][t]_ [=][ w][t] _[−]_ _[w]t[∗][.][ w]t[∗]_ _[and][ ρ]t[∗]_ _[are the optimal parameters given by TD(0)]_
_algorithm corresponding to policy parameter θt. Cα, σ are constants and γt, αt are step-sizes defined_
_in Assumption 3, ∥wt∥≤_ _Cw (Algorithm 2, step 8), Cr is the upper bound on rewards (Assumption 5),_
_Constant Gθ is defined in Lemma 7. Cs = L[2]p[G][2]θ_ [max][t] _αγt[2][2]t_ [+ 4(][C][r][ + 2][C][w][)][2][.][ L][p][ is Lipchitz constant]

_defined in Lemma 14._

_Proof._


1
_ρt+1 = ρt + αt_

_M_


_M�−1_ � �

_R[π](st,i) −_ _ρt + ϕ[π](s[′]t,i[)][⊺][w][¯][t]_ _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][¯][t]_

_i=0_


_ρt+1 −_ _ρ[∗]t+1_ [=][ ρ][t] _[−]_ _[ρ]t[∗]_ [+][ ρ]t[∗] _[−]_ _[ρ]t[∗]+1_

1 _M�−1_ � �
+ αt _M_ _R[π](st,i) −_ _ρt + ϕ[π](s[′]t,i[)][⊺][w][¯][t]_ _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][¯][t]_

_i=0_

= ρt − _ρ[∗]t_ [+][ ρ]t[∗] _[−]_ _[ρ]t[∗]+1_

1 _M�−1_ � �
+ αt _M_ _R[π](st,i) −_ _ρ[∗]t_ [+][ ϕ][π][(][s]t,i[′] [)][⊺][w][¯][t] _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][¯][t]_

_i=0_


+ αt(ρ[∗]t _[−]_ _[ρ][t][)]_
_ρt+1 −_ _ρ[∗]t+1_ [=][ ρ][t] _[−]_ _[ρ]t[∗]_ [+][ ρ]t[∗] _[−]_ _[ρ]t[∗]+1_
+ αt(ρ[∗]t _[−]_ _[ρ][t][)]_

� 1 _M�−1_ �
+ αt _M_ (ϕ[π](s[′]t,i[)][ −] _[ϕ][π][(][s][t,i][))][⊺][( ¯][w][t]_ _[−]_ _[w][t][)]_

_i=0_


� 1
+ αt

_M_


_M�−1_ �

(R[π](st,i) − _ρ[∗]t_ [+][ ϕ][π][(][s]t,i[′] [)][⊺][w][t] _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][t][)]_
_i=0_


= ρt − _ρ[∗]t_ [+][ ρ]t[∗] _[−]_ _[ρ]t[∗]+1_
+ αt(ρ[∗]t _[−]_ _[ρ][t][)]_

� 1 _M�−1_ �
+ αt _M_ (ϕ[π](s[′]t,i[)][ −] _[ϕ][π][(][s][t,i][))][⊺][( ¯][w][t]_ _[−]_ _[w][t][)]_

_i=0_

+ αt�l(Bt, wt, θt) − [¯]l(wt, θt)�

+ αt�¯l(wt, θt) − ¯l(wt∗[, θ][t][)]�


-----

Here,


� 1
_l(Bt, wt, θt) :=_

_M_


_M�−1_ �

(R[π](st,i) − _ρ[∗]t_ [+][ ϕ][π][(][s]t,i[′] [)][⊺][w][t] _[−]_ _[ϕ][π][(][s][t,i][)][⊺][w][t][)]_
_i=0_


�
¯l(wt, θt) := _d[π][�]s, π(θt)��R[π](s) −_ _ρ(π(θt)) + ϕ[π](s[′])[⊺]wt −_ _ϕ[π](s)[⊺]wt�_ _ds_

_S_

¯l(Bt, ρt, wt, θt) := (ρ[∗]t _[−]_ _[ρ][t][)]_

� 1 _M�−1_ �
+ _M_ (ϕ[π](s[′]t,i[)][ −] _[ϕ][π][(][s][t,i][))][⊺][( ¯][w][t]_ _[−]_ _[w][t][)]_

_i=0_

+ �l(Bt, wt, θt) − [¯]l(wt, θt)�

+ �¯l(wt, θt) − ¯l(wt∗[, θ][t][)]�


_||∆ρt+1||[2]_ = ||∆ρt + ρ[∗]t [+][ α][t][l][(][B][t][, w][t][, ρ][t][, θ][t][)][||][2]

= ||∆ρt||[2] + ||ρ[∗]t _[−]_ _[ρ]t[∗]+1[||][2][ +][ α]t[2][||][l][(][B][t][,][ ¯][w][t][, ρ][t][, θ][t][)][||][2]_

+ 2⟨∆ρt, ρ[∗]t _[−]_ _[ρ]t[∗]+1[⟩]_

+ 2αt⟨∆ρt, l(Bt, ¯wt, ρt, θt)⟩
+ 2αt⟨ρ[∗]t _[−]_ _[ρ]t[∗]+1[, l][(][B][t][, ρ][t][,][ ¯][w][t][, θ][t][)][⟩]_

_≤||∆ρt||[2]_ + 2||ρ[∗]t _[−]_ _[ρ]t[∗]+1[||][2][ + 2][α]t[2][||][l][(][B][t][,][ ¯][w][t][, ρ][t][, θ][t][)][||][2]_

+ 2⟨∆ρt, ρ[∗]t _[−]_ _[ρ]t[∗]+1[⟩]_

+ 2αt⟨∆ρt, l(Bt, ¯wt, ρt, θt)⟩

E||∆ρt+1||[2] _≤_ E||∆ρt||[2] + 2E||ρ[∗]t _[−]_ _[ρ]t[∗]+1[||][2]_ _⃝1_

+ 2αt[2][E][||][l][(][B][t][,][ ¯][w][t][, ρ][t][, θ][t][)][||][2] _⃝2_
+ 2E⟨∆ρt, ρ[∗]t _[−]_ _[ρ]t[∗]+1[⟩]_ _⃝3_

+ 2αtE⟨∆ρt1 − ∆ρt⟩ _⃝4_

_M_ _−1_
�

+ 2αtE⟨∆ρt, _M[1]_ (ϕ[π](s[′]t,i[)][ −] _[ϕ][π][(][s][t,i][))][⊺][( ¯][w][t]_ _[−]_ _[w][t][)][⟩]_ _⃝5_

_i=0_

+ 2αtE⟨∆ρt, l(Bt, wt, θt) − [¯]l(wt, θt)⟩ _⃝6_

+ 2αtE⟨∆ρt, [¯]l(wt, θt) − [¯]l(wt[∗][, θ][t][)][⟩] _⃝7_

From equation A.14:
### ⃝1 :

E||ρ[∗]t _[−]_ _[ρ]t[∗]+1[||][2][ ≤]_ _[L][2]p[E][||][θ][t][+1]_ _[−]_ _[θ][t][||][2][(Lemma 14)]_

### ⃝2 :


(A.14)


E||l(Bt, ρt, ¯wt, θt)||[2] = E|| [1]

_M_

� 1
_≤_ E

_M_


_M_ _−1_
�

_i=0_


_M�−1_ �2

(Cr + Cr + 2Cw + 2Cw)
_i=0_


�R[π](st,i) − _ρt +_ �ϕ[π](s[′]t,i[)][ −] _[ϕ][π][(][s][t,i][)]�⊺w¯t�||[2]_


### ⃝3 :

 ⃝4 :


= 4(Cr + 2C2)[2]

E⟨∆ρt, ρ[∗]t _[−]_ _[ρ]t[∗]+1[⟩≤]_ [E][||][∆][ρ][t][|| |][ρ][∗]t _[−]_ _[ρ]t[∗]+1[|]_

_≤_ _LpE|∆ρt| ||θt+1 −_ _θt||_

E⟨∆ρt, −∆ρt⟩ = −E|∆ρt|[2]


-----

### ⃝5 :

 ⃝6 :

 ⃝7 :


E⟨∆ρt, [1]

_M_

� 1
_≤_ E

_M_


_M_ _−1_
�

_i=0_


_M�−1_ �

_||ϕ[π](s[′]t,i[)][ −]_ _[ϕ][π][(][s][t,i][)][|| ||][w][ ¯][t]_ _[−]_ _[w][t][|| |][∆][ρ][t][|]_
_i=0_


�ϕ[π](s[′]t,i[)][⊺] _[−]_ _[ϕ][π][(][s][t,i][)][⊺][�]( ¯wt −_ _wt)⟩_


_≤_ 4CwE|∆ρt|

E⟨∆ρt, l(Bt, wt, θt) − [¯]l(wt, θt)⟩ = E⟨∆ρt, E[l(Bt, wt, θt) − [¯]l(wt, θt)|∆ρt]⟩

Note: E[l(Bt, wt, θt) − [¯]l(wt, θt)|∆ρt]

E⟨∆ρt, l(Bt, wt, θt) − [¯]l(wt, θt)⟩ = 0


⊺
E[⟨∆ρt, [¯]l(wt, θt) − [¯]l(wt[∗][, θ][t][)][⟩][] =][ E][[][⟨][∆][ρ][t][,] �E[ϕ[π](s[′])] − _ϕ[π](s)�_ (wt − _wt∗[)][⟩][]]_

_≤_ E[⟨ϕ[π](s[′]) − _ϕ[π](s), ∆wt⟩|∆ρt|]_

_≤_ E[||ϕ[π](s[′]) − _ϕ[π](s)|| ||∆wt|| |∆ρt|]_
_≤_ 2E(||∆wt|| |∆ρt|)


Combining 1⃝- 7⃝ into equation A.14:

E||∆ρt+1||[2] _≤_ (1 − 2αt)E||∆ρt||[2] + 2L[2]p[E][||][θ][t][+1] _[−]_ _[θ][t][||][2]_

+ 8αt[2][(][C][r] [+ 2][C][w][)][2][ + 2][L][p][E][|][∆][ρ][t][| ||][θ][t][+1] _[−]_ _[θ][t][||]_
+ 8αtCwE|∆ρt| + 4αtE||∆wt|| |∆ρt|


_T −1_
�

_t=0_


=
_⇒_

From equation A.15:
### ⃝1 :


_T −1_
�

E||∆ρt||[2] _≤_
_t=0_


1 �

2αt E||∆ρt||[2] _−_ E||∆ρt+1||[2][�] _⃝1_


+

+

+


_T −1_
�

_t=0_

_T −1_
�

_t=0_


_T −1_
�

2E||∆wt|| |∆ρt| _⃝4_
_t=0_


� _L2p[γ]t[2]_

_αt_ _G[2]θ_ [+ 4][α][t][(][C][r] [+ 2][C][w][)][2][�] _⃝2_

� _γt_ �
_Lp_ _αt_ _Gθ + 4Cw_ E|∆ρt| _⃝3_


(A.15)


�

� 1
E|∆ρt|[2] + [1] E|∆ρ0|[2] _−_ E|∆ρt|[2]

_α0_ _αT −1_


1

(E||∆ρt||[2] _−_ E||∆ρt+1||[2]) = [1]
_αt_ 2

_≤_ [1]

2


1
2


_T −1_
�

_t=0_


� _T� −1_ � 1 1

_−_

_t=0_ _αt_ _αt−1_

� _T� −1_ � 1 1

_−_

_t=0_ _αt_ _αt−1_


�
+ [1]

_α0_


�

4(Cr + 2Cw)[2]


_T_ _[σ]_

_≤_ [2(][C][r][ + 2][C][2][)][2]

_Cα_


-----

### ⃝2 :

_T −1_
�

_t=0_

### ⃝3 :


�L[2]p[G][2]θ _γt[2]_ + 4αt(Cr + 2Cw)[2][�] _≤_

_αt_

_≤_


_T −1_
�

_t=0_


_T� −1_ _γt[2]_

_t=0_ _Csαt_ (Cs = L[2]p[G][2]θ [max]t _αt[2]_ + 4(Cr + 2Cw)[2])


�L[2]p[G][2]θ [max]t _αγt[2]t[2]_ + 4(Cr + 2Cw)[2][�]αt


_T −1_
�

E||∆ρt||
_t=0_


_T −1_
�

_t=0_

### ⃝4 :


� _γt_ �
_LpGθ_ + 4Cw E||∆ρt|| =

_αt_


_≤_ _[C][s][C][α]_

1 _σ [T][ 1][−][σ]_
_−_

_T� −1_ _γt_

_LpGθ_ E||∆ρt|| + 4Cw

_αt_

_t=0_


� _T −1_
�

_≤_ _LpGθ_

_t=0_


� _γt_

_αt_


�2� 2[1] � _T� −1_ E|∆ρt|[2][�] 2[1] + 4Cw� _[T]�[ −][1]_ E|∆ρt|[2][�] 2[1] T 12

_t=0_ _t=0_


(using cauchy schwarz inequality)


_T −1_
� E||∆wt||[2]) 12 (

_t=0_


_T −1_
� E|∆ρt|[2]) 21

_t=0_


2


_T −1_
�

E||∆wt|| |∆ρt| ≤ 2(
_t−0_


(using cauchy schwarz inequality)


Combining 1⃝- 4⃝ into equation A.15


1

_T_


_T −1_
�

E||∆ρt||[2] _≤_ [2(][C][r][ + 2][C][w][)][2][T][ σ][−][1] + _[C][s][C][α][T][ −][σ]_
_t=0_ _Cα_ 1 − _σ_


_T −1_ � [1]2
�

E|∆ρt|[2]
_t=0_


_T −1_
�

_t=0_


� _γt_

_αt_


�2� 2[1] [�] 1

_T_


+ LpGθ


�
1

_T_


� 1
+ 4Cw

_T_


� 2

_T −_ 1E|∆ρt|[2][�] [1]
_t=0_


_T −1_ � 2[1]
�

E|∆ρt|[2]
_t=0_


_T −1_ � 2[1]
� [�] 1

E||∆wt||[2]

_T_

_t=0_


+ 2


�
1

_T_


_M_ (T ) = [1]

_T_


_T −1_
�

E||∆ρt||[2]
_t=0_


_T −1_
�

_N_ (T ) = [1] E||∆wt||[2]

_T_

_t=0_

� �
_M_ (T ) ≤ _K1 + K2_ _M_ (T ) + K3


�
_M_ (T ) _N_ (T )


Here,


_K1 = [2(][C][r][ + 2][C][2][)][2][T][ σ][−][1]_ + _[C][s][C][α][T][ −][σ]_

_Cα_ 1 − _σ_

� 1 _T� −1_ � _γt_ �2� 2[1]

_K2 = LpGθ_ + 4Cw

_T_ _αt_

_t=0_

_K3 = 2_


-----

From Lemma 4, we know that

�
_M_ (T ) ≤ 2( _K1 + K2)[2]_ + 2K3[2][N] [(][T] [)]

Hence,


1

_T_


_T� −1_ �[�]

E|∆ρt|[2] _≤_ 2
_t=0_


2(Cr + 2Cw)[2]

_T_ _[σ][−][1]_ + _[C][s][C][α]_
_Cα_ 1 − _σ [T][ −][σ]_


_T −1_
�

_t=0_


� _γt_

_αt_


�2� 2[1] �2

+ 4Cw


+ LpGθ


�
1

_T_


+ 8 [1]

_T_


_T −1_
�

E||∆wt||[2]
_t=0_


**Theorem 3. The on-policy average reward actor critic algorithm obtain ϵ-accurate optimal point**
_with sample complexity of Ω(ϵ[−][2][.][5])._


� 1 �
min + (1)
0≤t≤T −1 _[E][||∇][θ][ρ][(][θ][t][)][||][2][ =][ O]_ _T_ [0][.][4] _O_


min
0≤t≤T −1 _[E][||∇][θ][ρ][(][θ][t][)][||][2][ ≤]_ _[ϵ][ +][ O][(1)]_

_Proof. Using lemma 4 and lemma 5 we obtain,_


1

_T_


_T� −1_ �[�] 2Cw[2] � 1 _T� −1_ �2

_E||∆wt||[2]_ _≤_ 2 _T_ _[σ][−][1]_ + _[C][g][C][α]_ ( _[γ][t]_ )[2][�][1][/][2] + [2(][C][r][ + 3][C][w][)]
_t=0_ _λCα_ 1 − _σ [T][ −][σ][ +][ L][w]λ[G][θ]_ _T_ _t=0_ _αt_ _λ_

�[�] 2(Cr + 2Cw)[2] � 1 _T� −1_ �2

+ [4] _T_ _[σ][−][1]_ + _[C][s][C][α]_ ( _[γ][t]_ )[2][�][1][/][2] + 4Cw

_λ[2]_ _Cα_ 1 − _σ [T][ −][σ][ +][ L][p][G][θ]_ _T_ _t=0_ _αt_


+ [16]

_λ[2]T_


_T −1_
�

_E||∆wt||[2]_
_t=0_


2(Cr + 2Cw)[2]

_T_ _[σ][−][1]_ + _[C][s][C][α]_
_Cα_ 1 − _σ [T][ −][σ]_


2Cw[2] _T_ _[σ][−][1]_ + _[C][g][C][α]_
_λCα_ 1 − _σ [T][ −][σ]_

_T −1_
�

( _[γ][t]_ )[2][�][1][/][2] + [2(][C][r][ + 3][C][w][)]

_αt_ _λ_

_t=0_


(A.16)


=
_⇒_ [1]

_T_


_T� −1_ 2λ[2]

_E||∆wt||[2]_ _≤_

_λ[2]_ 16

_t=0_ _−_

+ _[L][w][G][θ]_

_λ_


�2
### ⃝1


4
+

_λ[2]_ 16
_−_


�[�]

� 1

_T_

�[�]


� 1
+ LpGθ

_T_


_T −1_
�

( _[γ][t]_ )[2][�][1][/][2] + 4Cw

_αt_

_t=0_


�2
### ⃝2


From equation A.16

### ⃝1 :

1

_T_


_T −1_
�

( _[γ][t]_ )[2]

_≤_ [1]
_αt_ _T_

_t=0_


_T� −1_ 1 _T_ _[−][2(][v][−][σ][)]_

( _[C][γ]_ )[2]
_t=0_ _Cα_ (1 + t)[2(][v][−][σ][)][ ≤] 1 − 2(v − _σ)_


� _T� −1_
∵

_t=0_


1 � _T_

1 + t[v][ ≤] 0


1

_t[v][ dt][ =][ T]1_ [ 1][−]v[v]

_−_


�


-----

�[�]


2Cw[2] _T_ _[σ][−][1]_ + _[C][g][C][α]_ � 1
_λCα_ 1 − _σ [T][ −][σ][ +][ L][w]λ[G][θ]_ _T_


_T −1_
�

( _[γ][t]_ )[2][�][1][/][2] + [2(][C][r][ + 3][C][w][)]

_αt_ _λ_

_t=0_


�2


�[�]
_≤_


� _T_ _[−][2(][v][−][σ][)]_

1 2(v _σ)_
_−_ _−_


�1/2
+ [2(][C][r][ + 3][C][w][)]

_λ_


2Cw[2] _T_ _[σ][−][1]_ + _[C][g][C][α]_
_λCα_ 1 − _σ [T][ −][σ][ +][ L][w]λ[G][θ]_


�2


3� 2Cw2 _T_ _[σ][−][1]_ + _[C][g][C][α]_ � _LwGθ_ �2� _T_ _[−][2(][v][−][σ][)]_
_≤_

_λCα_ 1 − _σ [T][ −][σ][ +]_ _λ_ 1 − 2(v − _σ)_

(∵ (a + b + c)[2] _≤_ 3(a[2] + b[2] + c[2]))

� 1 � � 1 � � 1 �
= + + + (1)
_O_ _O_ _O_ _O_

_T_ [1][−][σ] _T_ _[σ]_ _T_ [2(][v][−][σ][)]


� � 2(Cr + 3Cw)
+

_λ_


�2[�]


### ⃝2 (similar to 1⃝):


�[�]


�[�] 2(Cr + 2Cw)[2] � 1 _T� −1_

_T_ _[σ][−][1]_ + _[C][s][C][α]_ ( _[γ][t]_ )[2][�][1][/][2] + 4Cw
_Cα_ 1 − _σ [T][ −][σ][ +][ L][p][G][θ]_ _T_ _t=0_ _αt_

� 1 � � 1 � � 1 �
= + + + (1)
_O_ _O_ _O_ _O_

_T_ [1][−][σ] _T_ _[σ]_ _T_ [2(][v][−][σ][)]


�2


Combining 1⃝ and 2⃝:


�
+ (1) (A.17)
_O_


1

_T_


_T� −1_ � 1 � � 1

_E||∆wt)||[2]_ = O + O

_T_ [1][−][σ] _T_ _[σ]_

_t=0_


� � 1
+
_O_

_T_ [2(][v][−][σ][)]


Using lemma 3 and equation A.17


1

_T_


_T� −1_ � 1 � � 1 � � 1

_E||∇θρ(θt)||[2]_ = O + O + O

_T_ [1][−][v] _T_ _[v]_ _T_ [1][−][σ]

_t=0_

� 1 �
+ + (1)
_O_ _O_

_T_ [2(][v][−][σ][)]


� � 1 �
+
_O_

_T_ _[σ]_


� � 1
+
_O_

_T_ _[v]_


� � 1
+
_O_

_T_ _[σ]_


�


� 1 �
= min
_⇒_ 0≤t≤T −1 _[E][||∇][θ][ρ][(][θ][t][)][||][2][ =][ O]_ _T_ [1][−][v]

� 1
+
_O_

_T_ [2(][v][−][σ][)]


� � 1
+
_O_

_T_ [1][−][σ]


�
+ (1)
_O_


�
∵ mint _E||∇θρ(θt)||[2]_ _≤_ _T[1]_

By setting v = 3/5 and σ = 2/5, we obtain:


_T −1_
�

_E||∇θρ(θt)||[2][�]_
_t=0_


� 1
min
0≤t≤T −1 _[E][||∇][θ][ρ][(][θ][t][)][||][2][ =][ O]_ _T_ [0][.][4]


�
+ (1)
_O_


� 1
_O_

_T_ [0][.][4]


�
_ϵ_
_≤_


Hence, the sample complexity of on-policy average reward actor-critic algorithm is Ω(ϵ[−][2][.][5]).


-----

**Lemma 6. The optimal critic parameter w(θt)[∗]** _as a function of actor parameter θt is Lipchitz_
_continuous with constant Lw. Note: wt[∗]_ [:=][ w][(][θ][t][)][∗][.]

_||wt[∗]_ _[−]_ _[w]t[∗]+1[|| ≤]_ _[L][w][||][θ][t][+1]_ _[−]_ _[θ][t][||]_

_Proof. η is the l2-regularisation coefficient from Algorithm 2 and η > λ[all]max[, where][ λ][all]max_ [is defined]
in Lemma 11. Because of carefully setting the value of η, A(θt) is negative definite. Thus, for
on-policy TD(0) with l2-regularization and target estimators, the following condition holds true for
optimal critic parameter wt[∗][:]

_E[(R[π](s) −_ _ρ[∗]t_ [)][ϕ][π][(][s][) + (][ϕ][π][(][s][)(][E][[][ϕ][π][(][s][′][)]][ −] _[ϕ][π][(][s][))][⊺]_ _[−]_ _[ηI][)][w]t[∗][] = 0]_

_b(θt) := E[(R[π](s) −_ _ρ[∗]t_ [)][ϕ][π][(][s][)]]

_A(θt) := E[(ϕ[π](s)(E[ϕ[π](s[′])] −_ _ϕ[π](s))[⊺]_ _−_ _ηI)]_

∴ _b(θt) + A(θt)wt[∗]_ [= 0 =][⇒] _[w]t[∗]_ [=][ −][A][(][θ][t][)][−][1][b][(][θ][t][)]

_||wt[∗]_ _[−]_ _[w]t[∗]+1[||][ =][ ||][A][(][θ][t][)][−][1][b][(][θ][t][)][ −]_ _[A][(][θ][t][+1][)][−][1][b][(][θ][t][+1][)][||]_

_≤||A(θt)[−][1]b(θt) −_ _A(θt+1)[−][1]b(θt) + A(θt+1)[−][1]b(θt) −_ _A(θt+1)[−][1]b(θt+1)||_

_≤||A(θt)[−][1]_ _−_ _A(θt+1)[−][1]|| ||b(θt)||_ _⃝1_

+ ||A(θt+1)[−][1]|| ||b(θt) − _b(θt+1)|| 2⃝_
(A.18)

From equation A.18:

### ⃝1 :

_||A(θt)[−][1]_ _−_ _A(θt+1)[−][1]|| = ||A(θt)[−][1]A(θt+1)A(θt+1)[−][1]_ _−_ _A(θt)[−][1]A(θt)[A](θt+1)[−][1]||_

(A.19)
_≤||A(θt)[−][1]|| ||A(θt) −_ _A(θt+1)|| ||A(θt+1)[−][1]||_

From equation A.19:

Here, π[′] and π represents the policy with parameter θt+1 and θt respectively.

� �
_||A(θt) −_ _A(θt+1)|| ≤||_ _d[π][′]_ (s)(ϕ[π][′] (s)( _P_ _[π][′]_ (s[′]|s)ϕ[π][′] (s[′]) ds[′] _−_ _ϕ[π][′]_ (s))[⊺] _−_ _ηI) ds_

� �
_d[π](s)(ϕ[π](s)(_ _P_ _[π](s[′]_ _s)ϕ[π](s[′]) ds[′]_ _ϕ[π](s))[⊺]_ _ηI) ds_
_−_ _|_ _−_ _−_ _||_

� �
_d[π][′]_ (s)(ϕ[π][′] (s)( _P_ _[π][′]_ (s[′] _s)ϕ[π][′]_ (s[′]) ds[′])[⊺]) ds
_≤||_ _|_

� �
_d[π](s)(ϕ[π](s)(_ _P_ _[π](s[′]_ _s)ϕ[π](s[′]) ds[′])[⊺]) ds_ 1
_−_ _|_ _||_ _⃝_

� �
_d[π](s)(ϕ[π](s)(ϕ[π](s))[⊺]) ds_ _d[π][′]_ (s)(ϕ[π][′] (s)(ϕ[π][′] (s))[⊺]) ds 2
_≤||_ _−_ _||_ _⃝_

(A.20)

From equation A.20:


-----

### ⃝1 :

� � � �
_d[π][′]_ (s)(ϕ[π][′] (s)( _P_ _[π][′]_ (s[′] _s)ϕ[π][′]_ (s[′]) ds[′])[⊺]) ds _d[π](s)(ϕ[π](s)(_ _P_ _[π](s[′]_ _s)ϕ[π](s[′]) ds[′])[⊺]) ds_
_||_ _|_ _−_ _|_ _||_

� �
(d[π][′] (s) _d[π](s))ϕ[π][′]_ (s)( _P_ _[π][′]_ (s[′] _s)ϕ[π][′]_ (s[′]) ds[′])[⊺] _ds_
_≤||_ _−_ _|_ _||_

� �
+ _d[π](s)(ϕ[π][′]_ (s) _ϕ[π](s))(_ _P_ _[π][′]_ (s[′] _s)ϕ[π][′]_ (s[′]) ds[′])[⊺] _ds_
_||_ _−_ _|_ _||_

� �
+ _d[π](s)ϕ[π](s)(_ (P _[π][′]_ (s[′] _s)_ _P_ _[π](s[′]_ _s))ϕ[π][′]_ (s[′]) ds[′])[⊺] _ds_
_||_ _|_ _−_ _|_ _||_

� �
+ _d[π](s)ϕ[π](s)(_ _P_ _[π](s[′]_ _s)(ϕ[π][′]_ (s[′]) _ϕ[π](s[′])) ds[′][⊺]) ds_
_||_ _|_ _−_

_≤_ _Ld||θt+1 −_ _θt||_ (lemma 12)
+ Lϕ||θt+1 − _θt||_ (assumption 8)
+ Lt||θt+1 − _θt||_ (assumption 9)
+ Lϕ||θt+1 − _θt||_ (assumption 8)

� � � �
_d[π][′]_ (s)(ϕ[π][′] (s)( _P_ _[π][′]_ (s[′] _s)ϕ[π][′]_ (s[′]) ds[′])[⊺]) ds _d[π](s)(ϕ[π](s)(_ _P_ _[π](s[′]_ _s)ϕ[π](s[′]) ds[′])[⊺]) ds_
_||_ _|_ _−_ _|_ _||_

_≤_ (Ld + Lt + 2Lϕ)||θt+1 − _θt||_
(A.21)

From equation A.20:

### ⃝2 :

� �
_d[π](s)(ϕ[π](s)(ϕ[π](s))[⊺]) ds_ _d[π][′]_ (s)(ϕ[π][′] (s)(ϕ[π][′] (s))[⊺]) ds
_||_ _−_ _||_

�
(d[π](s) _d[π][′]_ (s))ϕ[π](s)(ϕ[π](s))[⊺] _ds_
_≤||_ _−_ _||_


�
+ _d[π][′]_ (s)(ϕ[π](s) _ϕ[π][′]_ (s))(ϕ[π](s))[⊺] _ds_
_||_ _−_ _||_

�
+ _d[π][′]_ (s)ϕ[π][′] (s)(ϕ[π](s) _ϕ[π][′]_ (s))[⊺] _ds_
_||_ _−_ _||_

_≤_ (Ld + 2Lϕ)||θt+1 − _θt||_

Using equation A.21 and equation A.22 in equation A.20


(A.22)


_||A(θt) −_ _A(θt+1)|| ≤_ (2Ld + 4Lϕ + Lt)||θt+1 − _θt||_ (A.23)


From equation A.18:


-----

### ⃝2 :

� �
_||b(θt) −_ _b(θt+1)|| = ||_ _d[π][′]_ (s)((R[π][′] (s) − _ρ[∗]t+1[)][ϕ][π][′]_ [(][s][)][ ds][ −] _d[π](s)(R[π](s) −_ _ρ[∗]t_ [)][ϕ][π][(][s][)][ ds][||]

� �
_d[π][′]_ (s)(R[π][′] (s)ϕ[π][′] (s) ds _d[π](s)R[π](s)ϕ[π](s) ds_
_≤||_ _−_ _||_

� �
+ || _d[π][′]_ (s)ρ[∗]t+1[ϕ][π][′] [(][s][)][ ds][ −] _d[π](s)ρ[∗]t_ _[ϕ][π][(][s][)][ ds][||]_

�
(d[π][′] (s) _d[π](s))R[π][′]_ (s)ϕ[π][′] (s) ds
_≤||_ _−_ _||_

�
+ _d[π](s)(R[π][′]_ (s) _R[π](s))ϕ[π][′]_ (s) ds
_||_ _−_ _||_

�
+ _d[π](s)R[π](s)(ϕ[π][′]_ (s) _ϕ[π](s) ds_
_||_ _−_ _||_

�
+ || (d[π][′] (s) − _d[π](s))ρ[∗]t+1[ϕ][π][′]_ [(][s][)][ ds][||]

�
+ || _d[π](s)(ρ[∗]t+1_ _[−]_ _[ρ]t[∗][)][ϕ][π][′]_ [(][s][)][ ds][||]

�
+ || _d[π](s)ρ[∗]t_ [(][ϕ][π][′] [(][s][)][ −] _[ϕ][π][(][s][)][ ds][||]_

_≤_ _CrLd||θt+1 −_ _θt||_ ( Assumption 5, Lemma 12)
+ Lr||θt+1 − _θt||_ ( Assumption 10)
+ CrLϕ||θt+1 − _θt||_ (Assumption 5,Assumption 8)
+ CrLd||θt+1 − _θt||_ ( Assumption 5, Lemma 12)
+ Lp||θt+1 − _θt||_ (Lemma 14)
+ CrLϕ||θt+1 − _θt||_ (Assumption 5,Assumption 8)

=⇒||b(θt) − _b(θt+1)|| ≤_ (2LdCr + 2CrLϕ + Lr + Lp)||θt+1 − _θt||_ (A.24)

Using equation A.19, equation A.23 and equation A.24 in equation A.18:


Note:


_||wt[∗]_ _[−]_ _[w]t[∗]+1[|| ≤||][A][(][θ][t][)][−][1][ −]_ _[A][(][θ][t][+1][)][−][1][|| ||][b][(][θ][t][)][||][ +][ ||][A][(][θ][t][+1][)][−][1][|| ||][b][(][θ][t][)][ −]_ _[b][(][θ][t][+1][)][||]_

_≤||A(θt)[−][1]|| ||A(θt) −_ _A(θt+1)|| ||A(θt+1)[−][1]|| ||b(θt)||_

+ ||A(θt+1)[−][1]|| ||b(θt) − _b(θt+1)||_

_≤_ (2Ld + 4Lϕ + Lt)||A(θt)[−][1]|| ||A(θt+1)[−][1]|| ||b(θt)|| ||θt+1 − _θt||_

+ (2LdCr + 2CrLϕ + Lr + Lp)||A(θt+1)[−][1]|| ||θt+1 − _θt||_

 - ||b(θt)|| = || � _d[π](s)(ϕ[π](s)(ϕ[π](s))[⊺]) ds|| ≤_ _Cr (Using Assumption 5)_

 - From Assumption 12, λmin is the lower bound on eigen values of A(θ) for all θ.

∴ _||wt[∗]_ _[−]_ _[w]t[∗]+1[|| ≤]_ _[C][r][(2][L][d][ + 4]λ[2]min[L][ϕ][ +][ L][t][)]_ _||θt+1 −_ _θt||_


+ [(2][L][d][C][r][ + 2][C]λ[r]min[L][ϕ][ +][ L][r][ +][ L][p][)] _||θt+1 −_ _θt||_

_≤_ _Lw||θt+1 −_ _θt||_


-----

where,


_Lw =_ _[C][r][(2][L][d][ + 4][L][ϕ][ +][ L][t][)]_ + [(2][L][d][C][r][ + 2][C][r][L][ϕ][ +][ L][r][ +][ L][p][)]

_λ[2]min_ _λmin_

**Lemma 7. Q[w]diff** _[is the approximate differential Q-value function parameterized by][ w][. Then there]_
_exist a constant Gθ, independent of policy parameter θ, such that:_


_M_ _−1_

_||_ [1] � _∇aQ[w]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s][)][|| ≤] _[G][θ]_

_M_

_i=0_


_Proof._


_||Q[w]diff_ [(][s, a][1][)][ −] _[Q][w]diff_ [(][s, a][2][)][|| ≤] _[L][a][||][a][1]_ _[−]_ _[a][2][||][ (Assumption 6)]_

=⇒||∇aQ[w]diff [(][s, a][)][|| ≤] _[L][a]_
=⇒||∇aQ[w]diff [(][s, a][)][|]a=π(s)[|| ≤] _[L][a]_


(A.25)


_||π(s, θ1) −_ _π(s, θ2)|| ≤_ _Lπ||θ1 −_ _θ2|| (Assumption 7)_ (A.26)
=⇒||∇θπ(s)|| ≤ _Lπ_

Using equation A.25 and equation A.26:


_∥_ [1]

_M_


_M_ _−1_
�

_∇aQ[w]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s][)][∇][θ][π][(][s][)][||]
_i=0_


_≤_ [1]

_M_


_M_ _−1_
�

_||∇aQ[w]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s][)][∇][θ][π][(][s][)][||]
_i=0_


_≤_ _LaLπ = Gθ_

**Lemma 8. The average reward estimate ρt is bounded.**
_∀t > 0 |ρt| ≤_ _Cr + 2Cw_

_Here, Cw is the upper bound on critic parameter wt (Algorithm 2, step 8), Cr is the upper bound on_
_rewards (Assumption 5)._

_Proof._

_|ρ0| ≤_ _Cr + 2Cw_ (Assumption 11)

For t = 1:


� 1
_ρ1 = ρ0 + α0_

_M_


_M�−1_ �

_Rπ(s0,i) + ϕ[π](s[′]0,i[)][⊺][w][¯][0]_ _[−]_ _[ϕ][π][(][s][0][,i][)][⊺][w][¯][0]_ _[−]_ _[ρ][0]_
_i=0_


� 1
= (1 − _α0)ρ0 + α0_

_M_


_M�−1_ �

_R[π](s0,i) + ϕ[π](s[′]0,i[)][⊺][w][¯][0]_ _[−]_ _[ϕ][π][(][s][0][,i][)][⊺][w][¯][0]_
_i=0_


� 1
_|ρ1| ≤_ (1 − _α0)|ρ0| + α0||_

_M_


_M�−1_ �

_R[π](s0,i) + ϕ[π](s[′]0,i[)][⊺][w][¯][0]_ _[−]_ _[ϕ][π][(][s][0][,i][)][⊺][w][¯][0]_ _||_
_i=0_


� 1
_≤_ (1 − _α0)|ρ0| + α0_

_M_


_M�−1_ �

_|R[π](s0,i)| + ||ϕ[π](s[′]0,i[)][|| ||][w][ ¯][0][||][ +][ ||][ϕ][π][(][s][0][,i][)][|| ||][w][ ¯][0][||]_
_i=0_


_≤_ (1 − _α0)(Cr + 2Cw) + (α0)(Cr + 2Cw) = (Cr + 2Cw)_ (Assumption 11)


-----

Therefore the bound hold for t = 1.
Let the bound hold for t = k. We will prove that the bound will also hold for k+1


� 1
_ρk+1 = ρk + αk_

_M_


_M�−1_ �

_Rπ(sk,i) + ϕ[π](s[′]k,i[)][⊺][w][¯][k]_ _[−]_ _[ϕ][π][(][s][k,i][)][⊺][w][¯][k]_ _[−]_ _[ρ][k]_
_i=0_


� 1
= (1 − _αk)ρk + αk_

_M_


_M�−1_ �

_R[π](sk,i) + ϕ[π](s[′]k,i[)][⊺][w][¯][k]_ _[−]_ _[ϕ][π][(][s][k,i][)][⊺][w][¯][k]_
_i=0_


� 1
_|ρk+1| ≤_ (1 − _αk)|ρk| + αk||_

_M_


_M�−1_ �

_R[π](sk,i) + ϕ[π](s[′]k,i[)][⊺][w][¯][k]_ _[−]_ _[ϕ][π][(][s][k,i][)][⊺][w][¯][k]_ _||_
_i=0_


� 1
_≤_ (1 − _αk)|ρk| + αk_

_M_


_M�−1_ �

_|R[π](sk,i)| + ||ϕ[π](s[′]k,i[)][|| ||][w][ ¯][k][||][ +][ ||][ϕ][π][(][s][k,i][)][|| ||][w][ ¯][k][||]_
_i=0_


_≤_ (1 − _αk)(Cr + 2Cw) + (αk)(Cr + 2Cw) = (Cr + 2Cw)_

The bound hold for t = k+1 as well. Hence by the principle of mathematical induction :

_∀t > 0 |ρt| ≤_ _Cr + 2Cw_

**Lemma 9. The norm of target critic estimator ¯wt is bounded**

_∀t > 0 || ¯wt|| ≤_ _Cw_

_Here, Cw is the upper bound on critic parameter wt (Algorithm 2, step 8)._

_Proof. For t=1:_

_w¯1 = (1 −_ _β0) ¯w0 + β0w1_
_|| ¯w1|| ≤_ (1 − _β0)|| ¯w0|| + β0||w1||_
_|| ¯w1|| ≤_ (1 − _β0)Cw + β0Cw_ (Assumption 11)
_|| ¯w1|| ≤_ _Cw_

The bound hold for t=1.
Let the bound hold for t = k. We will prove that the bound will also hold for k+1

_w¯k+1 = (1 −_ _βk) ¯wk + βkwk+1_
_|| ¯wk+1|| ≤_ (1 − _βk)|| ¯wk|| + βk||wk+1||_
_|| ¯wk+1|| ≤_ (1 − _βk)Cw + βkCw_ (Assumption 11)
_|| ¯wk+1|| ≤_ _Cw_

The bound hold for t = k+1 as well. Hence by the principle of mathematical induction :

_∀t > 0 || ¯wt|| ≤_ _Cw_

**Lemma 10. The norm of target average reward estimator ¯ρt is bounded**

_∀t > 0 ||ρ¯t|| ≤_ _Cr + 2Cw_
_Here, Cw is the upper bound on critic parameter wt (Algorithm 2, step 8), Cr is the upper bound on_
_rewards (Assumption 5)._


-----

_Proof. For t=1:_

_ρ¯1 = (1 −_ _β0)¯ρ0 + β0ρ1_
_||ρ¯1|| ≤_ (1 − _β0)||ρ¯0|| + β0||ρ1||_
_||ρ¯1|| ≤_ (1 − _β0)(Cr + 2Cw) + β0(Cr + 2Cw)_ (Assumption 11)
_||ρ¯1|| ≤_ _Cr + 2Cw_

The bound hold for t=1.
Let the bound hold for t = k. We will prove that the bound will also hold for k+1

_ρ¯k+1 = (1 −_ _βk)¯ρk + βkρk+1_
_||ρ¯k+1|| ≤_ (1 − _βk)||ρ¯k|| + βk||ρk+1||_
_||ρ¯k+1|| ≤_ (1 − _βk)(Cr + 2Cw) + βk(Cr + 2Cw)_ (Assumption 11)
_||ρ¯k+1|| ≤_ _Cr + 2Cw_

The bound hold for t = k+1 as well. Hence by the principle of mathematical induction :

_∀t > 0 ||ρ¯t|| ≤_ _Cr + 2Cw_

**Lemma 11. The A(θ) matrix defined below is negative definite for all values of θ (θ is the policy**
_parameter)._

� �
_A(θ) =_ _d[π](s)(ϕ[π](s)(_ _P_ _[π](s[′]_ _s)ϕ[π](s[′]) ds[′]_ _ϕ[π](s))[⊺]_ _ηI) ds_
_|_ _−_ _−_

_x_ _x[⊺]A(θ)x_ _λ_ _x_ _,_ _λ > 0_
_∀_ _≤−_ _||_ _||[2]_

_η is the l2-regularisation coefficient from Algorithm 2 and η > λ[all]max[, where][ λ][all]max_ _[is defined in the]_
_proof below._

_Proof. Let:_

� �
_A[′](θ) =_ _d[π](s)(ϕ[π](s)(_ _P_ _[π](s[′]_ _s)ϕ[π](s[′]) ds[′]_ _ϕ[π](s))[⊺]) ds = A(θ) + ηI_ (A.27)
_|_ _−_

Here, η is the l2-regularization coefficient from Algorithm 2.


_x[⊺]A[′](θ)x = x[⊺][�]_ _[A][′][(][θ][)][⊺]_ [+][ A][′][(][θ][)]

2


�
_x ≤_ _λmax(θ)||x||[2]_


� _A′(θ)⊺_ + A′(θ) �
Here, is a symmetric matrix and λmax(θ) is the maximum eigen value of the

2

� _A′(θ)⊺_ + A′(θ) �

. Using λ[all]max [from Assumption 13:]

2

=⇒ _x[⊺]A[′](θ)x ≤_ _λ[all]max[||][x][||][2]_

_x[⊺](A[′](θ) −_ _ηI)x ≤_ (λ[all]max _[−]_ _[η][)][||][x][||][2]_

_x[⊺]A(θ)x ≤_ (λ[all]max _[−]_ _[η][)][||][x][||][2][ (][using A.37)]_

Here, if we take η > λ[all]max [then we can set][ λ][ =][ η][ −] _[λ]max[all]_ [.]

= _x_ _x[⊺]A(θ)x_ _λ_ _x_ _,_ _λ > 0_
_⇒∀_ _≤−_ _||_ _||[2]_


-----

**Lemma 12. Let θ1 and θ2 be the policy parameter for π[′]** _and π respectively. d[π][′]_ (·) and d[π](·) be
_the stationary state distribution for π[′]_ _and π respectively. Here, DT V denotes the total variation_
_distance between two probability distribution function. We have:_

�
_|d[π][′]_ (s) − _d[π](s)| ds = 2DT V (d[π][′]_ _, d[π][′]_ ) ≤ _Ld||θ1 −_ _θ2||_

_Here, Ld = 2[m][+1](⌈logκ a[−][1]⌉_ + 1/κ)Lt. Lt is the Lipchitz constant for the transition probability
_density function (Assumption 9). Constants a and κ are from Assumption 2, m is the dimension of_
_state space._

_Proof._
�
_|d[π][′]_ (s) − _d[π](s)| ds = 2DT V (d[π][′]_ _, d[π]) = 2DT V (µ1, µ2)_

Let µ1 and µ2 be the stationary state probability measure for π[′] and π respectively. Then we have :

_dµ1 = d[π][′]_ (s) ds
_dµ2 = d[π](s) ds_

Using the result of Theorem 3.1 of Mitrophanov (2005):


�
2DT V (µ1, µ2) ≤ 2 _⌈logκ a[−][1]⌉_ + [1]

_κ_


�
_||K1 −_ _K2||_ (A.28)


where K1 and K2 are probability transition kernel for markov chain induced by policy π[′] and π.

From equation A.28:

�
_||K1 −_ _K2|| ≤_ sup _||_ _g(ds)(K1(·|s) −_ _K2(·|s))||T V_
_||g||T V =1_

� ��
_||_ _g(ds)(K1(·|s) −_ _K2(·|s))||T V ≤_ sup _|_ _f_ (s[′])(K1 − _k2)(ds[′]|s)g(ds)|_
_|f_ _|≤1_

��
_≤_ sup _|_ _f_ (s[′])(P _[π][′]_ (s[′]|s) − _P_ _[π](s[′]|s))(s[′]|s)g(ds)ds[′]|_
_|f_ _|≤1_

��

sup _f_ (s[′]) (P _[π][′]_ (s[′] _s)_ _P_ _[π](s[′]_ _s)_ _g(ds)ds[′]_
_≤_ _|_ _| |_ _|_ _−_ _|_ _|_
_|f_ _|≤1_

� �
_≤_ _Lt||θ1 −_ _θ2||_ _g(ds)_ _ds[′]_

_≤_ 2[m]Lt||θ1 − _θ2||_

=⇒||K1 − _K2|| ≤_ 2[m]Lt||θ1 − _θ2||_ (A.29)

From equation A.28 and equation A.29:

�
_|d[π][′]_ (s) − _d[π](s)| ds = 2DT V (d[π][′]_ _, d[π][′]_ ) ≤ 2[m][+1](⌈logκ a[−][1]⌉ + [1]

_κ_ [)][L][t][||][θ][1][ −] _[θ][2][||]_

_≤_ _Ld||θ1 −_ _θ2||_


-----

**Lemma 13. The optimal critic parameter wϵ[∗]** _[according to compatible function approximation]_
_Lemma (2) is bounded by constant Cwϵ∗ ._

_||wϵ[∗][|| ≤]_ _[C][w]ϵ[∗]_

_Proof. From Lemma2:_


�
_∇θρ(π) =_ _d[π](s)∇aQ[π]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds]

_S_

�
= _d[π](s)∇aQ[w]diff_ [(][s, a][)][|]a=π(s)[∇][θ][π][(][s, θ][)][ ds]

_S_

�
= _d[π](s)∇θπ(s, θ)∇θπ(s, θ)[⊺]wϵ[∗]_ _[ds]_

_S_

= E[∇θπ(s, θ)∇θπ(s, θ)[⊺]]wϵ[∗]

Here,
_Hθ = E[∇θπ(s, θ)∇θπ(s, θ)[⊺]]_


_∇θρ(π) = Hθwϵ[∗]_
=⇒ _wϵ[∗]_ [=][ H]θ[−][1][∇][θ][ρ][(][π][)]

=⇒||wϵ[∗][|| ≤||][H]θ[−][1][|| ||∇][θ][ρ][(][π][)][||]

By using Assumption 14, the lower bound on minimum eigenvalue of Hθ for all θ is λ[ϵ]min [and using]
Assumption 6 and 7 :

_||wϵ[∗][|| ≤]_ _[L]λ[a][ϵ]min[L][π]_ = Cwϵ∗

**Lemma 14. The average reward performance metric, defined in 3, ρ(π)(ρ(θ)) is Lipchitz continuous**
_wrt to the policy (actor) parameter θ._

_||ρ(θ1) −_ _ρ(θ2)|| ≤_ _Lp||θ1 −_ _θ2||_

_Proof. Let θ1 and θ2 be the policy parameters of policy π[′]_ and π.

_||ρ(θ1) −_ _ρ(θ2)|| = ||ρ(π[′]) −_ _ρ(π)||_

� �
= _d[π][′]_ (s)R[π][′] (s) ds _d[π](s)R[π](s) ds_
_||_ _−_ _||_

_S_ _S_

� �

(d[π][′] (s) _d[π](s))R[π][′]_ (s) ds + _d[π](s)(R[π][′]_ (s) _R[π](s)) ds_

_≤||_ _−_ _||_ _||_ _−_ _||_

_S_ _S_


_≤_ _Ld||θ1 −_ _θ2||_ (Lemma 12)
+ Lr||θ1 − _θ2||_ (Assumption 10)
_≤_ (Ld + Lr)||θ1 − _θ2|| = Lp||θ1 −_ _θ2||_ (Ld + Lr = Lp)

**Lemma 15. The optimal critic parameter w(θt)[∗]** _as a function of actor parameter θt is Lipchitz_
_continuous with constant Lv for off-policy case. Note: wt[∗]_ [=][ w][(][θ][t][)][∗][.][ µ][ is the behaviour policy.]

_||wt[∗]_ _[−]_ _[w]t[∗]+1[|| ≤]_ _[L][v][||][θ][t][+1]_ _[−]_ _[θ][t][||]_


-----

_Proof. η is the l2-regularisation coefficient from Algorithm 3 and η > χ[all]max[, where][ χ][all]max_ [is defined]
in Lemma 16. Because of carefully setting the value of η, A(θt) is negative definite. Thus, for
on-policy TD(0) with l2-regularization and target estimators, the following condition holds true for
optimal critic parameter wt[∗][:]

_E[(R[µ](s) −_ _ρ[∗]t_ [)][ϕ][π][(][s][) + (][ϕ][π][(][s][)(][E][[][ϕ][π][(][s][′][)]][ −] _[ϕ][π][(][s][))][⊺]_ _[−]_ _[ηI][)][w]t[∗][] = 0]_

_b(θt) := E[(R[µ](s) −_ _ρ[∗]t_ [)][ϕ][π][(][s][)]]

_A(θt) := E[(ϕ[π](s)(E[ϕ[π](s[′])] −_ _ϕ[π](s))[⊺]_ _−_ _ηI)]_

∴ _b(θt) + A(θt)wt[∗]_ [= 0 =][⇒] _[w]t[∗]_ [=][ −][A][(][θ][t][)][−][1][b][(][θ][t][)]

Expectation above is with respect to stationary state distribution d[µ]( ) of policy µ. Please note the

_·_
abuse of notation here, A(θt) is actually same as A[µ]off [(][θ][t][)][ of Lemma 16.]

_||wt[∗]_ _[−]_ _[w]t[∗]+1[||][ =][ ||][A][(][θ][t][)][−][1][b][(][θ][t][)][ −]_ _[A][(][θ][t][+1][)][−][1][b][(][θ][t][+1][)][||]_

_≤||A(θt)[−][1]b(θt) −_ _A(θt+1)[−][1]b(θt) + A(θt+1)[−][1]b(θt) −_ _A(θt+1)[−][1]b(θt+1)||_

_≤||A(θt)[−][1]_ _−_ _A(θt+1)[−][1]|| ||b(θt)||_ _⃝1_

+ ||A(θt+1)[−][1]|| ||b(θt) − _b(θt+1)||_ _⃝2_
(A.30)

From equation A.30:

### ⃝1 :

_||A(θt)[−][1]_ _−_ _A(θt+1)[−][1]|| = ||A(θt)[−][1]A(θt+1)A(θt+1)[−][1]_ _−_ _A(θt)[−][1]A(θt)[A](θt+1)[−][1]||_

(A.31)
_≤||A(θt)[−][1]|| ||A(θt) −_ _A(θt+1)|| ||A(θt+1)[−][1]||_

From equation A.31:

Here, π[′] and π represents the policy with parameter θt+1 and θt respectively and µ be the behaviour
policy .

� �
_||A(θt) −_ _A(θt+1)|| ≤||_ _d[µ](s)(ϕ[π][′]_ (s)( _P_ _[µ](s[′]|s)ϕ[π][′]_ (s[′]) ds[′] _−_ _ϕ[π][′]_ (s))[⊺] _−_ _ηI) ds_

� �
_d[µ](s)(ϕ[π](s)(_ _P_ _[µ](s[′]_ _s)ϕ[π](s[′]) ds[′]_ _ϕ[π](s))[⊺]_ _ηI) ds_
_−_ _|_ _−_ _−_ _||_

� �
_d[µ](s)(ϕ[π][′]_ (s)( _P_ _[µ](s[′]_ _s)ϕ[π][′]_ (s[′]) ds[′])[⊺]) ds
_≤||_ _|_

� �
_d[µ](s)(ϕ[π](s)(_ _P_ _[µ](s[′]_ _s)ϕ[π](s[′]) ds[′])[⊺]) ds_ 1
_−_ _|_ _||_ _⃝_

� �
_d[µ](s)(ϕ[π](s)(ϕ[π](s))[⊺]) ds_ _d[µ](s)(ϕ[π][′]_ (s)(ϕ[π][′] (s))[⊺]) ds 2
_≤||_ _−_ _||_ _⃝_

(A.32)

From equation A.32:


-----

### ⃝1 :

� � � �
_d[µ](s)(ϕ[π][′]_ (s)( _P_ _[µ](s[′]_ _s)ϕ[π][′]_ (s[′]) ds[′])[⊺]) ds _d[µ](s)(ϕ[π](s)(_ _P_ _[µ](s[′]_ _s)ϕ[π](s[′]) ds[′])[⊺]) ds_
_||_ _|_ _−_ _|_ _||_

� �
(d[µ](s) _d[µ](s))ϕ[π][′]_ (s)( _P_ _[µ](s[′]_ _s)ϕ[π][′]_ (s[′]) ds[′])[⊺] _ds_
_≤||_ _−_ _|_ _||_

� �
+ _d[µ](s)(ϕ[π][′]_ (s) _ϕ[π](s))(_ _P_ _[µ](s[′]_ _s)ϕ[π][′]_ (s[′]) ds[′])[⊺] _ds_
_||_ _−_ _|_ _||_

� �
+ _d[µ](s)ϕ[π](s)(_ (P _[µ](s[′]_ _s)_ _P_ _[µ](s[′]_ _s))ϕ[π][′]_ (s[′]) ds[′])[⊺] _ds_
_||_ _|_ _−_ _|_ _||_

� �
+ _d[µ](s)ϕ[π](s)(_ _P_ _[µ](s[′]_ _s)(ϕ[π][′]_ (s[′]) _ϕ[π](s[′])) ds[′][⊺]) ds_
_||_ _|_ _−_

_≤_ _Lϕ||θt+1 −_ _θt||_ (Assumption 8)
+ Lϕ||θt+1 − _θt||_ (Assumption 8)

� � � �
_d[µ](s)(ϕ[π][′]_ (s)( _P_ _[µ](s[′]_ _s)ϕ[π][′]_ (s[′]) ds[′])[⊺]) ds _d[µ](s)(ϕ[π](s)(_ _P_ _[µ](s[′]_ _s)ϕ[π](s[′]) ds[′])[⊺]) ds_
_||_ _|_ _−_ _|_ _||_

_≤_ 2Lϕ||θt+1 − _θt||_
(A.33)

From equation A.32:

### ⃝2 :

� �
_d[µ](s)(ϕ[π](s)(ϕ[π](s))[⊺]) ds_ _d[µ](s)(ϕ[π][′]_ (s)(ϕ[π][′] (s))[⊺]) ds
_||_ _−_ _||_


�
_d[µ](s)(ϕ[π](s)_ _ϕ[π][′]_ (s))(ϕ[π](s))[⊺] _ds_
_≤||_ _−_ _||_

�
+ _d[µ](s)ϕ[π][′]_ (s)(ϕ[π](s) _ϕ[π][′]_ (s))[⊺] _ds_
_||_ _−_ _||_

_≤_ 2Lϕ||θt+1 − _θt||_

Using equation A.33 and equation A.34 in equation A.32


(A.34)


_||A(θt) −_ _A(θt+1)|| ≤_ (4Lϕ + Lt)||θt+1 − _θt||_ (A.35)


From equation A.30:


-----

### ⃝2 :

� �
_||b(θt) −_ _b(θt+1)|| = ||_ _d[µ](s)((R[µ](s) −_ _ρ[∗]t+1[)][ϕ][π][′]_ [(][s][)][ ds][ −] _d[µ](s)(R[µ](s) −_ _ρ[∗]t_ [)][ϕ][π][(][s][)][ ds][||]

� �
_d[µ](s)(R[µ](s)ϕ[π][′]_ (s) ds _d[µ](s)R[µ](s)ϕ[π](s) ds_
_≤||_ _−_ _||_

� �
+ || _d[µ](s)ρ[∗]t+1[ϕ][π][′]_ [(][s][)][ ds][ −] _d[µ](s)ρ[∗]t_ _[ϕ][π][(][s][)][ ds][||]_

�
_d[µ](s)(R[µ](s)_ _R[µ](s))ϕ[π][′]_ (s) ds
_≤||_ _−_ _||_

�
+ _d[µ](s)R[µ](s)(ϕ[π][′]_ (s) _ϕ[π](s) ds_
_||_ _−_ _||_

�
+ || _d[µ](s)(ρ[∗]t+1_ _[−]_ _[ρ]t[∗][)][ϕ][π][′]_ [(][s][)][ ds][||]

�
+ || _d[µ](s)ρ[∗]t_ [(][ϕ][π][′] [(][s][)][ −] _[ϕ][π][(][s][)][ ds][||]_

_≤_ _CrLϕ||θt+1 −_ _θt||_ (Assumption 5)
+ Lp||θt+1 − _θt||_ (Lemma 14)
+ CrLϕ||θt+1 − _θt||_ (Assumption 5)

=⇒||b(θt) − _b(θt+1)|| ≤_ (2CrLϕ + Lp)||θt+1 − _θt||_ (A.36)

Using equation A.31, equation A.35 and equation A.36 in equation A.30:

_||wt[∗]_ _[−]_ _[w]t[∗]+1[|| ≤||][A][(][θ][t][)][−][1][ −]_ _[A][(][θ][t][+1][)][−][1][|| ||][b][(][θ][t][)][||][ +][ ||][A][(][θ][t][+1][)][−][1][|| ||][b][(][θ][t][)][ −]_ _[b][(][θ][t][+1][)][||]_

_≤||A(θt)[−][1]|| ||A(θt) −_ _A(θt+1)|| ||A(θt+1)[−][1]|| ||b(θt)||_

+ ||A(θt+1)[−][1]|| ||b(θt) − _b(θt+1)||_

_≤_ 4Lϕ||A(θt)[−][1]|| ||A(θt+1)[−][1]|| ||b(θt)|| ||θt+1 − _θt||_

+ (2CrLϕ + Lp)||A(θt+1)[−][1]|| ||θt+1 − _θt||_

Note:

    - ||b(θt)|| = || � _d[µ](s)(ϕ[π](s)(ϕ[π](s))[⊺]) ds|| ≤_ _Cr_ (Assumption 5)

    - Let λmin is the lower bound on eigen values of A(θ) for all θ.

∴ _||wt[∗]_ _[−]_ _[w]t[∗]+1[|| ≤]_ _[C][r]λ[(4][2]min[L][ϕ][)]_ _||θt+1 −_ _θt||_

+ [(2][C][r]λ[L]min[ϕ][ +][ L][p][)] _||θt+1 −_ _θt||_

_≤_ _Lv||θt+1 −_ _θt||_

where,

_Lv = [4][C][r][L][ϕ]_ + _[C][r][L][ϕ]_

_λ[2]min_ _λmin_


**Lemma 16. The A[µ]off** [(][θ][)][ matrix defined below is negative definite for all values of][ θ][ (][θ][ is the policy]
_parameter). θ[µ]_ _is the policy parameter for behaviour policy µ._

� �
_A[µ]off_ [(][θ][) :=] _d[µ](s)(ϕ[π](s)(_ _P_ _[π](s[′]|s)ϕ[π](s[′]) ds[′]_ _−_ _ϕ[π](s))[⊺]_ _−_ _ηI) ds_


-----

_∀x_ _x[⊺]A[µ]off_ [(][θ][)][x][ ≤−][λ][||][x][||][2][,] _λ > 0_

_η is the l2-regularisation coefficient from Algorithm 3 and η > χ[all]max[, where][ χ][all]max_ _[is defined in the]_
_proof below._

_Proof. Let:_

� �
_A[µ]off_ _′(θ) =_ _d[µ](s)(ϕ[π](s)(_ _P_ _[π](s[′]|s)ϕ[π](s[′]) ds[′]_ _−_ _ϕ[π](s))[⊺]) ds = A[µ]off_ [(][θ][) +][ ηI] (A.37)

Here, η is the l2-regularization coefficient from Algorithm 2.


_x[⊺]A[µ]off_ _′(θ)x = x⊺[�]_ _[A]off[µ]_ _′(θ)⊺_ 2+ A[µ]off _′(θ)_


�
_x ≤_ _χmax(θ)||x||[2]_


� _A[µ]off_ _′(θ)⊺_ + A[µ]off _′(θ)_ �
Here, is a symmetric matrix and χmax(θ) is the maximum eigen value of

2

� _A[µ]off_ _′(θ)⊺_ + A[µ]off _′(θ)_ �
the . Using χ[all]max [from Assumption 15:]

2

=⇒ _x[⊺]A[µ]off_ _′(θ)x ≤_ _χallmax[||][x][||][2]_

_x[⊺](A[µ]off_ _′(θ) −_ _ηI)x ≤_ (χallmax _[−]_ _[η][)][||][x][||][2]_

_x[⊺]A[µ]off_ [(][θ][)][x][ ≤] [(][χ]max[all]

_[−]_ _[η][)][||][x][||][2][ (][using A.37)]_

Here, if we take η > χ[all]max [then we can set][ λ][ =][ η][ −] _[χ]max[all]_ [.]

=⇒∀x _x[⊺]A[µ]off_ [(][θ][)][x][ ≤−][λ][||][x][||][2][,] _λ > 0_

**Lemma 17. Let the cumulative error of off-policy actor be** [�]t[T]=0[ −][1] _[E][||][ �]∇θρ(θt)||[2]_ _and cumulative_
_error of critic be_ [�]t[T]=0[ −][1] _[E][||][∆][w][t][||][2][.][ θ][t][ and][ w][t][ are the actor and linear critic parameter at time t.][ θ][µ]_
_is the policy parameter for behavior policy µ. Bound on the cumulative error of off-policy actor with_
_behaviour policy µ is proven using cumulative error of critic as:_


_T −1_
�

_E||∆wt||[2]) + 6Cπ[4][(][τ][ 2][ + 4]M [C]w[2]_ _ϵ[∗]_ [)]
_t=0_


1

_T_


_T −1_
� _E||∇[�]θρ(θt)||[2]_ _≤_ 4 _C[C]γ[r]_ _T_ _[v][−][1]_ + 6Cπ[4][( 1]T

_t=0_


+ 2 _[C][γ][L][J]_ _[G]θ[2]_ _T_ _[−][v]_ + _[Z]_

1 _v_ _T_
_−_


_T −1_
�

_E_ _θ[µ]_ _θ[t]_
_||_ _−_ _||[2]_
_t=0_


_Here, Cr is the upper bound on rewards (Assumption 5), Cγ, v are constants used for step size_
_γt (Assumption 3, ∥∇θπ(s)∥≤_ _Cπ (Assumption 7), ∆wt = wt −_ _wt[∗][, τ][ = max][t]_ _[∥][w]t[∗]_ _[−]_ _[w]ϵ,t[∗]_ _[∥][,]_
_wϵ[∗]_ _[is the optimal critic parameter according to Lemma2.][ w]t[∗]_ _[is the optimal parameters given]_
_by TD(0) algorithm corresponding to policy parameter θt. Constant Cwϵ∗_ _[is defined in Lemma]_
_13. LJ is the coefficient used in smoothness condition of the non convex function ρ(θ). Constant_
_Gθ is defined in Lemma 7. M is the size of batch of samples used to update parameters. Z =_
2[m][+1]C(⌈logκ a[−][1]⌉ + 1/κ)Lt with Lt being the Lipchitz constant for the transition probability
_density function (Assumption 9). Constants a and κ are from Assumption 2, m is the dimension of_
_state space, and C = maxs ∥∇aQ[π]diff_ [(][s, a][)][|][a][=][π][(][s][)][∇][θ][π][(][s, θ][)][∥][.]


-----

_Proof._


_T −1_
�

_E||∇θρ(θt) +_ _∇[�]θρ(θt) −∇θρ(θt)||[2]_
_t=0_


1

_T_


_T −1_
�

_E||∇[�]θρ(θt)||[2]_ = T[1]
_t=0_


_T −1_
�

_E||∇[�]θρ(θt) −∇θρ(θt)||[2]_
_t=0_


_≤_ [1]

_T_

Using Theorem 2 and Lemma 3:


_T −1_
�

_E||∇θρ(θt)||[2]_ + [1]

_T_

_t=0_


_T −1_
�

_E||∆wt||[2]) + 6Cπ[4][(][τ][ 2][ + 4]M [C]w[2]_ _ϵ[∗]_ [)]
_t=0_


1

_T_


_T −1_
� _E||∇[�]θρ(θt)||[2]_ _≤_ 4 _C[C]γ[r]_ _T_ _[v][−][1]_ + 6Cπ[4][( 1]T

_t=0_


+ 2 _[C][γ][L][J]_ _[G]θ[2]_ _T_ _[−][v]_ + _[Z]_

1 _v_ _T_
_−_


_T −1_
�

_E_ _θ[µ]_ _θ[t]_
_||_ _−_ _||[2]_
_t=0_


**Theorem 4. The off-policy average reward actor critic algorithm (Algorithm 3) with behavior policy**
_µ obtains an ϵ-accurate optimal point with sample complexity of Ω(ϵ[−][2][.][5]). Here θµ refers to the_
_behavior policy parameter and θt refers to the target or current policy parameter. We obtain_


� 1 �
0≤mint≤T −1 _[E][∥]∇[�]θρ(θt)∥[2]_ = O _T_ [0][.][4] + O(1) + O(Wθ[2][)]


_≤_ _ϵ + O(1) + O(Wθ[2][)]_
_where Wθ := maxt_ _∥θµ −_ _θt∥._

_Proof. Lemma 4 and Lemma 5 will hold in the case of off-policy update. Lemma 4 will require_
Lemma 15 instead of Lemma 6.

Using Lemma 4 and Lemma 5 and using the procedure followed in Theorem 3 to obtain asymptotic
notations, we have:


�
+ (1) (A.38)
_O_


1

_T_


_T� −1_ � 1

_E||∆wt||[2]_ = O

_T_ [1][−][σ]

_t=0_


� � 1
+
_O_

_T_ _[σ]_


� � 1
+
_O_

_T_ [2(][v][−][σ][)]


Using Lemma 17 and equation A.38:

� 1
0≤mint≤T −1 _[E][||][ �]∇θρ(θt)||[2]_ = O _T_ [1][−][v]


� � 1
+
_O_

_T_ _[v]_


� � 1
+
_O_

_T_ [1][−][σ]


� � 1
+
_O_

_T_ _[σ]_


�


_T −1_
�

_E_ _θ[µ]_ _θ[t]_
_||_ _−_ _||[2]_
_t=0_


� 1
+
_O_

_T_ [2(][v][−][σ][)]

By setting v = 3/5 and σ = 2/5, we obtain:


�
+ (1) + _[Z]_
_O_

_T_


-----

� 1
0≤mint≤T −1 _[E][||][ �]∇θρ(θt)||[2]_ = O _T_ [0][.][4]

� 1
=
_O_

_T_ [0][.][4]

� 1
=
_O_

_T_ [0][.][4]


Further,


� 1
_O_

_T_ [0][.][4]


� _T −1_

�

+ (1) + _[Z]_ _E_ _θ[µ]_ _θ[t]_
_O_ _||_ _−_ _||[2]_

_T_

_t=0_

�
+ O(1) + ZNθ[2]

�
+ O(1) + O(Nθ[2][)][.]

�
_ϵ._
_≤_


Hence, the sample complexity of off-policy average reward actor-critic algorithm is Ω(ϵ[−][2][.][5]).

A.2 BOUNDEDNESS OF CRITIC PARAMETER

In this section we prove the critic parameter w used in Algorithm 2 and 3 is bounded even without
using projection operator ΓCw defined as ΓCw : R[k] _→_ _B, where B(⊂_ R[k]) is a compact convex set.
Let policy π is parameterized by θ.

For simplicity of proof we are assuming the batch size M to be 1. Critic parameter wt ∈ R[k],
_ϕ[π](s) ∈_ R[k] and ρt is a scalar. Let the update rules used for critic parameter and average reward
estimator be as follows:

� �
_wt+1 = wt + αt_ _R[π](st) −_ _ρ¯t + ϕ[π](s[′]t[)][⊺][w][¯][t]_ _[−]_ _[ϕ][π][(][s][t][)][⊺][w][t]_ _ϕ[π](st) −_ _αtηwt_

� �
_ρt+1 = ρt + αt_ _R[π](st) −_ _ρt + ϕ[π](s[′]t[)][⊺][w][¯][t]_ _[−]_ _[ϕ][π][(][s][t][)][⊺][w][¯][t]_ (A.39)

_wt+1 = wt + βt(wt+1_ _wt+1)_
_−_

_ρt+1 = ρt + βt(ρt+1 −_ _ρt+1)_

Let us define zt as [wt ρt][⊺] and ¯zt as [ ¯wt ¯ρt][⊺]. 0 is a vector in R[k] and I0 is an identity matrix in
R[(][k][+1)][×][(][k][+1)] with I0[k][k] = 0 (assuming indexing starts from 0).


_R[π](st)_ �ϕ[π]1(st)� + �ϕ[π]ϕ([π]s[′]t([)]s[⊺]t)[−]ϕ[π][ϕ]([π]s[′]t[(][)][s][⊺][t][)][⊺] _−ϕ[π]0(st)_


��w¯t
_ρ¯t_


�
+ αt


�


�wt+1
_ρt+1_

�w¯t+1
_ρ¯t+1_


� = �wt
_ρt_


� = �w¯t
_ρ¯t_


�


(A.40)


�ϕ[π](st)ϕ[π](st)[⊺] **0**
_−_ **0[⊺]** 1


��wρtt� _−_ _ηI0_


�wt
_ρt_


� [�]


� + βt� [�]wρtt+1+1


� �w¯t
_−_ _ρ¯t_


��


Here, R[π](st) �ϕ[π](st)� = Rϕ[π][(][s][t][)][,][ A][ϕ][(][s][t][, s]t[′] [) =] � _ϕ[π](st)ϕ[π](s[′]t[)][⊺]_ _−ϕ[π](st)�_ and Bϕ(st) =
1 _ϕ[π](s[′]t[)][⊺]_ _[−]_ _[ϕ][π][(][s][t][)][⊺]_ 0

�ϕ[π](st)ϕ[π](st)[⊺] **0�**
**0[⊺]** 1

_zt+1 = zt + αt�Rϕ[π][(][s][t][) +][ A][ϕ][(][s][t][, s][′]t[)¯][z][t]_ _[−]_ [(][B][ϕ][(][s][t][) +][ ηI][0][)][z][t]� (A.41)

_z¯t+1 = ¯zt + βt(zt+1_ _z¯t)_
_−_


Now, we will use the extension of stability criteria for iterates given Borkar & Meyn (2000) to two
timescale stochastic approximation scheme (Lakshminarayanan & Bhatnagar, 2017) to show the


-----

boundedness of the critic parameter and average reward estimator together. Let us write A.41 in the
standard form of stochastic approximation scheme.

_zt+1 = zt + αt�h(zt, ¯zt) + Mt[1]+1�_

Let, _R¯ϕ[π]_ = �S _[d][π][(][s][t][)][R]ϕ[π][(][s][t][)][ ds][t][,]_ _A¯ϕ_ = �S _[d][π][(][s][t][)]_ �S _[P][ π][(][s]t[′]_ _[|][s][t][)][A][ϕ][(][s][t][, s][′]t[)][ ds][′]t_ _[ds][t][,]_ _B¯ϕ_ =

�

_S_ _[d][π][(][s][t][)][B][ϕ][(][s][t][)][ s][t]_


Here,


�
_h(zt, ¯zt) =_ _d[π](st)�Rϕ[π][(][s][t][) +][ A][ϕ][(][s][t][, s][′]t[)¯][z][t]_ _[−]_ [(][B][ϕ][(][s][t][) +][ ηI][0][)][z][t]� _dst_

_S_

= R[¯]ϕ[π] [+ ¯][A][ϕ][z][¯][t] _[−]_ [( ¯][B][ϕ] [+][ ηI][0][)][z][t]
_Mt[1]+1_ [=][R]ϕ[π][(][s][t][) +][ A][ϕ][(][s][t][, s][′]t[)¯][z][t] _[−]_ [(][B][ϕ][(][s][t][) +][ ηI][0][)][z][t] _[−]_ _[h][(][z][t][,][ ¯][z][t][)]_

_z¯t+1 = ¯zt + βt�g(zt, ¯zt) + Mt[2]+1_ [+][ ϵ][(][n][)]�

Here,
_g(zt, ¯zt) =λ(¯zt) −_ _z¯t_
_Mt[2]+1_ [=0]

_λ(¯zt) =(Bϕ + ηI0)[−][1](Rϕ[π]_ [+][ A][ϕ][z][¯][t][)]

_ϵ(n) =zt+1 −_ _λ(¯zt)_

_λ(¯zt) is the unique globally asymptotically stable equilibrium point of the ODE ˙z = h(z(t), ¯z). λ_
used here has no relation to usage of λ in any other section of the paper. Using Lemma 1 of Chapter
6 of (Borkar, 2009), we have ∥zt+1 − _λ(¯zt)∥→_ 0. Hence ϵ(n) = o(1). Therefore we can use the
conclusion of (Lakshminarayanan & Bhatnagar, 2017).

We will now satisfy condition A1 till condition A5 of (Lakshminarayanan & Bhatnagar, 2017) to
prove the boundedness of the critic parameter:

**Condition A1:**
_∥h(z1, ¯z1) −_ _h(z2, ¯z2)∥_ = ∥A[¯]ϕ(¯z1 − _z¯2) −_ ( B[¯]ϕ + ηI0)(z1 − _z2)∥_

_≤∥A[¯]ϕ∥∥z¯1 −_ _z¯2∥_ + ∥B[¯]ϕ + ηI0∥∥z1 − _z2∥_

_≤_ max(∥A[¯]ϕ∥, ∥B[¯]ϕ + ηI0∥)(∥z¯1 − _z¯2∥_ + ∥∥z1 − _z2∥)_

= Lh(∥z¯1 − _z¯2∥_ + ∥∥z1 − _z2∥) (Lh = max(∥A[¯]ϕ∥, ∥B[¯]ϕ + ηI0∥))_
(A.42)


Therefore, h(z, ¯z) is Lipchitz continuous with constant Lh.

_∥g(z1, ¯z1) −_ _g(z2, ¯z2)∥_ = ∥(( B[¯]ϕ + ηI0)Aϕ − _I)(¯z1 −_ _z¯2)∥_

_≤∥(( B[¯]ϕ + ηI0)Aϕ −_ _I)∥∥z¯1 −_ _z¯2∥_

= Lg∥z¯1 − _z¯2∥_ (Lg = ∥(( B[¯]ϕ + ηI0)Aϕ − _I)∥)_


(A.43)


Therefore, g(z, ¯z) is Lipchitz continuous with constant Lg.

Using A.42 and A.43, condition A1 is satisfied.

**Condition A2:**
Let us define an increasing sequence of σ−fields {Ft} as {zm, ¯zm, Mm[1] _[,][ M][2]m[, m][ ≤]_ _[t][}][.]_

_E[Mt[1]+1[|F][t][] =][ E][[][R]ϕ[π][(][s][t][) +][ A][ϕ][(][s][t][, s][′]t[)¯][z][t]_ _[−]_ [(][B][ϕ][(][s][t][) +][ ηI][0][)][z][t] _[−]_ _[h][(][z][t][,][ ¯][z][t][)][|F][t][]]_

�
= _d[π](st)�Rϕ[π][(][s][t][) +][ A][ϕ][(][s][t][, s][′]t[)¯][z][t]_ _[−]_ [(][B][ϕ][(][s][t][) +][ ηI][0][)][z][t]� _dst −_ _h(zt, ¯zt)_

_S_


= 0


-----

_E[Mt[2]+1[|F][t][] = 0]_

Hence, {Mt[1][}][ and][ {M]t[2][}][ are martingale difference sequence.]

_∥Mt[1]+1[∥][2][ =][ ∥][(][R]ϕ[π][(][s][t][)][ −]_ _[R][¯][ϕ][) + (][A][ϕ][(][s][t][, s][′]t[)][ −]_ _[A][¯][ϕ][)¯][z][t]_ _[−]_ [(][B][ϕ][(][s][t][)][ −] _[B][¯][ϕ][(][s][t][))][z][t][∥][2]_

_≤_ 3(∥Rϕ[π][(][s][t][)][ −] _[R][¯][ϕ][∥][2][ +][ ∥][(][A][ϕ][(][s][t][, s][′]t[)][ −]_ _[A][¯][ϕ][)][∥][2][∥][z][¯][t][∥][2][ +][ ∥][B][ϕ][(][s][t][)][ −]_ _[B][¯][ϕ][(][s][t][))][∥][2][∥][z][t][∥][2][)]_

_≤_ _K1(1 + ∥zt∥[2]_ + ∥z¯t∥[2])


Here, K1 = 6 max(∥Rϕ[π][(][s][t][)][∥][,][ ∥][(][A][ϕ][(][s][t][, s]t[′] [)][∥][,][ ∥][B][ϕ][(][s][t][)][∥][)][ and it follows from Assumption 4 and 5.]
We have, E[∥Mt[1]+1[∥][2][||F][t][]][ ≤] _[K][1][(1+][∥][z][t][∥][2][+][∥][z][¯][t][∥][2][)][ and][ E][[][∥M][2]t+1[∥][2][||F][t][]][ ≤]_ _[K][2][(1+][∥][z][t][∥][2][+][∥][z][¯][t][∥][2][)][.]_
_K2 can be any positive constant. Hence condition A2 is satisfied._

**Condition A3:**

We have, [�]t _[α][t][ =][ �]t_ (1+Cαt)[σ] = ∞, [�]t _[β][t][ =][ �]t_ (1+Cβt)[u] = ∞ and [�]t[(][α]t[2] [+][ β]t[2][) =]

�t �( (1+Cαt)[σ][ )][2][ + (] (1+Cβt)[u][ )][2][�] _< ∞. We can carefully set the value of σ and u to satisfy the conditions_

on step sizes.

**Condition A4:**


_hc(z, ¯z) :=_ _[h][(][cz, c][z][¯][)]_

_c_

_R¯ϕ[π]_ [+][ c][ ¯][A][ϕ][z][¯][t][ −] _[c][( ¯][B][ϕ][ +][ ηI][0][)][z][t]_
_hc(z, ¯z) =_

_c_

_R¯ϕ[π]_ [+][ c][ ¯][A][ϕ][z][¯][t][ −] _[c][( ¯][B][ϕ][ +][ ηI][0][)][z][t]_

lim
_c→∞_ _[h][c][(][z,][ ¯][z][) = lim]c→∞_ _c_

= A[¯]ϕz¯t ( B[¯]ϕ + ηI0)zt
_−_


Let us define h∞(zt, ¯zt) := A[¯]ϕz¯t − ( B[¯]ϕ + ηI0)zt.The ODE ˙z(t) := h∞(z(t), ¯z) has a unique
globally asymptotically stable equilibrium pointpositive definite matrix. Let Cϕ = �S _[d][π][(][s][t][)][ϕ][π][(][s][t] λ[)][ϕ]∞[π][(](¯[s]z[t]) = ( [)][⊺]_ _[ds][t]B[.][¯]ϕ + ηI0)[−][1][ ¯]Aϕz¯ if ( B[¯]ϕ + ηI0) is_

_B¯ϕ + ηI0 =_ �Cϕ + ηI **0�**
**0[⊺]** 1


�Cϕ + ηI **0**

[w[⊺] _ρ]_
**0[⊺]** 1


��w�

= w[⊺](Cϕ + ηI)w + ρ[2]

_ρ_


If η is strictly greater than negative of the minimum eigenvalue of Cϕ then,


�w
_∀_ _p_


� �0� �Cϕ + ηI **0**
= [w[⊺] _ρ]_
_̸_ 0 **0[⊺]** 1


��w
_ρ_


�

_> 0_

(A.44)
�

_> 0_


�w
_∀_ _p_


� �0
=
_̸_ 0


� _w_

[w[⊺] _ρ]_ �B¯ϕ + ηI0� [�]
_ρ_


Hence, for η + λmin(Cϕ) > 0, _B[¯]ϕ + ηI0 is positive definite matrix._ Therefore, the ODE
_z˙(t) := h∞(z(t), ¯z) has a unique globally asymptotically stable equilibrium point λ∞(¯z) and_
_λ∞(0) = 0. Condition A4 is satisfied._

**Condition A5:**


-----

_gc(¯z) :=_ _[g][(][cλ][∞][(¯][z][)][, c][z][¯][)]_

_c_

( B[¯]ϕ + ηI0)[−][1](Rϕ[π] [+][ cA][ϕ][z][¯][)][ −] _[c][z][¯]_
_gc(¯z) =_

_c_

( B[¯]ϕ + ηI0)[−][1](Rϕ[π] [+][ cA][ϕ][z][¯][)][ −] _[c][z][¯]_
lim
_c→∞_ _[g][c][(¯][z][) = lim]c→∞_ _c_

= ( B[¯]ϕ + ηI0)[−][1]Aϕz¯ − _z¯_


(A.45)


Let us define g∞ := (( B[¯]ϕ + ηI0)[−][1]Aϕ − _I)¯z. The ODE_ _z¯[˙](t) = g∞(¯z(t)) has origin as its unique_
globally asymptotically stable equilibrium if I − ( B[¯]ϕ + ηI0)[−][1]Aϕ is positive definite matrix.

_∥· ∥_ refers to L2-norm. λi are the eigenvalues of the matrix Cϕ. Let us assume the following:


1 1
max(1, max
_i_ [(] _λi + η_ [)) =][ ∥][( ¯][B][ϕ][ +][ ηI][0][)][−][1][∥≤] _∥Aϕ∥_

=⇒∥( B[¯]ϕ + ηI0)[−][1]∥∥Aϕ∥ _< 1_

=⇒∥x∥∥( B[¯]ϕ + ηI0)[−][1]∥∥Aϕ∥∥x∥ _< ∥x∥[2]_

=⇒∥x[⊺]( B[¯]ϕ + ηI0)[−][1]Aϕx∥ _< ∥x∥[2]_

=⇒ _x[⊺]( B[¯]ϕ + ηI0)[−][1]Aϕx < ∥x∥[2]_

=⇒ _x[⊺](I −_ ( B[¯]ϕ + ηI0)[−][1]Aϕ)x > 0


(A.46)


Hence, if max(1, maxi( _λi1+η_ [))][ <] _∥A1ϕ∥_ [, then][ I][ −] [( ¯][B][ϕ][ +] _[ηI][0][)][−][1][A][ϕ][ is positive definite matrix. There-]_

fore, the ODE _z¯[˙](t) = g∞(¯z(t)) has origin as its unique globally asymptotically stable. Condition A5_
is satisfied.

Let us the consider the ODE ˙z(t) = h(z(t), ¯z). Here, h(z(t), ¯z) = R[¯]ϕ[π] [+ ¯][A][ϕ][z][¯][ −] [( ¯][B][ϕ][ +][ ηI][0][)][z][t]
As earlier, for η + λmin(Cϕ) > 0, _B[¯]ϕ + ηI0 is positive definite matrix. Therefore, the ODE_
_z˙(t) := h(z(t), ¯z) has a unique globally asymptotically stable equilibrium point λ(¯z) = (Bϕ +_
_ηI0)[−][1](Rϕ[π]_ [+][ A][ϕ][z][¯][t][)][.]

Conditions A1 to A5 are satisfied, therefore supt ∥zt∥ _< ∞, which implies iterates are bounded._
Hence critic parameter wt is bounded.


-----

### B ALGORITHM AND HYPERPARAMETERS

B.1 (OFF-POLICY) ARO-DDPG PRACTICAL ALGORITHM

**Algorithm 1 (Off-Policy) ARO-DDPG Practical Algorithm**

Initialize actor parameter θ and critic parameters w1, w2.Initialize actor target parameter θ → _θ_
Initialize critic target parameters w1 → _w1, w2 →_ _w2. Initialize average reward parameter ρ._
Initialize target average reward parameter ρ _ρ. Initialize Replay buffer = {}_
_→_

1: t = 0, s0 = env.reset()
2: while t ≤ total steps do
3: _at = π(st) + ϵ {ϵ denotes the noise}_

4: _st+1 ∼_ _P_ (·|st, at) and rt = R(st, at)

5: Store {st, at, st+1} in the Replay Buffer

6: **if t % eval_freq == 0 then**

7: Evaluate(agent)

8: **end if**

9: **if t % critic_update_freq == 0 then**

10: Update critic according to (24) - (27)

11: **end if**

12: **if t % actor_update_freq == 0 then**

13: Update actor according to (28) - (29)

14: Update target estimators according to (30) - (32)

15: **end if**

16: **if st+1 is terminal then**

17: _st = env.reset()_

18: **else**

19: _st = st+1_

20: **end if**

21: end while

B.2 FINITE TIME ANALYSIS ALGORITHM

Here we present the algorithm with linear function approximator for which finite time analysis was
done. Bt denotes the batch of tuple of the form {si, ai, s[′]i[}][ sampled from the buffer at timestep][ t][.]
ΓCw is a projection operator defined as ΓCw : R[k] _→_ _B, where B(⊂_ R[k]) is a compact convex set.
Here, the critic parameter w ∈ R[k].


-----

**Algorithm 2 On-policy AR-DPG with Linear FA**

Initialize actor parameter θ and critic parameters w. Initialize actor target parameter θ _θ._
_→_
Initialize critic target parameters w _w.Initialize average reward parameter ρ_
_→_
Initialize target average reward parameter ρ _ρ_
_→_
Initialize buffer = {}

1: t = 0, s0 = env.reset()
2: while t ≤ total steps do
3: _at = π(st) + ϵ {ϵ is the noise}_

4: _st+1 ∼_ _P_ (·|st, at) and rt = R(st, at)

5: Store {st, at, st+1} in the Buffer

6: **if t % critic_update_freq == 0 then**

7: Sample Bt = {si, ai, s[′]i[}]i[M]=0[−][1] from the Replay Buffer

8: _wt+1 = ΓCw_ �wt + _[α][t]_ �iM=0−1�R[π](si) _−_ _ρ¯t +_ _ϕ[π](s[′]i[)][⊺][w][¯][t][ −]_ _[ϕ][π][(][s][i][)][⊺][w][t]�ϕ[π](si)_ _−_ _αtηwt�_

_M_

9: _ρt+1 = ρt +_ _[α][t]_ �Mi=0−1 �R[π](si) − _ρt + ϕ[π](s[′]i[)][⊺][w][¯][t][ −]_ _[ϕ][π][(][s][i][)][⊺][w][¯][t]�_

_M_

10: _wt+1 = wt + βt(wt+1_ _wt+1)_
_−_

11: _ρt+1 = ρt + βt(ρt+1 −_ _ρt+1)_

12: _θt+1 = θt +_ _M[γ][t]_ �Mi=0−1 _[∇][a][Q]diff[w]_ [(][s][i][, a][)][|][a][=][π][(][s]i[)][∇][θ][π][(][s][i][)]

13: buffer = {}

14: **end if**

15: **if st+1 is terminal then**

16: _st = env.reset()_

17: **else**

18: _st = st+1_

19: **end if**

20: end while

**Algorithm 3 Off-policy AR-DPG with Linear FA**
Initialize actor parameter θ and critic parameters w
Initialize actor target parameter θ _θ and_
_→_
Initialize critic target parameters w _w_
_→_
Initialize average reward parameter ρ and
Initialize target average reward parameter ρ _ρ_
_→_
_µ is the behavior policy_
Initialize Replay buffer = {}

1: t = 0, s0 = env.reset()
2: while t ≤ total steps do
3: _at = µ(st) + ϵ {ϵ is the noise}_

4: _st+1 ∼_ _P_ (·|st, at) and rt = R(st, at)

5: Store {st, at, st+1} in the Replay Buffer

6: Sample Bt = {si, ai, s[′]i[}][M]i=0[−][1] from the Replay Buffer

7: _wt+1 = ΓCw_ �wt + _[α][t]_ �Mi=0−1�R[µ](si) − _ρ¯t + ϕ[π](s[′]i[)][⊺][w][¯][t][ −]_ _[ϕ][π][(][s][i][)][⊺][w][t]�ϕ[π](si) −_ _αtηwt�_

_M_

8: _ρt+1 = ρt +_ _[α][t]_ �Mi=0−1 �R[µ](si) − _ρt + ϕ[π](s[′]i[)][⊺][w][¯][t][ −]_ _[ϕ][π][(][s][i][)][⊺][w][¯][t]�_

_M_

9: _wt+1 = wt + βt(wt+1_ _wt+1)_
_−_

10: _ρt+1 = ρt + βt(ρt+1 −_ _ρt+1)_

11: _θt+1 = θt +_ _M[γ][t]_ �Mi=0−1 _[∇][a][Q]diff[w]_ [(][s][i][, a][)][|][a][=][π][(][s]i[)][∇][θ][π][(][s][i][)]

12: **if st+1 is terminal then**

13: _st = env.reset()_

14: **else**

15: _st = st+1_

16: **end if**

17: end while


-----

B.3 HYPERPARAMETERS

|Hyperparameter|Value|
|---|---|
|Buffer Size Total Environment Steps Batch size Evaluation Frequency Training Episode Length Evaluation Episode Length Activation Function Learning rate Actor Learning rate Critic Learning rate Average reward parameter No. of Hidden Layers No. of Nodes in Hidden Layer Update frequency No. of Critic updates No. of Actor updates Polyak averaging constant|1e6 1e6 256 5000 1000 10000 ReLU 3e-4 3e-4 3e-4 2 128 10 steps 10 5 0.995|


-----


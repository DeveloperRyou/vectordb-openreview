# EFFICIENTTTS 2: VARIATIONAL END-TO-END TEXT## TO-SPEECH SYNTHESIS AND VOICE CONVERSION

**Anonymous authors**
Paper under double-blind review

### ABSTRACT

Text-to-speech (TTS) field is recently dominated by one-stage text-to-waveform
models, in which the speech quality is significantly improved compared to twostage models. However, the best-performing open-sourced one-stage model, the
VITS (Kim et al. (2021)), is not fully differentiable and suffers from relatively
high computation costs. To address these issues, we propose EfficientTTS 2
(EFTS2), a fully differentiable end-to-end TTS framework that is highly efficient.
Our method adopts an adversarial training process, with a differentiable aligner
and a hierarchical-VAE-based waveform generator. The differentiable aligner is
built upon the EfficientTTS (Miao et al., 2021). A hybrid attention mechanism and
a variational alignment predictor are incorporated into our network to improve
the expressiveness of the aligner. The use of the hierarchical-VAE-based waveform generator not only alleviates the one-to-many mapping problem in waveform
generation but also allows the model to learn hierarchical and explainable latent
variables that control different aspects of the generated speech. We also extend
EFTS2 to the voice conversion (VC) task and propose EFTS2-VC, an end-to-end
VC model that allows efficient and high-quality conversion. Experimental results
suggest that the two proposed models match their strong counterparts in speech
quality with a faster inference speed and smaller model size.

### 1 INTRODUCTION

Text-to-speech (TTS) task aims at producing human-like synthetic speech signals from text inputs.
In recent years, neural network systems have dominated the TTS field, sparked by the development of autoregressive (AR) models (Wang et al., 2017; Shen et al., 2018; Ping et al., 2018) and
non-autoregressive (NAR) models (Miao et al., 2020; Ren et al., 2019; 2021). The conventional
neural TTS systems cascade two separate models: an acoustic model that transforms the input text
sequences into acoustic features (e.g. mel-spectrogram) (Wang et al., 2017; Ren et al., 2019), followed by a neural vocoder that transforms the acoustic features into audio waveforms (Valin &
Skoglund, 2019; Yamamoto et al., 2020). Although two-stage TTS systems have demonstrated the
capability of producing human-like speech, these systems come with several disadvantages. First
of all, the acoustic model and the neural vocoder cannot be optimized jointly, which often hurts the
quality of the generated speech. Moreover, the separate training pipeline not only complicates the
training and deployment but also makes it difficult for modeling downstream tasks.

Recently, in the TTS field, there is a growing interest in developing one-stage text-to-waveform
models that can be trained without the need for mel-spectrograms (Weiss et al., 2021; Donahue et al.,
2021; Kim et al., 2021). Among all the open-sourced text-to-waveform models, VITS (Kim et al.,
2021) achieves the best model performance and efficiency. However, it still has some drawbacks.
Firstly, the MAS method (Kim et al., 2020) used to learn sequence alignment in VITS is precluded
in the standard back-propagation process, thus affecting training efficiency. Secondly, in order to
generate a time-aligned textual representation, VITS simply repeats each hidden text representation
by its corresponding duration. This repetition operation is non-differentiable thus hurting the quality
of generated speech. Thirdly, VITS utilizes bijective transformations, specifically affine coupling
layers, to compute latent representations. However, for affine coupling layers, only half of the input
data gets updated after each transformation. Therefore, one has to stack multiple affine coupling
layers to generate meaningful latent representations, which increases the model size and further
reduces the model‚Äôs efficiency. A recent work NaturalSpeech (Tan et al., 2022) improves upon


-----

VITS by leveraging a learnable differentiable aligner and a bidirectional prior/posterior module.
However, the training of the learnable differentiable aligner requires a warm-up stage, which is a
pretraining process with the help of external aligners. Although the bidirectional prior/posterior
module of NaturalSpeech can reduce the training and inference mismatch caused by the bijective
flow module, it further increases the model‚Äôs computational cost of training.

A recent work EfficientTTS (EFTS) (Miao et al., 2021) proposed a NAR architecture with differentiable alignment modeling that is optimized jointly with the rest of the model. In EFTS, a family of
text-to-mel-spectrograms models and a text-to-waveform model are developed. However, the performance of the text-to-waveform model is close to but no better than two-stage models. Inspired by
EFTS, we propose an end-to-end text-to-waveform TTS system, the EfficientTTS 2 (EFTS2), that
overcomes the above issues of current one-stage models with competitive model performance and
higher efficiency. The main contributions of this paper are as follows:

We improve upon the alignment framework of EFTS by proposing a hybrid attention mech
_‚Ä¢_
anism and a variational alignment predictor, empowering the model to learn expressive latent time-aligned representation and have controllable diversity in speech rhythms. (Section
2.2.1)

We introduce a 2-layer hierarchical-VAE-based waveform generator that not only produces

_‚Ä¢_
high-quality outputs but also learns hierarchical and explainable latent variables that control
different aspects of the generated speech. (Section 2.2.2)

We develop an end-to-end adversarial TTS model, EFTS2, that is fully differentiable and

_‚Ä¢_
can be trained end-to-end. It matches the baseline VITS in naturalness and offers faster
inference speed and a smaller model footprint. (Section 2.2)

We extend EFTS2 to the voice conversion (VC) task and propose EFTS2-VC, an end-to
_‚Ä¢_
end VC model. The conversion performance of EFTS2-VC is comparable to a state-of-theart model (YourTTS, Casanova et al. (2022)) while obtaining significantly faster inference
speed and much more expressive speaker-independent latent representations. (Section 2.3)

### 2 METHOD

Our goal is to build an ideal TTS model that enables end-to-end training and high-fidelity speech
generation. To achieve this, we consider two major challenges in designing the model:

**(i) Differentiable aligner. The TTS datasets usually consist of thousands of audio files with corre-**
sponding text scripts that are, however, not time aligned with the audios. Many previous TTS works
either use external aligners (Ren et al., 2019; 2021; Chen et al., 2021) or non-differentiable internal
aligners (Kim et al., 2020; 2021; Popov et al., 2021) for alignment modeling, which complicates
the training procedure and reduces the model‚Äôs efficiency. An ideal TTS model requires an internal
differentiable aligner that can be optimized jointly with the rest of the network. Soft attention (Bahdanau et al., 2015) is mostly used in building an internal differentiable aligner. However, computing
soft attention requires autoregressive decoding, which is inefficient for speech generation (Weiss
et al., 2021). Donahue et al. proposes to use Gaussian upsampling and Dynamic Time Warping
(DTW) for alignment learning, while training such a system is inefficient. To the best of our knowledge, EFTS is the only NAR framework that enables both differentiable alignment modeling and
high-quality speech generation. Therefore, we integrate and extend it into the proposed models.

**(ii) Generative modeling framework. The goal of a generative task, such as TTS, is to estimate**
the probability distribution of the training data, which is usually intractable in practice. Multiple
deep generative frameworks have been proposed to address this problem, including Auto-Regressive
models (ARs,Bahdanau et al. (2015)), Normalizing Flows (NFs, Kingma & Dhariwal (2018)), Denoising Diffusion Probabilistic Models (DDPMs, Ho et al. (2020)), Generative Adversarial Networks (GANs, Goodfellow et al. (2014)) and Variational Auto-Encoders (VAEs, Kingma & Welling
(2014)). However, AR models have linear growing generation steps; NFs use bijective transformations and often suffer from large model footprints; DDPMs require many iterations to produce
high-quality samples. In this work, we propose to use GAN structure with a hierarchical-VAE-based
generator, which allows efficient training and high-fidelity generation.


-----

The rest of this section is structured as follows. In Section 2.1, we discuss the background knowledge
of the differentiable aligner of EFTS. In Section 2.2, we present the proposed TTS model EFTS2.
In Section 2.3, we propose EFTS2-VC, a voice conversion model built on EFTS2.

2.1 BACKGROUND: DIFFERENTIAL ALIGNER IN EFFICIENTTTS

In this part, we briefly describe the underlying previous work EFTS, upon which we build our model
that simultaneously learns text-audio alignment and speech generation.

The architecture of EFTS is shown in Figure 1. A text-encoder encodes the text sequence x
_‚ààR[T][ 1]_
into a hidden vector xh ‚ààR[T][1][,D], while a mel-encoder encodes the mel-spectrogram y ‚ààR[T][ 2][,D][mel]
into vector yh ‚ààR[T][2][,D]. During training, the text-mel alignment is computed using a scaled dotproduct attention mechanism (Vaswani et al., 2017), as Eq. (1), which enables parallel computation.

**_Œ± = SoftMax(_** **_[y][h]‚àö[ ¬∑][ x][h]_** ) (1)

_D_

However, yh is unavailable in the inference phase, making computing Œ± intractable. To address
this problem, Miao et al. introduce the idea of alignment vectors, which are used to reconstruct the
attention matrix using a series of non-parametric differentiable transformations:

**_Œ± ‚ààR[T][1][,T][2][ Eq]‚àí‚Üí[.][(3)]_** **_œÄ ‚ààR[T][2][ Eq]‚àí‚Üí[.][(5)]_** **_e ‚ààR[T][1][ Eq]‚àí‚Üí[.][(6)]_** **_Œ±[‚Ä≤]_** _‚ààR[T][1][,T][2]_ (2)

where œÄ ‚ààR[T][2] and e ‚ààR[T][1] are two alignment vectors and Œ±[‚Ä≤] is the reconstructed alignment matrix. A parametric alignment predictor with an output of ÀÜe, the predicted e, is trained
jointly given input xh and therefore allows tractable computation of Œ±[‚Ä≤] in the inference phase
based on ÀÜe. The alignment vector œÄ is defined as the sum of the input index weighted by Œ±:


_œÄj =_


_T1‚àí1_
ÔøΩ

_Œ±i,j ‚àó_ _i_ (3)
_i=0_


where 0 ‚â§ _i ‚â§_ _T1 ‚àí_ 1 and 0 ‚â§ _j ‚â§_ _T2 ‚àí_ 1 are indexes of the
input and output sequence respectively. Here, œÄ can be considered
as the expected location that each output timestep attends to over all
possible input locations. According to the conclusion of EFTS, œÄ
should follow some monotonic constraints including:

_œÄ0 = 0,_ 0 ‚â§ ‚àÜœÄi ‚â§ 1, _œÄT2‚àí1 = T1 ‚àí_ 1 (4)

where ‚àÜœÄj = œÄj ‚àí _œÄj‚àí1. Therefore, additional transformations are_
employed to constraint œÄ to be monotonic (Miao et al. (2021), Eq.
(8-10)).

It is worth noting that, the alignment vector œÄ is a representation
of the alignment matrix with the same length of the output. However, for a sequence-to-sequence task with inconsistent input-output
length like TTS, it is more natural to have an input-level alignment
vector during the inference phase. Thus, a differentiable re-sampling
method is proposed in EFTS. Let e denote the re-sampled alignment
vector, then e is computed as follows:

|ùë• ‚Ñé|ùë• ‚Ñé|
|---|---|
|Text- Encoder||


Figure 1: Overall architecture of EFTS. The arrow
with a broken line represents
the computation in the inference phase only.


_Œ≥i,j =_ ÔøΩT2exp (‚àí1 _‚àíœÉ[‚àí][2](œÄj ‚àí_ _i)[2])_ _,_ _ei =_
_n=0_ [exp (][‚àí][œÉ][‚àí][2][(][œÄ][n][ ‚àí] _[i][)][2][)]_


_T2‚àí1_
ÔøΩ

_Œ≥i,n ‚àó_ _n_ (5)
_n=0_


Here œÉ is a hyper-parameter. In EFTS, e is called the aligned position vector. A Gaussian transformation is used to calculate Œ±[‚Ä≤] from e:

_Œ±i,j[‚Ä≤]_ [=] ÔøΩT1exp (‚àí1 _‚àíœÉ[‚àí][2](ei ‚àí_ _j)[2])_ (6)
_m=0_ [exp (][‚àí][œÉ][‚àí][2][(][e][m][ ‚àí] _[j][)][2][)]_

In the training phase, e is used to construct Œ±[‚Ä≤]. In the inference phase, Œ±[‚Ä≤] is derived from the
predicted alignment vector ÀÜe. As a replacement of the original attention matrix Œ±, the reconstructed
attention matrix Œ±[‚Ä≤] is further used to map the hidden vector xh to time aligned representation xalign
and produce the outputs.


-----

**Differentiable**
**Aligner**


**blocks**

|VAP Decoder WaveNet blocks|Col2|
|---|---|
|||

|Col1|Col2|
|---|---|
|Differentiable Aligner Attention Reconstruction MSE VAP Decoder WaveNet blocks // VAP Encoder WaveNet blocks KL Scaled Loss Dot-Product Attention||
|||
|||


(a) Training Phase


(b) Differentiable Aligner


**Phoneme Sequence**

(c) Inference Phase


Figure 2: Overall architecture of EFTS2‚Äôs generator. LP refers to linear projection. The dotted lines
refer to training objectives.

2.2 EFFICIENTTTS 2: VARIATIONAL END-TO-END TEXT-TO-SPEECH

To better describe our model, we divide the generator, shown in Figure 2, into two main blocks: (i)
the differentiable aligner, which maps the input hidden state xh to time aligned hidden representation xalign; and (ii) the hierarchical-VAE-based waveform generator, which produces the output
waveform y from xalign. More details will be discussed in the rest of this section.

2.2.1 DIFFERENTIABLE ALIGNER

Grounded on EFTS, we construct the differentiable aligner with two major improvements: (i) a
hybrid attention mechanism and (ii) a variational alignment predictor. The structure of the differentiable aligner is shown in Figure 2b.

**Hybrid attention mechanism The performance of the aligner in EFTS heavily depends on the ex-**
pressiveness of the reconstructed attention matrix Œ±[‚Ä≤], which is derived from the alignment vector
**_e. Here, e can be considered as the expected aligned positions for each input token over all possi-_**
ble output frames. However, in the TTS task, one input token normally attends to multiple output
frames. Therefore, it is better to incorporate the boundary positions of each input token when constructing the attention matrix. To this end, we introduce a hybrid attention mechanism that integrates
two attention matrices: the first attention matrix Œ±[(1)] is derived from e as in EFTS (Eq.2-6) and the
second attention matrix Œ±[(2)] is derived from the token boundaries using the following transformations:

Eq.(10)
**_Œ±_** **_œÄ_** (a _, b_ ) **_Œ±[(2)]_** (7)
_‚ààR[T][1][,T][2][ Eq]‚àí‚Üí[.][(3)]_ _‚ààR[T][2][ Eq]‚àí‚Üí[.][(9)]_ _‚ààR[T][1]_ _‚ààR[T][1]_ _‚àí‚Üí_ _‚ààR[T][1][,T][2]_

where (a _, b_ ) are start and end boundaries of the input tokens. We call the process
_‚ààR[T][1]_ _‚ààR[T][1]_
from the attention matrix Œ± to boundary pairs (a, b) the Attention to Boundaries (A2B) transformation and the process from boundary pairs (a, b) to the reconstructed attention matrix Œ±[(2)] the
_Boundaries to Attention (B2A) transformation. Inspired by Eq. (5), the A2B transformation is_


-----

formulated using the following equations:

_Œ≤i,j =_ ÔøΩTn2=0exp (‚àí1 [exp (]‚àíœÉ[‚àí][‚àí][2][œÉ](œÄ[‚àí]j[2] ‚àí[(][œÄ][n]p[ ‚àí]i)[2][p]) _[i][)][2][)]_ _, where pi =_ ÔøΩi0 ‚àí, 0.5, _i0 = 0 < i < T1_ (8)

_ai =_ _TÔøΩ2‚àí1_ _Œ≤i,n_ _n,_ _bi =_ ÔøΩai+1, _i < T1 ‚àí_ 1 (9)

_‚àó_ _T2_ 1, _i = T1_ 1
_n=0_ _‚àí_ _‚àí_

In the meantime, the B2A transformation is designed as follows:

energyi,j = ‚àíœÉ[‚àí][2](|j ‚àí _ai| + |bi ‚àí_ _j| ‚àí_ (bi ‚àí _ai))[2],_ _Œ±i,j[(2)]_ [=] ÔøΩT1exp (energy‚àí1 _i,j)_ (10)

_m=0_ [exp (energy]m,j[)]

As can be seen, for the ith input token with its corresponding boundaries (ai, bi), {energyi,j}
reaches the maximum value 0 only if the output position j falls into its boundaries, meaning
_ai ‚â§_ _j ‚â§_ _bi. For an output position outside of the boundaries, the further it is away from the_
boundaries, the lower value of {energyi,j} it gets, resulting in less attention weight.

Note that the proposed B2A approach works for all those TTS models with explicit token durations,
and is potentially better than the conventional approaches: (i) compared to the repetition operation
(Ren et al., 2019; 2021; Kim et al., 2020; 2021), the proposed approach is differentiable and enables
batch computation; (ii) compared to the popular Gaussian upsampling (Donahue et al., 2021; Shen
et al., 2020) that considers only the centralized position, the proposed approach employs boundary
positions, which is more informative; (iii) compared to the learnable upsampling (Elias et al., 2021;
Tan et al., 2022), the proposed approach is monotonic and much easier to train.

In preliminary experiments, we found out that the model performance is greatly influenced by choice
of œÉ in Eq. (6) and Eq. (10). In order to obtain better model performance, we use learnable œÉ in this
work. We further map the hidden representation xh to a time-aligned hidden representation xalign
using a approach similar to the multi-head attention mechanism in Vaswani et al. (2017):

head[(][i][)] = Œ±[(][i][)] _¬∑ (xhW_ [(][i][)]), **_xalign = Concat(head[(1)], head[(2)])W[o]_** (11)

where {W [(][i][)]}, W _[o]_ are learnable linear transformations. The xalign is then fed into the hierarchicalVAE-based waveform generator as input.

**Variational alignment predictor NAR TTS models generate the entire output speech in parallel,**
thus alignment information is required in advance. To address this problem, many previous NAR
models train a duration predictor to predict the duration of each input token (Ren et al., 2019; Kim
et al., 2021). Similarly, EFTS employs an aligned position predictor to predict the aligned position
vector e. As opposed to a vanilla deterministic alignment predictor (DAP), in this work, we use a
variational alignment predictor (VAP) to predict the alignment vector e and the boundary positions
**_a and b. The main motivation behind this is to consider the alignment prediction problem as a_**
generative problem since one text input can be expressed with different rhythms. Specifically, the
VAP encoder receives the relative distances e **_a and b_** **_a, and outputs a latent posterior distribution_**
_‚àí_ _‚àí_
_qœÜ(zA|e_ _‚àí_ **_a, b_** _‚àí_ **_a, xh) conditioned on xh, while the VAP decoder estimates the output distribution_**
by inputting zA, and conditioned on xh. The prior distribution is a standard Gaussian distribution.
For simplicity, both the encoder and the decoder of VAP are parameterized with non-causal WaveNet
residual blocks. The training objective of the VAP is computed as:

_Lalign =Œª1(‚à•d[(1)]Œ∏_ [(][z][A][)][ ‚àí] [log(][e][ ‚àí] **_[a][ +][ œµ][)][‚à•][2][ +][ ‚à•][d]Œ∏[(2)][(][z][A][)][ ‚àí]_** [log(][b][ ‚àí] **_[a][ +][ œµ][)][‚à•][2][)+]_**

_Œª2 DKL(zA; N_ (¬µ[(]œÜ[A][)][(][e][ ‚àí] **_[b][,][ b][ ‚àí]_** **_[a][,][ x][h][)][, œÉ]œÜ[(][A][)](e ‚àí_** **_b, b ‚àí_** **_a, xh)) || N_** (zA; 0, I)) (12)

where, d[(1)]Œ∏ and d[(2)]Œ∏ are outputs of the VAP decoder, ¬µ[(]œÜ[A][)] and œÉœÜ[(][A][)] are outputs of the VAP encoder,
and œµ is a small value to avoid numerical instabilities. The first term in Eq. (12) is the reconstruction
loss that computes the log-scale mean square error (MSE) between the predicted relative distances
and target relative distances. The second term is the KL divergence between the posterior and prior
distributions. In the inference phase, the alignment vector ÀÜe and boundary positions ÀÜa and **[ÀÜ]b are**
computed as:


ÔøΩ
0 _i = 0_
ÀÜbi‚àí1 _i > 0_ _[,]_ _eÀÜi = exp((d[(1)]Œ∏_ [(][z][A][))][i][)] _[‚àí]_ _[œµ]_ [+ÀÜ][a][i][ (13)]


ÀÜbi =


_i_
ÔøΩ

(exp((d[(2)]Œ∏ [(][z][A][))][m][)] _[‚àí]_ _[œµ][)][,]_ _aÀÜi =_
_m=0_


-----

where zA is sampled from the standard Gaussian distribution. A stop gradient operation is added
to the inputs of the VAP encoder, which helps the model to learn a more accurate alignment in the
training phase.

2.2.2 HIERARCHICAL-VAE-BASED WAVEFORM GENERATOR

Producing high-quality waveforms from linguistic features (e.g. texts, phonemes, or hidden linguistic representation xalign) is known as a particularly challenging problem. This is mainly because linguistic features do not contain enough necessary information (e.g. pitch and energies) for
waveform generation. A primary idea is to use a VAE-based generator that learns the waveform
generation from a latent variable z. This z is sampled from an informative posterior distribution
_qœÜ(z|y) parameterized by a network with acoustic features as input. A prior estimator pŒ∏(z|xalign)_
with xalign as input is also trained jointly. Training such a system is to minimize the reconstruction
error between the real and predicted waveform and the KL divergence between the prior and the
posterior distribution. However, the prior distribution contains no acoustic information while the
learned posterior must be informative w.r.t. the acoustic information. The information gap makes it
hard to minimize the KL divergence between the prior and the posterior distribution. To tackle this
problem, we introduce a 2-layer hierarchical-VAE structure that enables informative prior formulation. The hierarchical-VAE-based waveform generator is composed of the following blocks: (i) a
posterior network which takes the linear spectrograms ylin as input and outputs two latent Gaussian
posterior qœÜ(z1|ylin), qœÜ(z2|ylin); (ii) a hierarchical prior network which consists of two stochastic
layers: the first layer receives xalign and outputs a latent Gaussian prior pŒ∏(z1|xalign); the second layer takes a latent variable z1 and formulates another latent Gaussian prior pŒ∏(z2|z1), where
**_z1 is sampled from posterior distribution qœÜ(z1|ylin) in training phase and from prior distribution_**
_pŒ∏(z1|xalign) in inference phase; (iii) a decoder which produces the waveform from the latent vari-_
able z2 where z2 is sampled from posterior distribution qœÜ(z2|ylin) in training phase and from prior
distribution pŒ∏(z2|z1) in inference phase.

Therefore, the overall prior and posterior distributions are formulated as:

_pŒ∏(z|xalign) = pŒ∏(z1|xalign)pŒ∏(z2|z1),_ _qœÜ(z|y) = qœÜ(z1|ylin)qœÜ(z2|ylin)_ (14)

The training objective is :

_Lwav = ‚àí_ EqœÜ(z|y)[log pŒ∏(y|z)] + DKL(qœÜ(z|y) || pŒ∏(z|xalign))

=Lrecons + DKL(qœÜ(z1|ylin) || pŒ∏(z1|xalign)) + DKL(qœÜ(z2|ylin) || pŒ∏(z2|z1))

=Œª3‚à•ymel ‚àí **_yÀÜmel‚à•1 + Œª4 (DKL(N_** (z1; ¬µ[(1)]œÜ [(][y][lin][)][, œÉ]œÜ[(1)][(][y][lin][))][ ||]

_N_ (z1; ¬µ[(1)]Œ∏ [(][x][align][)][, œÉ]Œ∏[(1)][(][x][align][))) +][ D][KL][(][N] [(][z][2][;][ ¬µ]œÜ[(2)][(][y][lin][)][, œÉ]œÜ[(2)][(][y][lin][))][ ||]

_N_ (z2; ¬µ[(2)]Œ∏ [(][z][1][)][, œÉ]Œ∏[(2)][(][z][1][))))] (15)

Here, the reconstruction loss is the ‚Ñì1 loss between the target mel-spectrogram ymel and the predicted mel-spectrogram ÀÜymel which is derived from the generated waveform ÀÜy. In this work, the two
prior estimators and the posterior estimator are all parameterized by stacks of non-causal WaveNet
residual blocks with the estimated means and variations, while the decoder is inspired by the generator of HiFi-GAN (Kong et al., 2020). Similar to EFTS and VITS, the decoder part is trained on
sliced latent variables with corresponding sliced audio segments for memory efficiency. There are
some previous TTS models (Kim et al., 2021; Tan et al., 2022) that also incorporate the VAE framework in end-to-end waveform generation. EFTS2 differs from them in two aspects: (i) EFTS2 uses
2-layer hierarchical VAE while previous works use single-layer VAE; and (ii) in previous work, the
KL divergence between the prior and posterior distributions is estimated between a latent variable
(which is just a sample from posterior distribution) and a multivariate Gaussian distribution, while
EFTS2 computes the KL divergence between two multivariate Gaussian distributions which allows
for a more efficient training.

2.2.3 THE OVERALL MODEL ARCHITECTURE

The overall model architecture of EFTS2 is based on GAN, which consists of a generator and multiple discriminators. We follow Kong et al. (2020) in implementing the multiple discriminators whose
performance is experimentally confirmed by many previous works (You et al., 2021; Kim et al.,


-----

2021). The feature matching loss Lfm is also employed for training stability. In the training phase,
a phoneme sequence x is passed through a phoneme encoder to produce the latent representation
**_xh, while the corresponding linear spectrogram ylin is passed through a spectrogram encoder to_**
produce the latent representation yh and two latent Gaussian posteriors. [1] Same as EFTS and VITS,
the phoneme encoder is parameterized by a stack of feed-forward Transformer blocks. The proposed
differentiable aligner receives the latent representation xh and yh and outputs the time-aligned latent
representation xalign. Then xalign is further fed to the hierarchical-VAE-based waveform generator
to produce the output ÀÜy. The overall training objective of the proposed generator G is:

_Ltotal = Lwav + Lalign + Ladv(G) + Lfm(G)_ (16)

2.3 EFTS2-VC: END-TO-END VOICE CONVERSION
Voice conversion (VC) is a task that modifies a source speech signal with the target speaker‚Äôs timbre while keeping the linguistic contents of the source speech unchanged. The proposed voice
conversion model, EFTS2-VC (shown in Appendix A), is built upon EFTS2 with several module
differences:

The alignment predictor is excluded in EFTS2-VC since there is no need to explicitly tell

_‚Ä¢_
the text-spectrogram alignment in the inference phase.

_‚Ä¢ Instead of using e or the token boundaries, the reconstructed attention matrix Œ±[‚Ä≤]_ is derived
from œÄ (Eq. (17)). This not only simplifies the computation pipeline but also allows
the network to obtain a more accurate text-spectrogram alignment. Similar to the TTS
model, EFTS2-VC uses multiple reconstructed attentions by employing multiple learnable
_{œÉœÄ[(][k][)][|][k][ = 1][, ..., H][}][ .]_

_Œ±i,j[‚Ä≤]_ [=] ÔøΩmT1=0exp (‚àí1 [exp (]‚àíœÉ[‚àí]œÄ[‚àí][2][œÉ][(]œÄ[‚àí][œÄ][2][j][(][‚àí][œÄ]j[i][‚àí][)][2][)][m][)][2][)] (17)

The speaker embedding of the source waveform, which is extracted from a trained speaker

_‚Ä¢_
encoder, is introduced as conditions to the spectrogram encoder, the second prior network,
and the HiFi-GAN generator.
Again, we consider the hierarchical-VAE-based framework discussed in Section 2.2.2. During training, the prior distribution pŒ∏(z1|xalign) is estimated by a stack of WaveNet blocks with xalign as
the only input. Since xalign is the time-aligned textual representation without any information about
the speaker identity, we can easily draw the conclusion that the prior distribution pŒ∏(z1|xalign) contains only the textual information and does not contain any information about the speaker identity.
The conclusion can be further extended to the posterior distribution pœÜ(z1|ylin) since the network
is trained by minimizing the KL divergence between the prior distribution and posterior distribution. Therefore, the spectrogram encoder works as a speaker disentanglement network that strips the
speaker identity while preserving the textual (or content) information. Then the second prior network and the variational decoder reconstruct the speech from the content information and the input
speaker embeddings. During inference, the disentanglement network and the reconstruction network are conditioned on different speaker embeddings. Specifically, the disentanglement network
receives the spectrogram and the speaker embedding from a source speaker, and outputs a latent distribution pœÜ(z1|ylin). Meanwhile, the reconstruction network produces the output waveform from
the latent variable z1 and the speaker embedding from a target speaker. With these designs, EFTS2VC performs an ideal conversion that preserves the content information of the source spectrogram
while producing the output speech that matches the speaker characteristics of a target speaker.

### 3 EXPERIMENTS

Due to the paper page limitation, we put the dataset settings, model configurations, and training details in Appendix B. We present the experimental results in the following subsections and Appendix
C. [2]

1Ideally, an end-to-end TTS system should operate on unnormalised text. We use external tools to convert
the unnormalised texts to phonemes in this work. We will explore some data-driven approaches in the future to
address this limitation.
[2Audio samples can be found in https://anonymous6666audio.github.io/efts2/](https://anonymous6666audio.github.io/efts2/)


-----

Table 1: MOS results from baseline models, the ablation studies and EFTS2 on LJSpeech.

**Model** **MOS**

Ground Truth 4.59 ¬± 0.12

EFTS-CNN + HIFI-GAN 4.17 ¬± 0.08
VITS 4.48 ¬± 0.14

EFTS2 (1-layer VAE) 4.12 ¬± 0.12
EFTS2 (Single Attention) 4.31 ¬± 0.11
EFTS2 (DAP) 4.44 ¬± 0.08

EFTS2 **4.48 ¬± 0.11**


Table 2: MOS and sim-MOS for voice conversion experiments on VCTK dataset.

**Speaker** **Model** **MOS** **sim-MOS**

Seen EFTS2-VC **4.22 ¬± 0.08** 4.14 ¬± 0.14
YourTTS 4.18 ¬± 0.08 4.14 ¬± 0.12

Unseen EFTS2-VC **4.08 ¬± 0.13** 3.95 ¬± 0.08
YourTTS 4.06 ¬± 0.06 **3.98 ¬± 0.13**


Table 3: Comparison of the number of parameters and inference speed. All models are benchmarked
using the same hardware.

**Total Params** **Inference Params** **Inference Speed**
**Model** **Real-time**
(M) (M) (kHz)

EFTS-CNN + HIFI-GAN 58.34 52.18 1768.32 _√ó80.20_
VITS 34.64 27.74 1508.31 _√ó68.04_
EFTS2 **32.38** **24.35** **2247.52** _√ó101.92_

3.1 TTS SPEECH QUALITY EVALUATION

In this subsection, we compare the quality of audio samples generated by EFTS2 and the baseline
models. The baseline models are the best-performing publicly-available model VITS (Kim et al.,
2021) and the best-performing model in the EFTS family, EFTS-CNN + HIFI-GAN, with our own
implementation. The quality of audio samples is measured by the 5-scale mean opinion score (MOS)
evaluation. Ablation studies are also conducted to validate our design choices. The MOS results
on LJ-Speech dataset (Ito, 2017) are shown in Table 1. EFTS2 significantly outperforms 2-stage
EFTS-CNN. In addition, we also observed that there is no significant difference between EFTS2
and VITS. Both models achieve comparable scores to ground truth audios, which means that the
speech quality of EFTS2 and VITS is very close to natural speech. Ablation studies confirm the
importance of our design choices. Removing either the hierarchical-VAE structure (1-layer VAE) or
the hybrid attention mechanism (Single Attention) leads to a significant MOS decrease. Although
the deterministic alignment predictor (DAP) has a similar MOS score to the variational alignment
predictor, it lacks diversity which is very important for speech generation.

3.2 MODEL EFFICIENCY

The inductive biases of the proposed hierarchical-VAE-based generator make the overall model
smaller and significantly faster than the baseline models. The model size and inference speed of
EFTS2 along with the baseline models are presented in Table 3. Since EFTS2‚Äôs generator employs a
significantly smaller number of convolution blocks in comparison with VITS, the inference speed is
greatly improved. Specifically, EFTS2 is capable of running at a frequency of 2247.52 kHz, which
is 1.5 faster than VITS.
_√ó_

3.3 VOICE CONVERSION

The conversion performance of EFTS2-VC is evaluated on the VCTK dataset (Yamagishi et al.,
2019) with a comparison to the baseline model YourTTS (Casanova et al., 2022). Table 2 presents
the MOS scores and similarity scores. EFTS2-VC achieves slightly better MOS scores and similarity scores for seen speakers and comparable similarity scores for unseen speakers. Note that the
conversion of YourTTS requires running the flow module bidirectionally, which results in a slow
conversion speed. On the other hand, EFTS2-VC is significantly faster. It runs 2.15 faster than
_√ó_
YourTTS on Tesla V100 GPU.


-----

Figure 3: F 0 contours obtained from the test samples generated by EFTS2 with different t1.

3.4 ANALYSIS OF THE LATENT HIERARCHY

One question from Section 2.2.2 is whether the hierarchical architecture of the proposed generator
empowers the model to have controllable diversity in hierarchical and explainable latent variables.
Analysis of two latent variables z1 and z2 confirms this statement. Figure 3 shows the scatter plots
of the F 0 contours extracted from audios generated with 3 different sets of z1 and z2. All the audios
are synthesized using the same phoneme sequence and the same alignment, which means the timealigned text representation xalign are precisely the same for all waveforms. The t1 and t2 are two
scaling factors applied on the variances of the latent distributions pŒ∏(z1) and pŒ∏(z2) respectively. For
each pair of t1 and t2, 5 pairs of z1 and z2 are sampled and then used to synthesize waveforms. As
shown in Figure 3, increasing t1 considerably increases the variation of F 0, whereas large t2 barely
produces any variation on F 0 when t1 = 0. This means essential acoustic features such as F 0 are
mostly fixed after z1 is sampled. In other words, pŒ∏(z1) is a linguistic latent representation offering
variations on the spectrogram domain acoustic information, while pŒ∏(z2) contains the spectrogram
domain acoustic information and offers variations on time domain information. This is important
because though we did not explicitly give model any constrain, it still learns the hierarchical and
explainable latent representations with controllable diversity.

### 4 CONCLUSION AND DISCUSSION

We presented EfficientTTS 2 (EFTS2), a novel end-to-end TTS model that adopts an adversarial
training process, with a generator composed of a differentiable aligner and a hierarchical-VAEbased speech generator. Compare to baseline models, EFTS2 is fully differentiable and enjoys a
smaller model size and higher model efficiency, while still allowing high-fidelity speech generation
with controllable diversity. Moreover, we extend EFTS2 to the VC task and propose a VC model,
EFTS2-VC, that is capable of efficient and high-quality end-to-end voice conversion.

The primary goal of this work is to build a competitive TTS model that allows for end-to-end highquality speech generation. In the meantime, the proposed design choices can easily be incorporated
into other TTS frameworks. Firstly, the proposed B2A approach could potentially be a handier
replacement for conventional upsampling techniques in nearly all NAR TTS models, given that it
is differentiable, informative, and computationally cheap. Secondly, the differentiable aligner may
be a superior alternative for any external aligner or non-differentiable aligner, as it improves the
uniformity of the model and makes the training process end-to-end. Thirdly, the 2-layer hierarchicalVAE-based waveform generator can potentially outperform the popular flow-VAE-based counterpart
(Kim et al., 2021; Tan et al., 2022) since it is more efficient and offers more flexibility in network
design. Lastly and most importantly, the entire architecture of EFTS2 could serve as a practical
solution to sequence-to-sequence tasks that have the nature of monotonic alignments. We leave
these assumptions to future work while providing our implementations as a research basis for further
exploration.


-----

### 5 REPRODUCIBILITY STATEMENT

To encourage reproducibility, we attach the code of EFTS2 and EFTS2-VC in the supplemental materials. Please refer to the README.md in the supplemental materials for the training and inference
details of our code.

### REFERENCES

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly
Learning to Align and Translate. In International Conference on Learning Representations, 2015.

Edresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren G¬®olge, and
Moacir A Ponti. YourTTS: Towards zero-shot multi-speaker TTS and zero-shot voice conversion
for everyone. In International Conference on Machine Learning, 2022.

Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, Najim Dehak, and William
Chan. WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis. In Interspeech, 2021.

Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition. In
_Interspeech, 2018._

Jeff Donahue, Sander Dieleman, Mikolaj Binkowski, Erich Elsen, and Karen Simonyan. End-to-End
Adversarial Text-to-Speech. In International Conference on Learning Representations, 2021.

Isaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang, Ye Jia, RJ Skerry-Ryan, and Yonghui Wu. Parallel
Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling. In
_Interspeech, 2021._

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Infor_mation Processing Systems, 2014._

Hee Soo Heo, Bong-Jin Lee, Jaesung Huh, and Joon Son Chung. Clova baseline system for the
voxceleb speaker recognition challenge 2020. In arXiv, 2020.

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Advances
_in Neural Information Processing Systems, 2020._

[K Ito. The lj speech dataset. 2017. URL https://keithito.com/LJ-Speech-Dataset/.](https://keithito.com/LJ-Speech-Dataset/)

Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-TTS: A Generative Flow for
Text-to-Speech via Monotonic Alignment Search. In Advances in Neural Information Processing
_Systems, 2020._

Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional Variational Autoencoder with Adversarial
Learning for End-to-End Text-to-Speech. In International Conference on Machine Learning,
2021.

D. P. Kingma and P Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. In Ad_vances in Neural Information Processing Systems, 2018._

Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Confer_ence on Learning Representations, 2014._

Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative Adversarial Networks for
Efficient and High Fidelity Speech Synthesis . In Advances in Neural Information Processing
_Systems, 2020._

Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In International Con_ference on Learning Representations, 2019._

C. Miao, S. Liang, M. Chen, J. Ma, S. Wang, and J. Xiao. Flow-TTS: A Non-Autoregressive Network for Text to Speech Based On Flow. In ICASSP 2020-2020 IEEE International Conference
_on Acoustics, Speech and Signal Processing (ICASSP), 2020._


-----

Chenfeng Miao, Liang Shuang, Zhengchen Liu, Chen Minchuan, Jun Ma, Shaojun Wang, and Jing
Xiao. EfficientTTS: An Efficient and High-Quality Text-to-Speech Architecture. In International
_Conference on Machine Learning, 2021._

W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang, J. Raiman, and J. Miller. Deep
Voice 3: 2000-Speaker Neural Text-to-Speech. In International Conference on Learning Repre_sentations, 2018._

Wei Ping, Kainan Peng, and Jitong Chen. Clarinet: Parallel wave generation in end-to-end text-tospeech. In International Conference on Learning Representations, 2019.

Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-TTS:
A Diffusion Probabilistic Model for Text-to-Speech. In International Conference on Machine
_Learning, 2021._

Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu. FastSpeech: Fast, Robust and
Controllable Text to Speech. In Advances in Neural Information Processing Systems, 2019.

Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. FastSpeech 2:
Fast and High-Quality End-to-End Text to Speech . In International Conference on Learning
_Representations, 2021._

J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, and
R. Skerry Ryan. Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. In ICASSP 2018-2018 IEEE International Conference on Acoustics, Speech and Signal
_Processing (ICASSP), 2018._

Jonathan Shen, Ye Jia, Mike Chrzanowski, Yu Zhang, Isaac Elias, Heiga Zen, and Yonghui Wu.
Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised
Duration Modeling. In arXiv, 2020.

Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng,
Yuanhao Yi, Lei He, Frank Soong, Tao Qin, Sheng Zhao, and Tie-Yan Liu. NaturalSpeech: Endto-End Text to Speech Synthesis with Human-Level Quality. In arXiv, 2022.

Jean-Marc Valin and Jan Skoglund. LPCNet: Improving neural speech synthesis through linear prediction. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
_Processing (ICASSP). IEEE, 2019._

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor_mation Processing Systems, 2017._

Y. Wang, R. Skerry-Ryan, D. Stanton, R. J. Weiss Y. Wu, N. Jaitly, and Z. Yang. Tacotron: Towards
End-to-End Speech Synthesis. In Interspeech, 2017.

Ron J Weiss, RJ Skerry-Ryan, Eric Battenberg, Soroosh Mariooryad, and Diederik P Kingma. WaveTacotron: Spectrogram-Free End-to-End Text-to-Speech Synthesis. In ICASSP 2021-2021 IEEE
_International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021._

Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. CSTR VCTK Corpus: English
Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92). University of Edinburgh.
The Centre for Speech Technology Research (CSTR), 2019.

Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In
_ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing_
_(ICASSP). IEEE, 2020._

Jaeseong You, Dalhyun Kim, Gyuhyeon Nam, Geumbyeol Hwang, and Gyeongsu Chae. GAN
Vocoder: Multi-Resolution Discriminator Is All You Need. In Interspeech, 2021.


-----

**Hierarchical**
**Prior**


**Decoder**


**Aligner**

|Hierarchical Prior LP Prior Network 2 WaveNet blocks LP Prior Network 1 WaveNet blocks Differentiable Aligner|Decoder Raw Wave HiFi-GAN Generator|
|---|---|


**Phoneme**

**Sequence**


**Wave**


**Wave**


**Wave**


(a) Training Phase


(b) Differentiable Aligner


(c) Inference Phase


Figure 4: Overall model architecture of EFTS2-VC. LP refers to linear projection. The dotted lines
refer to the training objectives.

### A OVERALL ARCHITECTURE OF EFTS2-VC‚ÄôS GENERATOR

The overall architecture of EFTS2-VC‚Äôs generator is shown in Figure 4.

### B EXPERIMENTAL SETUP

**Datasets Two public datasets are used in our experiments, the LJ Speech dataset (Ito, 2017) and**
the VCTK dataset (Yamagishi et al., 2019). The LJ Speech dataset is an English speech corpus
consisting of 13,100 audio clips of a single female speaker. Each audio file is a single-channel 16-bit
PCM with a sampling rate of 22050 Hz. The VCTK dataset is a multi-speaker English speech corpus
that contains 44 hours of audio clips of 108 native speakers with various accents. The original audio
format is 16-bit PCM with a sample rate of 44kHz. In our experiments, all audio clips are converted
into 16-bit and down-sampled to 22050 Hz. Both datasets are randomly split into a training set, a
validation set, and a test set.

**Preprocessing The linear spectrograms of the original audio are used as the input of the spectrogram**
encoder. The FFT size, hop size, and window size used in Short-time Fourier transform (STFT) to
obtain linear spectrograms are set to 1024, 256, and 1024 respectively. Before training, the text
sequences are converted to phoneme sequences using open-sourced software phonemizer [3].

**Configurations The phoneme encoder of EFTS2 is a stack of 6 Feed-Forward Transformer (FFT)**
blocks, where each FFT block consists of a multi-head attention layer with 2 attention heads and a
convolutional feed-forward layer with a hidden size of 192. The HiFi-GAN generator consists of 4
residual convolution blocks, where each block has a transpose convolution layer and 3 1D convolution layers following Kong et al. (2020). The rest of EFTS2 is composed of stacks of non-causal
WaveNet residual blocks. Specifically, the number of convolution layers in the VAP encoder, the
VAP decoder, the first prior network, the second prior network, and the spectrogram encoder are 3,
3, 3, 5, and 16 respectively. The kernel size is 5 and the dilation rate is 1 for all the WaveNet layers. EFTS2-VC shares similar model configurations with EFTS2 except that the variational aligned
predictor is excluded from EFTS2-VC. The scaling factor tA, t1, t2 are set to 0.7, 0.8, 0.3 respectively. Two linear projections that produce prior distributions (LP blocks in Hierarchical Prior block

[3https://github.com/bootphon/phonemizer](https://github.com/bootphon/phonemizer)


-----

(a) Œ±[(1)] (b) Œ±[(2)]

Figure 5: Visualization of the attention matrices of EFTS2

in Figure 2a) and two linear projections that produce posterior distributions (green LP blocks in
Posterior block in Figure 2a) are initialized with zeros, such that both the prior and the posterior
are initially standard Gaussian distributions, with a KL divergence of zero. The deterministic alignment predictor (DAP), which takes xh as input and outputs the alignment vectors (eg. ÀÜa, **_b[ÀÜ], ÀÜe), is_**
parameterized by 2 convolution layers and a linear mapping. Each convolution layer is followed by
a layer normalization and a leaky ReLU activiation. The trained speaker encoder of EFTS2-VC is a
speaker recognition model (Heo et al., 2020) trained on the voxceleb2 (Chung et al., 2018) dataset.
The pre-trained model is publicly available (Heo et al., 2020). The baseline model of EFTS2-VC is
a pre-trained model from YourTTS (Casanova et al., 2022) trained on the VCTK dataset. For a fair
comparison, we down-sampled the generated audios of EFTS2-VC to 16 kHz to match the sample
rate of YourTTS‚Äô generated audios during the evaluation process. The hyper-parameters of EFTS2
and EFTS2-VC are listed in Table 4.

**Training Both EFTS2 and EFTS-VC are trained on 4 Tesla V100 GPUs with 16G memory. The**
batch size on each GPU is set to 32. The AdamW optimizer (Loshchilov & Hutter, 2019) with
_Œ≤1 = 0.8, Œ≤2 = 0.99 is used to train the models. The initial learning rate is set to 2 ‚àó_ 10[‚àí][4] and
decays at every training epoch with a decay rate of 0.998. Both models converge at 500k[th] step.

**MOS Evaluation We conducted the Mean Opinion Score (MOS) tests to evaluate the model per-**
formance of EFTS2 and EFTS2-VC. 15 raters were asked to make naturalness judgments about the
randomly selected audio samples from the test set, and then gave their rating scores on a 5-point
Likert scale score (1 = Bad; 2 = Poor; 3 = Fair; 4 = Good; 5 = Excellent) with rating increments of
0.5.

### C ANALYSIS OF EFTS2

C.1 VISUALIZATION OF THE ATTENTION MATRICES

The attention matrices of EFTS2 are visualized in Figure 5. As can be seen, both Œ±[(1)] and Œ±[(2)]

are monotonic. While Œ±[(2)] learns clean boundaries for the input tokens, Œ±[(1)] learns a more smooth
alignment.

C.2 TRAINING LOSS COMPARISON

Comparisons of the model performance between a TTS model using a 2-layer hierarchical-VAEbased generator and a TTS model using a 1-layer VAE-based generator are visualized in Figure 6. As
shown in the figure, with an additional variational structure, both the KL loss and mel-spectrogram
reconstruction loss decrease significantly.


-----

Table 4: Hyper-parameters of EFTS2 and EFTS2-VC

**Modules** **EFTS2** **EFTS2-VC**

EmbeddingDimension = 192,
FFTBlocks = 6,
HiddenDimension = 512,

Phoneme Encoder

AttentionHeads = 2,
ConvFilterSize = 768,
ConvKernelSize = 3

WaveNet Layers = 16,
kernelSize = 5,

Spectrogram Encoder

Dilation = 1,
FilterSize = 192,

WaveNet Layers = 3,
kernelSize = 3,

VAP Encoder          
Dilation = 1,
FilterSize = 192,

WaveNet Layers = 3,
kernelSize = 3,

VAP Decoder          
Dilation = 1,
FilterSize = 192,

AttentionHeads = 2,
Attention reconstruction
HiddenDimension = 384,

WaveNet Layers = 3,
kernelSize = 5,

Prior Network 1

Dilation = 1,
FilterSize = 192,

WaveNet Layers = 5,
kernelSize = 5,

Prior Network 2

Dilation = 1,
FilterSize = 192,

ConvBlocks = 4,
UpsamplingRate = [8,8,2,2],
UpsamplingKernelSizes = [16,16,4,4]

HiFi-GAN Generator UpsampleInitialChannel = 512

ConvLayers = 3,
ConvKernelSize = [3,7,11],
ConvDilation = [1,3,5]

### D COMPARISON OF DIFFERENT ALIGNERS

We conducted a 5-point side-by-side Comparative Mean Opinion Score (CMOS) evaluation to verify
the effectiveness of the proposed aligner. We consider the following approaches for comparison:

_‚Ä¢ Non-Differentiable approaches._ **ND-External-Repetition: external aligner with re-**
peated upsampling (Ren et al., 2019; 2021); and ND-MAS : internal aligner using MAS
(Kim et al., 2021).

_‚Ä¢ Differentiable approaches. D-External-Gaussian-Central: external aligner using the_
upsamlping approach proposed by EATS (Donahue et al., 2021; Shen et al., 2020); D**External-Gaussian-Boundaries: external aligner using proposed B2A approach following**
Eq. (10); D-Internal-Learnable: internal aligner with a single learnable attention (Elias
et al., 2021). The token boundaries are derived from œÄ following Eq. (9). D-Internal**Gaussian-e: internal aligner. The attention is derived from alignment vector e (Miao et al.,**
2021); D-Internal-Gaussian-Boundaries: internal aligner. The attention is derived from
token boundaries following Eq. (10); D-Hybrid: The proposed hybrid attention.

|Modules|EFTS2|EFTS2-VC|
|---|---|---|
|Phoneme Encoder|EmbeddingDimension = 192, FFTBlocks = 6, HiddenDimension = 512, AttentionHeads = 2, ConvFilterSize = 768, ConvKernelSize = 3||
|Spectrogram Encoder|WaveNet Layers = 16, kernelSize = 5, Dilation = 1, FilterSize = 192,||
|VAP Encoder|WaveNet Layers = 3, kernelSize = 3, Dilation = 1, FilterSize = 192,|-|
|VAP Decoder|WaveNet Layers = 3, kernelSize = 3, Dilation = 1, FilterSize = 192,|-|
|Attention reconstruction|AttentionHeads = 2, HiddenDimension = 384,||
|Prior Network 1|WaveNet Layers = 3, kernelSize = 5, Dilation = 1, FilterSize = 192,||
|Prior Network 2|WaveNet Layers = 5, kernelSize = 5, Dilation = 1, FilterSize = 192,||
|HiFi-GAN Generator|ConvBlocks = 4, UpsamplingRate = [8,8,2,2], UpsamplingKernelSizes = [16,16,4,4] UpsampleInitialChannel = 512 ConvLayers = 3, ConvKernelSize = [3,7,11], ConvDilation = [1,3,5]||


-----

(a) KL loss curves of EFTS2 (b) Mel reconstruction loss curves of EFTS2

Figure 6: Loss curves of the ablations

Table 5: Comparson of different aligners.

**Model** **CMOS**

D-Hybrid (ours) 0

ND-External-Repetition (Ren et al., 2021) -0.25
ND-Internal-MAS (Kim et al., 2021) -0.10
D-External-Gaussian-Central (Donahue et al., 2021) -0.23
D-External-Gaussian-Boundaries (ours) -0.18
D-Internal-Learnable (Elias et al., 2021) does not converge
D-Internal-Gaussian-e (Miao et al., 2021) -0.18
D-Internal-Gaussian-Boundaries (ours) -0.02

All these models are built up using the 2-layer-hierarchical-VAE based waveform generator and
deterministic alignment predictor (DAP). For both the two non-differentiable models, the convolutional prior network 1 is excluded. The first variational prior of the two non-differentiable models
is formulated by firstly mapping the text hidden representation xh to an input level Gaussian prior
distribution, and then expanding the Gaussian prior through repetition. The phoneme durations are
extracted using MAS (Kim et al., 2021) for all those models using external aligners.

The CMOS results are presented in in Table 5, and we have the following observations: 1) the model
using learnable upsampling D-Internal-Learnable, does not converge at all while other models are
able to produce reasonable results; 2) the models with internal aligners outperform those using
external aligners even if facilitated with the same upsampling approach, which demonstrates the importance of jointly learning alignment and speech generation. The proposed approach D-Internal**Gaussian-Boundaries, that uses Gaussian attention derived from token boundaries, significantly**
outperforms other upsampling approaches. The best model D-Hybrid, that combines D-Internal**Gaussian-e and D-Internal-Gaussian-Boundaries, further boosts the model performance. 3) D-**
**Hybrid achieves a performance gain of 0.1 over ND-Internal-MAS, verifying the significance of**
the proposed differentiable aligner over MAS. We also notice that ND-Internal-MAS performs
worse than VITS because VITS achieves comparable speech quality of our best-performing model,
while there is a notable performance gap between ND-Internal-MAS and D-Hybrid. One assumption is that the repeated latent variable of ND-Internal-MAS has very similar local representations,
which leads to the large receptive field size required for the decoder network.


-----

Table 6: CMOS comparison between EFTS2 and baseline models

**Model** **CMOS**

EFTS2 0

VITS -0.02
EFTS-CNN + HiFiGAN -0.31

Table 7: Comparison with previous text-to-waveform models

**Model** **High Efficiency** **High Quality** **Differentiable** **End-to-End Training**

Clarinet (Ping et al., 2019) ‚úì
WaveTacotron (Weiss et al., 2021) ‚úì ‚úì
EATS (Donahue et al., 2021) ‚úì ‚úì ‚úì
EFTS-wav (Miao et al., 2021) ‚úì ‚úì ‚úì
FastSpeech2s (Ren et al., 2021) ‚úì ‚úì
VITS (Kim et al., 2021) ‚úì ‚úì ‚úì
NaturalSpeech (Tan et al., 2022) ‚úì ‚úì ‚úì
EFTS2 ‚úì ‚úì ‚úì ‚úì

### E COMPARISONS WITH BASELINE MODELS AND OTHER
TEXT-TO-WAVEFORM MODELS

As EFTS2 achieves very similar MOS score to VITS, we further present the side-by-side CMOS
results in Table 6. As can be seen, EFTS2 slightly outperforms VITS with a CMOS score increase
of 0.02. In Table 7 we compare the advantages of EFTS2 with previous text-to-waveform models
in terms of training pipelines, differentiability, model performance, and model efficiency. EFTS2 is
the only differentiable model that allows for end-to-end training, high-quality and high-efficiency
generation.


-----


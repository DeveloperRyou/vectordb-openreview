## GLOBALLY OPTIMAL TRAINING OF NEURAL NET### WORKS WITH THRESHOLD ACTIVATION FUNCTIONS

**Tolga Ergen, Halil Ibrahim Gulluk, Jonathan Lacotte & Mert Pilanci**
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
ergen,gulluk,lacotte,pilanci @stanford.edu
_{_ _}_

#### ABSTRACT

Threshold activation functions are highly preferable in neural networks due to
their efficiency in hardware implementations. Moreover, their mode of operation
is more interpretable and resembles that of biological neurons. However, traditional gradient based algorithms such as Gradient Descent cannot be used to train
the parameters of neural networks with threshold activations since the activation
function has zero gradient except at a single non-differentiable point. To this end,
we study weight decay regularized training problems of deep neural networks
with threshold activations. We first show that regularized deep threshold network
training problems can be equivalently formulated as a standard convex optimization
problem, which parallels the LASSO method, provided that the last hidden layer
width exceeds a certain threshold. We also derive a simplified convex optimization
formulation when the dataset can be shattered at a certain layer of the network. We
corroborate our theoretical results with various numerical experiments.

#### 1 INTRODUCTION

In the past decade, deep neural networks have proven remarkably useful in solving challenging problems and become popular in many applications. The choice of activation plays a crucial role in their
performance and practical implementation. In particular, even though neural networks with popular
activation functions such as ReLU are successfully employed, they require advanced computational
resources in training and evaluation, e.g., Graphical Processing Units (GPUs) (Coates et al., 2013).
Consequently, training such deep networks is challenging especially without sophisticated hardware.
On the other hand, the threshold activation offers a multitude of advantages: (1) computational efficiency, (2) compression/quantization to binary latent dimension, (3) interpretability. Unfortunately,
gradient based optimization methods fail in optimizing threshold activation networks due to the fact
that the gradient is zero almost everywhere. To close this gap, we analyze the training problem of
deep neural networks with the threshold activation function defined as

�s if x 0
_σs(x):= s1{x ≥_ 0} = _≥_ (1)
0 otherwise _[,]_

where s ∈ R is a trainable amplitude parameter for the neuron. Our main result is that globally
optimal deep threshold networks can be trained by solving a convex optimization problem.

1.1 WHY SHOULD WE CARE ABOUT THRESHOLD NETWORKS?

Neural networks with threshold activations are highly desirable due to the following reasons:

- Since the threshold activation (1) is restricted to take values in 0, s, threshold neural network
_{_ _}_
models are far more suitable for hardware implementations (Bartlett & Downs, 1992; Corwin et al.,

1994). Specifically, these networks have significantly lower memory footprint, less computational
complexity, and consume less energy (Helwegen et al., 2019).

- Modern neural networks have extremely large number of full precision trainable parameters so
that several computational barriers emerge during hardware implementations. One approach to


-----

(a) (n, d) = (20, 100) (b) (n, d) = (50, 50) (c) (n, d) = (100, 20)

Figure 1: Training comparison of our convex program in (7) with the non-convex training heuristic
STE. We also indicate the time taken to solve the convex programs with markers. For non-convex STE,
we repeat the training with 5 different initializations. In each case, our convex training algorithms
achieve lower objective than all the non-convex heuristics (see Appendix B.5 for details).

mitigate these issues is reducing the network size by grouping the parameters via a hash function
(Hubara et al., 2017; Chen et al., 2015). However, this still requires full precision training before
the application of the hash function and thus fails to remedy the computational issues. On the other
hand, neural networks with threshold activations need a minimal amount of bits.

- Another approach to reduce the complexity is to quantize the weights and activations of the network
(Hubara et al., 2017) and the threshold activation is inherently in a two level quantized form.

- The threshold activation is a valid model to simulate the behaviour of a biological neuron as detailed
in Jain et al. (1996). Therefore, progress in this research field could shed light into the connection
between biological and artificial neural networks.

1.2 RELATED WORK

Although threshold networks are essential for several practical applications as detailed in the previous
section, training their parameters is a difficult non-differentiable optimization problem due to the
discrete nature in (1). For training of deep neural networks with popular activations, the common
practice is to use first order gradient based algorithms such as Gradient Descent (GD) since the
well known backpropagation algorithm efficiently calculates the gradient with respect to parameters.
However, the threshold activation in (1) has zero gradient except at a single non-differentiable point
zero, and therefore, one cannot directly use gradient based algorithms to train the parameters of the
network. In order to remedy this issue numerous heuristic algorithms have been proposed in the
literature as detailed below but they still fail to globally optimize the training objective (see Figure 1).

The Straight-Through Estimator (STE) is a widely used heuristics to train threshold networks (Bengio
et al., 2013; Hinton, 2012). Since the gradient is zero almost everywhere, Bengio et al. (2013);
Hinton (2012) proposed replacing the threshold activation with the identity function during only the
backward pass. Later on, this approach is extended to employ various forms of the ReLU activation
function, e.g., clipped ReLU, vanilla ReLU, Leaky ReLU (Yin et al., 2019b; Cai et al., 2017; Xiao
et al.), during the backward pass. Additionally, clipped versions of the identity function were also
used as an alternative to STE (Hubara et al., 2017; Courbariaux et al., 2016; Rastegari et al., 2016).

1.3 CONTRIBUTIONS

- We introduce polynomial-time trainable convex formulations of regularized deep threshold network
training problems provided that a layer width exceeds a threshold detailed in Table 1.

- In Theorem 2.2, we prove that the original non-convex training problem for two-layer networks is
equivalent to standard convex optimization problems.

- We show that deep threshold network training problems are equivalent to standard convex optimization problems in Theorem 3.2. In stark contrast to two-layer networks, deep threshold networks
can have a richer set of hyperplane arrangements due to multiple nonlinear layers (see Lemma 3.5).

- In Section 3.1, we characterize the evolution of the set of hyperplane arrangements and consequently
hidden layer representation space as a recursive process (see Figure 3) as the network gets deeper.

- We prove that when a certain layer width exceeds ([√]n/L), the regularized L-layer threshold
_O_
network training further simplifies to a problem that can be solved in (n) time.
_O_


-----

Table 1: Summary of our results for the optimization of weight decay regularized threshold network
training problems (n: # of data samples, d: feature dimension, ml: # of hidden neurons in layer l, r:
rank of the training data matrix, m[∗]: critical width, i.e., # of neurons that obeys 0 _m[∗]_ _n + 1)_
_≤_ _≤_

**Result** **Depth** **Complexity** **Minimum width** **Globally optimal**

Theorem 2.2 2 _O(n[3][r])_ _m ≥_ _m[∗]_ (convex opt)

Theorem 2.3 2 _O(n)_ _m ≥_ _n + 2_ (convex opt)

Theorem 3.2 _L_ _O(n[3][r][ �]l[L]=1[−][2]_ _[m][l]_ ) _mL−1 ≥_ _m[∗]_ (convex opt)

Corollary 3.4 _L_ _O(n)_ _∃l : ml ≥O([√]n/L)_ (convex opt)

**Notation and preliminaries: We use lowercase and uppercase bold letters to denote vectors and**
matrices, respectively. We also use [n] to denote the set 1, 2, . . ., n . We denote the training data
_{_ _}_
matrix consisting of d dimensional n samples as X ∈ R[n][×][d], label vector as y ∈ R[n], and the l[th]

layer weights of a neural network as W[(][l][)] _∈_ R[m][l][−][1][×][m][l], where m0 = d, mL = 1.

#### 2 TWO-LAYER THRESHOLD NETWORKS

We first consider the following two-layer threshold network

|Result|Depth|Complexity|Minimum width|Globally optimal|
|---|---|---|---|---|
|Theorem 2.2|2|O(n3r)|m ≥m∗|(convex opt)|
|Theorem 2.3|2|O(n)|m ≥n + 2|(convex opt)|
|Theorem 3.2|L|O(n3r QL l=− 12 ml)|m ≥m∗ L−1|(convex opt)|
|Corollary 3.4|L|O(n)|√ ∃l : m ≥O( n/L) l|(convex opt)|


_fθ,2(X)=σs(XW[(1)])w[(2)]=_


_m_
�sj1{Xwj[(1)][≥][0][}][w]j[(2)][,] (2)

_j=1_


where the set of the trainable parameters are W[(1)] _∈_ R[d][×][m], s ∈ R[m], w[(2)] _∈_ R[m] and θ is a compact
representation for the parameters, i.e., θ := **W[(1)], s, w[(2)]** . Note that we include bias terms by
_{_ _}_
concatenating a vector of ones to X. Next, consider the weight decay regularized training objective


1
_P2[noncvx]_ := min 2 [+][ β]
**W[(1)],s,w[(2)]** 2 2

_[∥][f][θ,][2][(][X][)][ −]_ **[y][∥][2]**


_m_
�

_j=1_


�
_∥wj[(1)][∥]2[2]_ [+][ |][s][j][|][2][ +][ |][w]j[(2)][|][2][�] _._ (3)


Now, we apply a scaling between variables sj and wj[(2)] to reach an equivalent optimization problem.

**Lemma 2.1 (Optimal scaling). The training problem in (3) can be equivalently stated as**

1
_P2[noncvx]_ = minθ∈Θs 2 _[∥][f][θ,][2][(][X][)][ −]_ **[y][∥]2[2]** [+][ β][∥][w][(2)][∥][1][,] (4)

_where Θs := {θ : |sj| = 1, ∀j ∈_ [m]}.

We next define the set of hyperplane arrangement patterns of the data matrix X as

_H(X):= {1{Xw ≥_ 0} : w ∈ R[d]} ⊂{0, 1}[n]. (5)

We denote the distinct elements of the set H(X) by d1, . . ., dP ∈{0, 1}[n], where P := |H(X)| is
the number of hyperplane arrangements. Using this fixed set of hyperplane arrangements {di}i[P]=1[,]
we next prove that (4) is equivalent to the standard Lasso method (Tibshirani, 1996).
**Theorem 2.2. Let m ≥** _m[∗], then the non-convex regularized training problem (4) is equivalent to_

1
_P2[cvx]_ = min 2 [+][ β][∥][w][∥][1][,] (6)
**w∈R[P]** 2 _[∥][Dw][ −]_ **[y][∥][2]**

_where D = [d1_ **d2** _. . ._ **dP ] is a fixed n × P matrix. Here, m[∗]** _is the cardinality of the optimal_
_solution, which satisfies m[∗]_ _≤_ _n + 1. Also, it holds that P2[noncvx]_ = P2[cvx].

Theorem 2.2 proves that the original non-convex training problem in (3) can be equivalently solved as
a standard convex Lasso problem using the hyperplane arrangement patterns as features. Surprisingly,
the non-zero support of the convex optimizer in (6) matches to that of the optimal weight-decay
regularized threshold activation neurons in the non-convex problem (3). This brings us two major
advantages over the standard non-convex training:

- Since (6) is a standard convex problem, it can be globally optimized without resorting to non-convex
optimization heuristics, e.g., initialization schemes, learning rate schedules etc.

- Since (6) is a convex Lasso problem, there exists many efficient solvers (Efron et al., 2004).


-----

2.1 SIMPLIFIED CONVEX FORMULATION FOR COMPLETE ARRANGEMENTS

We now show that if the set of hyperplane arrangements of X is complete, i.e., = 0, 1 contains
_H_ _{_ _}[n]_
all boolean sequences of length n, then the non-convex optimization problem in (3) can be simplified.
We call these instances complete arrangements. In the case of two-layer threshold networks, complete
arrangements emerge when the width of the network exceeds a threshold, specifically m _n + 2._
_≥_

We note that the m _n regime, also known as memorization, has been extensively studied in the_
_≥_
recent literature (Bubeck et al., 2020; de Dios & Bruna, 2020; Pilanci & Ergen, 2020; Rosset et al.,
2007). Particularly, these studies showed that as long as the width exceeds the number of samples,
there exists a neural network model that can exactly fit an arbitrary dataset. Vershynin (2020); Bartlett
et al. (2019) further improved the condition on the width by utilizing the expressive power of deeper
networks and developed more sophisticated weight construction algorithms to fit the data.
**Theorem 2.3. We assume that the set of hyperplane arrangements of X is complete, i.e., equal to the**
_set of all length-n Boolean sequences”_ = 0, 1 _. Suppose m_ _n + 2, then (4) is equivalent to_
_H_ _{_ _}[n]_ _≥_

1
_Pv[cvx]2_ [:= min]δ∈R[n] 2 _[∥][δ][ −]_ **[y][∥]2[2]** [+][ β][(][∥][(][δ][)][+][∥][∞] [+][ ∥][(][−][δ][)][+][∥][∞][)][ .] (7)

_and it holds that P2[noncvx]_ = Pv[cvx]2 _[. Also, one can construct an optimal network with][ n][ + 2][ neurons in]_
_time_ (n) based on the optimal solution to the convex problem (7).
_O_

Based on Theorem 2.3, when the data matrix can be shattered, i.e., all 2[n] possible y 0, 1
_∈{_ _}[n]_
labelings of the data points can be separated via a linear classifier, it follows that the set of hyperplane
arrangements is complete. Consequently, the non-convex problem in (3) further simplifies to (7).

2.2 TRAINING COMPLEXITY

We first briefly summarize our complexity results for solving (6) and (7), and then provide the details
of the derivations below. Our analysis reveals two interesting regimes:

- (incomplete arrangements) When n +1 _m_ _m[∗], we can solve (6) in_ (n[3][r]) complexity, where
_≥_ _≥_ _O_
_r := rank(X). Notice this is polynomial-time whenever the rank r is fixed._

- (complete arrangements) When m _n_ +2, we can solve (7) in closed-form, and the reconstruction
_≥_
of the non-convex parameters W[(1)] and w[(2)] only (n) time independent of d.
_O_

**Computational complexity of (6): To solve the optimization problem in (6), we first enumerate**
all possible hyperplane arrangements {di}i[P]=1[. It is well known that given a rank-][r][ data matrix the]
number of hyperplane arrangements P is upper bounded by (Stanley et al., 2004; Cover, 1965)


�r
_,_ (8)


_P_ 2
_≤_


_r−1_
�

_k=0_


�n 1� � _e(n_ 1)
_−_ _−_

2r
_≤_

_k_ _r_


where r = rank(X) min(n, d). Furthermore, these can be enumerated in (n[r]) (Edelsbrunner
_≤_ _O_
et al., 1986). Then, the complexity for solving (6) is (P [3]) (n[3][r]) (Efron et al., 2004).
_O_ _≈O_

**Computational complexity of (7): The problem in (7) is the proximal operator of a polyhedral norm.**
Since the problem is separable over the positive and negative parts of the parameter vector δ, the
optimal solution can be obtained by applying two proximal steps (Parikh & Boyd, 2014). As noted in
Theorem 2.3, the reconstruction of the non-convex parameters W[(1)] and w[(2)] requires (n) time.
_O_

2.3 A GEOMETRIC INTERPRETATION

To provide a geometric interpretation, we consider the weakly regularized case where β 0. In this
_→_
case, (6) reduces to the following minimum norm interpolation problem

min s.t. **Dw = y.** (9)
**w∈R[P][ ∥][w][∥][1]**

**Proposition 2.4. The minimum ℓ1 norm interpolation problem in (9) can be equivalently stated as**

mint≥0 _[t]_ s.t. **y ∈** _tConv{±dj, ∀j ∈_ [P ]},


-----

_where Conv(_ ) denotes the convex hull of a set _. This corresponds to the gauge function (see Rock-_
_A_ _A_
_afellar (2015)) of the hyperplane arrangement patterns and their negatives. We provide visualizations_
_of the convex set Conv{±dj, ∀j ∈_ [P ]} in Figures 3 and 4 (see Appendix) for Example 3.1.

Proposition 2.4 implies that the non-convex threshold network training problem in (3) implicitly
represents the label vector y as the convex combination of the hyperplane arrangements determined
by the data matrix X. Therefore, we explicitly characterize the representation space of threshold
networks. We also remark that this interpretation extends to arbitrary depth as shown in Theorem 3.2.

2.4 HOW TO OPTIMIZE THE HIDDEN LAYER WEIGHTS?

After training via the proposed convex programs in (6) and (7), we need to reconstruct the layer
weights of the non-convex model in (2). We first construct the optimal hyperplane arrangements
**di for (7) as detailed in Appendix A.4. Then, we have the following prediction model (2). Notice**
changing wj[(1)] to any other vector wj[′] [with same norm and such that][ 1][{][Xw]j[(1)] _≥_ 0} = 1{Xwj[′] _[≥]_ [0][}]
does not change the optimal training objective. Therefore, there are multiple global optima and the
one we choose might impact the generalization performance as discussed in Section 5.

#### 3 DEEP THRESHOLD NETWORKS

We now analyze L-layer parallel deep threshold networks model with mL−1 subnetworks defined as


_fθ,L(X) =_


_mL−1_
�

_σs(L−1)_ (X[(]k[L][−][2)]wk[(][L][−][1)])wk[(][L][)], (10)
_k=1_


where θ := {{Wk[(][l][)][,][ s]k[(][l][)][}]l[L]=1[}][m]k=1[L][−][1] [,][ θ][ ∈] [Θ:=][ {][θ][ :][ W]k[(][l][)] _∈_ R[m][l][−][1][×][m][l] _, s[(]k[l][)]_ _[∈]_ [R][m][l] _[,][ ∀][l, k][}][,]_

**X[(0)]k** [:=][ X][,] **X[(]k[l][)]** [:=][ σ]s[(]k[l][)] [(][X][(]k[l][−][1)]Wk[(][l][)][)][,][ ∀][l][ ∈] [[][L][ −] [1]][,]

and the subscript k is the index for the subnetworks (see Ergen & Pilanci (2021a;b); Wang et al.
(2023) for more details about parallel networks). We next show that the standard weight decay
regularized training problem can be cast as an ℓ1-norm minimization problem as in Lemma 2.1.

**Lemma 3.1. The following L-layer regularized threshold network training problem**


_L_
�

(∥Wk[(][l][)][∥]F[2] [+][∥][s]k[(][l][)][∥]2[2][)] (11)
_l=1_


1
_PL[noncvx]=minθ∈Θ_ 2 _[∥][f][θ,L][(][X][)][−][y][∥]2[2][+]_ _[β]2_

_can be reformulated as_


_mL−1_
�

_k=1_


1
_PL[noncvx]_ = minθ∈Θs 2 _[∥][f][θ,L][(][X][)][ −]_ **[y][∥]2[2]** [+][ β][∥][w][(][L][)][∥][1][,] (12)

_where Θs := {θ ∈_ Θ : |s[(]k[L][−][1)]| = 1}.

3.1 CHARACTERIZING THE SET OF HYPERPLANE ARRANGEMENTS FOR DEEP NETWORKS

We first define hyperplane arrangements for L-layer networks with a single subnetwork (i.e., mL−1 =
1, thus, we drop the index k). We denote the set of hyperplane arrangements as

_HL(X):= {1{X[(][L][−][2)]w[(][L][−][1)]_ _≥_ 0} : θ ∈ Θ}.

We also denote the elements of the set HL(X) by d1, . . ., dPL−1 ∈{0, 1}[n] and PL−1 = |HL(X)|
is the number of hyperplane arrangements in the layer L 1.
_−_

To construct the hyperplane arrangement matrix D[(][l][)], we define a matrix valued operator as follows

� � �
**D[(1)]** := A (X) = [d1 **d2** _. . ._ **dP1** ], D[(][l][+1)] := _A_ **D[(]S[l][)]** _, ∀l ∈_ [L − 2]. (13)

_|S|=ml_


-----

Here, the operator ( ) outputs a matrix whose columns contain all possible hyperplane arrangements
_A_ _·_
corresponding to its input matrix as in (5). In particular, D[(1)] denotes the arrangements for the
first layer given the input matrix X. The notation D[(][l][)] 0, 1 denotes the submatrix of
_S_ _∈{_ _}[n][×][m][l]_
**D[(][l][)]** 0, 1 indexed by the subset of its columns, where the index runs over all subsets of
_∈{_ _}[n][×][P][l]_ _S_ _S_
size ml. Finally, ⊔ is an operator that takes a union of these column vectors and outputs a matrix of
size n × Pl+1 containing these as columns. Note that we may omit repeated columns and denote the
total number of unique columns as Pl+1, since this does not change the value of our convex program.

We next provide an analytical example describing the construction of the matrix D[(][l][)].
**Example 3.1. We illustrate an example with the training data X = [−1 1; 0 1; 1 1] ∈** R[3][×][2].
_Inspecting the data samples (rows of X), we observe that all possible arrangement patterns are_


**D[(1)]** = (X) =
_A_


�0 0 0 1 1 1�
0 0 1 1 1 0
0 1 1 1 0 0


=⇒ _P1 = 6._ (14)


_For the second layer, we first specify the number of neurons in the first layer as m1 = 2. Thus, we_
_need to consider all possible column pairs in (14). We have_


��0 0��

= 0 0
_⇒A_

0 1

��0 0��

= 0 1
_⇒A_

0 1


**D[(1)]{1,2}** [=]

**D[(1)]{1,3}** [=]

_..._


�0 0�
0 0
0 1

�0 0�
0 1
0 1


=

=


�0 0 1 1�
0 0 1 1
0 1 1 0

�0 0 1 1�
0 1 1 0
0 1 1 0


_We then construct the hyperplane arrangement matrix as_


�


� � �
**D[(2)]** = **D[(1)]** =

_A_ _S_
_|S|=2_


�0 0 0 0 1 1 1 1
0 0 1 1 0 0 1 1
0 1 0 1 0 1 0 1


_,_


_which shows that P2 = 8. Consequently, we obtain the maximum possible arrangement patterns,_
_i.e.,_ 0, 1 _, in the second layer even though we are not able to obtain some of these patterns in the_
_{_ _}[3]_
_first layer in (14). We also provide a three dimensional visualization of this example in Figure 3._

3.2 POLYNOMIAL-TIME TRAINABLE CONVEX FORMULATION

Based on the procedure described in Section 3.1 to compute the arrangement matrix D[(][l][)], we now
derive an exact formulation for the non-convex training problem in (12).
**Theorem 3.2. Suppose that mL−1 ≥** _m[∗], then the non-convex training problem (12) is equivalent to_

_PL[cvx]_ = **w∈minR[PL][−][1]** 12 ���D(L−1)w − **y���22** [+][ β][∥][w][∥][1][,] (15)

_where D[(][L][−][1)]_ _∈{0, 1}[n][×][P][L][−][1]_ _is a fixed matrix constructed via (13) and m[∗]_ _denotes the cardinality_
_of the optimal solution, which satisfies where m[∗]_ _≤_ _n + 1. Also, it holds that PL[noncvx]_ = PL[cvx][.]

Theorem 3.2 shows that two-layer and deep networks simplify to very similar convex Lasso problems
(i.e. (6) and (15)). However, the set of hyperplane arrangements is larger for deep networks as
analyzed in Section 3.1. Thus, the structure of the diagonal matrix D and the problem dimensionality
are significantly different for these problems.

3.3 SIMPLIFIED CONVEX FORMULATION

Here, we show that the data X can be shattered at a certain layer, i.e., Hl(X) = {0, 1}[n] for a certain
_l ∈_ [L], if the number of hidden neurons in a certain layer ml satisfies ml ≥ _C[√]n/L. Then we_
can alternatively formulate (11) as a simpler convex problem. Therefore, compared to the two-layer
networks in Section 2.1, we substantially improve the condition on layer width by benefiting from
the depth L, which also confirms the benign impact of the depth on the optimization.


-----

**Lemma 3.3. If ∃l, C such that ml ≥** _C[√]n/L, then the set of hyperplane arrangements is complete,_
_i.e., HL(X) = {0, 1}[n]._

We next use Lemma 3.3 to derive a simpler form of (11).
**Corollary 3.4. As a direct consequence of Theorem 2.3 and Lemma 3.3, the non-convex deep**
_threshold network training problem in (12) can be cast as the following convex program_

1
_PL[noncvx]_ = Pv[cvx]2 [= min]δ∈R[n] 2 _[∥][δ][ −]_ **[y][∥]2[2]** [+][ β][(][∥][(][δ][)][+][∥][∞] [+][ ∥][(][−][δ][)][+][∥][∞][)][ .]

Surprisingly, both two-layer and deep networks share the same convex formulation in this case.
However, notice that two-layer networks require a condition on the data matrix in Theorem 2.3
whereas the result in Corollary 3.4 requires a milder condition on the layer widths.

3.4 TRAINING COMPLEXITY

Here, we first briefly summarize our complexity results for the convex training of deep networks.
Based on the convex problems in (15) and Corollary 3.4, we have two regimes:

- When n+1 ≥ _mL−1 ≥_ _m[∗], we solve (15) with O(n[3][r][ �]k[L]=1[−][2]_ _[m][k]_ ) complexity, where r := rank(X).
Note that this is polynomial-time when r and the number of neurons in each layer {ml}[L]l=1[−][2] [are]
constants.

- When ∃l, C : ml ≥ _C[√]n/L, we solve (7) in closed-form, and the reconstruction of the non-convex_
parameters requires (n) time as proven in Appendix A.9.
_O_

**Computational complexity for (15): We first need to obtain an upperbound on the problem dimen-**
sionality PL−1, which is stated in the next result.
**Lemma 3.5. The cardinality of the hyperplane arrangement set for an L-layer network HL(X)**
_can be bounded as |HL(X)| = PL−1 ≲_ _O(n[r][ �]k[L]=1[−][2]_ _[m][k]_ ), where r = rank(X) and ml denotes the
_number of hidden neurons in the l[th]_ _layer._

Lemma 3.5 shows that the set of hyperplane arrangements gets significantly larger as the depth
of the network increases. However, the cardinality of this set is still a polynomial term since that
_r < min{n, d} and {ml}[L]l=1[−][2]_ [are fixed constants.]

To solve (15), we first enumerate all possible arrangements {di}i[P]=1[L][−][1] to construct the matrix D[(][L][−][1)].
Then, we solve a standard convex Lasso problem, which requires O(PL[3]−1[)][ complexity (][Efron et al.][,]

2004). Thus, based on Lemma 3.5, the overall complexity is O(PL[3]−2[)][ ≈O][(][n][3][r][ �]k[L]=1[−][2] _[m][k]_ ).

**Computational complexity for (7): Since Corollary 3.4 yields (7), the complexity is O(n) time.**

#### 4 EXTENSIONS TO ARBITRARY LOSS FUNCTIONS

In the previous sections, we considered squared error as the loss function to give a clear description
of our approach. However, all the derivations extend to arbitrary convex loss. Now, we consider the
regularized training problem with a convex loss function ( _, y), e.g., hinge loss, cross entropy,_
_L_ _·_

_mL−1_ _L_
� �

minθ∈Θ _[L][(][f][θ,L][(][X][)][,][ y][) +][ β]2_ _k=1_ _l=1(∥Wk[(][l][)][∥]F[2]_ [+][ ∥][s]k[(][l][)][∥]2[2][)][.] (16)

Then, we have the following generic loss results.
**Corollary 4.1. Theorem 3.2 implies that when mL−1 ≥** _m[∗], (16) can be equivalently stated as_

� �
_PL[cvx]_ = min **D[(][L][−][1)]w, y** + β∥w∥1. (17)
**w∈R[PL][−][1][ L]**

_Alternatively when HL(X) = {0, 1}[n], based on Corollary 3.4, the equivalent convex problem is_
min (18)
**_δ∈R[n][ L][(][δ][,][ y][) +][ β][(][∥][(][δ][)][+][∥][∞]_** [+][ ∥][(][−][δ][)][+][∥][∞][)][.]

Corollary 4.1 shows that (17) and (18) are equivalent to the non-convex training problem in (16).
More importantly, they can be globally optimized via efficient convex optimization solvers.


-----

(a) (n, d, m1, m2) = (100, 20, 1000, 100)

(b) (n, d, m1, m2) = (50, 50, 1000, 50)

(c) (n, d, m1, m2) = (20, 100, 1000, 20)

Figure 2: In this figure, we compare the classification performance of three-layer threshold networks
trained with the setup described in Figure 1 for a single initialization trial. This experiment shows
that our convex training approach not only provides the globally optimal training performance but
also generalize remarkably well on the test data (see Appendix B.5 for details).

#### 5 EXPERIMENTS

In this section[1], we present numerical experiments verifying our theoretical results in the previous
sections. As discussed in Section 2.4, after solving the proposed convex problems in (15) and (7),
there exist multiple set of weight matrices yielding the same optimal objective value. Therefore,
to have a good generalization performance on test data, we use some heuristic methods for the
construction of the non-convex parameters {W[(][l][)]}l[L]=1[. Below, we provide details regarding the]
weight construction and review some baseline non-convex training methods.

**Convex-Lasso: To solve the problem (15), we first approximate arrangement patterns of the data**
matrix X by generating i.i.d. Gaussian weights G ∈ R[d][×][ ˜]P and subsample the arrangement patterns
via 1[XG 0]. Then, we use G as the hidden layer weights to construct the network. We repeat
_≥_
this process for every layer. Notice that here we sample a fixed subset of arrangements instead of
enumerating all possible P arrangements. Thus, this approximately solves (15) by subsampling its
decision variables, however, it still performs significantly better than standard non-convex training.
**Convex-PI: After solving (7), to recover the hidden layer weights of (2), we solve 1{Xwj[(1)]** _≥_ 0} =

**dj as wj[(1)]** = X[†]dj, where X[†] denotes the pseudo-inverse of X. The resulting weights wj[(1)] enforce

the preactivations Xwj[(1)] to be zero or one. Thus, if an entry is slightly higher or less than zero due to
precision issues during the pseudo-inverse, it might give wrong output after the threshold activation.
To avoid such cases, we use 0.5 threshold in the test phase, i.e., 1{Xtestwj[(1)] _≥_ 0.5}.

**Convex-SVM: Another approach to solve 1{Xwj[(1)]** _≥_ 0} = dj is to use Support Vector Machines

1We provide additional experiments and details in Appendix B.


-----

Table 2: Test performance comparison on CIFAR-10 (Krizhevsky et al., 2014), MNIST (LeCun),
and UCI (Dua & Graff, 2017) datasets. We repeat simulations over multiple seeds with two-layer
networks and compare the mean/std of the test accuracies of non-convex heuristics trained with SGD
with our convex program in (6). Our convex approach achieves highest test accuracy for 9 of 13
datasets whereas the best non-convex heuristic achieves the highest test accuracy only for 4 datasets.

**Dataset** **Convex-Lasso (Ours)** **Nonconvex-STE** **Nonconvex-ReLU** **Nonconvex-LReLU** **Nonconvex-CReLU**

**Accuracy** **Time(s)** **Accuracy** **Time(s)** **Accuracy** **Time(s)** **Accuracy** **Time(s)** **Accuracy** **Time(s)**

CIFAR-10 **0.816 ± 0.008** **8.9 ± 0.3** 0.81 ± 0.004 83.5 ± 4.9 0.803 ± 0.004 85.8 ± 4.7 0.798 ± 0.004 92.1 ± 4.9 0.808 ± 0.004 87.1 ± 4.5
MNIST **0.9991 ± 1.9e[−][4]** **39.4 ± 0.1** 0.9986 ± 3.5e[−][4] 61.3 ± 0.04 0.9984 ± 4.6e[−][4] 63.4 ± 0.1 0.9985 ± 2.9e[−][4] 75.5 ± 0.1 0.9985 ± 2.9e[−][4] 64.9 ± 0.07
bank 0.895 ± 0.007 7.72 ± 1.02 0.892 ± 0.008 **5.83 ± 0.06** **0.900 ± 0.008** 5.96 ± 0.12 0.899 ± 0.008 8.41 ± 0.12 0.897 ± 0.008 6.35 ± 0.11
chess-krvkp **0.945 ± 0.005** **5.34 ± 0.38** 0.937 ± 0.008 6.78 ± 0.20 0.934 ± 0.007 6.17 ± 0.21 **0.945 ± 0.007** 7.44 ± 0.10 0.941 ± 0.013 6.15 ± 0.47
mammographic **0.818 ± 0.014** **2.64 ± 0.52** 0.808 ± 0.011 5.40 ± 0.63 0.803 ± 0.014 6.51 ± 0.64 0.801 ± 0.013 5.76 ± 0.185 0.817 ± 0.019 5.29 ± 0.13
oocytes-4d **0.787 ± 0.020** **2.23 ± 0.09** **0.787 ± 0.038** 5.61 ± 0.26 0.756 ± 0.021 7.09 ± 0.27 0.723 ± 0.006 6.22 ± 0.12 0.732 ± 0.030 5.79 ± 0.04
oocytes-2f **0.799 ± 0.022** **1.99 ± 0.06** 0.776 ± 0.035 5.24 ± 0.05 0.774 ± 0.022 6.97 ± 0.05 0.775 ± 0.027 5.89 ± 0.04 0.783 ± 0.023 5.46 ± 0.04
ozone **0.967 ± 0.006** **3.65 ± 0.18** **0.967 ± 0.005** 6.30 ± 0.30 **0.967 ± 0.005** 6.89 ± 0.15 **0.967 ± 0.005** 7.86 ± 0.06 **0.967 ± 0.005** 6.20 ± 0.10
pima 0.719 ± 0.019 **1.67 ± 0.11** 0.727 ± 0.018 5.20 ± 0.29 0.730 ± 0.031 6.54 ± 0.21 **0.734 ± 0.025** 5.72 ± 0.07 0.729 ± 0.015 5.23 ± 0.02
spambase 0.919 ± 0.007 6.91 ± 0.34 0.924 ± 0.004 7.41 ± 0.04 0.925 ± 0.005 **6.17 ± 0.07** 0.921 ± 0.003 8.78 ± 0.20 **0.926 ± 0.005** 6.61 ± 0.11
statlog-german **0.761 ± 0.030** **2.22 ± 0.09** 0.755 ± 0.021 5.84 ± 0.70 0.756 ± 0.037 6.39 ± 0.69 0.753 ± 0.039 5.89 ± 0.07 0.758 ± 0.037 5.48 ± 0.12
tic-tac-toe **0.980 ± 0.010** **1.89 ± 0.25** 0.954 ± 0.009 4.97 ± 0.03 0.932 ± 0.025 6.63 ± 0.04 0.926 ± 0.016 5.61 ± 0.04 0.951 ± 0.012 5.18 ± 0.03
titanic 0.778 ± 0.041 **0.35 ± 0.03** 0.790 ± 0.024 5.06 ± 0.04 0.784 ± 0.026 6.30 ± 0.26 **0.796 ± 0.017** 6.24 ± 0.23 0.784 ± 0.026 5.19 ± 0.01

Accuracy/Time **9/13** **11/13** **2/13** **1/13** **2/13** **1/13** **4/13** **0/13** **2/13** **0/13**

(SVMs), which find the maximum margin vector. Particularly, we set the zero entries of di as −1
and then directly run the SVM to get the maximum margin hidden neurons corresponding to this
arrangement. Since the labels are in the form +1, 1 in this case, we do not need additional
_{_ _−_ _}_
thresholding as in the previous approach.
**Nonconvex-STE (Bengio et al., 2013): This is the standard non-convex training algorithm, where**
the threshold activations is replaced with the identity function during the backward pass.
**STE Variants: We also benchmark against variants of STE. Specifically, we replace the threshold**
activation with ReLU (Nonconvex-ReLU (Yin et al., 2019a)), Leaky ReLU (Nonconvex-LReLU
(Xiao et al.)), and clipped ReLU (Nonconvex-CReLU (Cai et al., 2017)) during the backward pass.

**Synthetic Datasets: We compare the performances of Convex-PI and Convex-SVM trained via (7)**
with the non-convex heuristic methods mentioned above. We first run each non-convex heuristic
for five different initializations and then plot the best performing one in Figure 1. This experiment
clearly shows that the non-convex heuristics fail to achieve the globally optimal training performance
provided by our convex approaches. For the same setup, we also compare the training and test
accuracies for three different regimes, i.e., n > d, n = d, and n < d. As seen in Figure 2, our convex
approaches not only globally optimize the training objective but also generalize well on the test data.

**Real Datasets: In Table 2, we compare the test accuracies of two-layer threshold network trained via**
our convex formulation in (15), i.e., Convex-Lasso and the non-convex heuristics mentioned above.
For this experiment, we use CIFAR-10 (Krizhevsky et al., 2014), MNIST (LeCun), and the datasets
in the UCI repository (Dua & Graff, 2017) which are preprocessed as in Fernandez-Delgado et al.´
(2014). Here, our convex training approach achieves the highest test accuracy for most of the datasets
while the non-convex heuristics perform well only for a few datasets. Therefore, we also validates
the good generalization capabilities of the proposed convex training methods on real datasets.

#### 6 CONCLUSION

We proved that the training problem of regularized deep threshold networks can be equivalently
formulated as a standard convex optimization problem with a fixed data matrix consisting of hyperplane arrangements determined by the data matrix and layer weights. Since the proposed formulation
parallels the well studied Lasso model, we have two major advantages over the standard non-convex
training methods: 1) We globally optimize the network without resorting to any optimization heuristic or extensive hyperparameter search (e.g., learning rate schedule and initialization scheme); 2)
We efficiently solve the training problem using specialized solvers for Lasso. We also provided a
computational complexity analysis and showed that the proposed convex program can be solved in
polynomial-time. Moreover, when a layer width exceeds a certain threshold, a simpler alternative
convex formulation can be solved in (n). Lastly, as a by product of our analysis, we characterize
_O_
the recursive process behind the set of hyperplane arrangements for deep networks. Even though
this set rapidly grows as the network gets deeper, globally optimizing the resulting Lasso problem
still requires polynomial-time complexity for fixed data rank. We also note that the convex analysis
proposed in this work is generic in the sense that it can be applied to various architectures including
batch normalization (Ergen et al., 2022b), vector output networks (Sahiner et al., 2020; 2021), polynomial activations (Bartan & Pilanci, 2021), GANs (Sahiner et al., 2022a), autoregressive models
(Gupta et al., 2021), and Transformers (Ergen et al., 2022a; Sahiner et al., 2022b).

|Dataset|Convex-Lasso (Ours)|Nonconvex-STE Nonconvex-ReLU Nonconvex-LReLU Nonconvex-CReLU|Col4|Col5|Col6|
|---|---|---|---|---|---|
||Accuracy Time(s)|Accuracy Time(s)|Accuracy Time(s)|Accuracy Time(s)|Accuracy Time(s)|
|CIFAR-10 MNIST bank chess-krvkp mammographic oocytes-4d oocytes-2f ozone pima spambase statlog-german tic-tac-toe titanic|0.816 ± 0.008 8.9 ± 0.3 0.9991 ± 1.9e−4 39.4 ± 0.1 0.895 ± 0.007 7.72 ± 1.02 0.945 ± 0.005 5.34 ± 0.38 0.818 ± 0.014 2.64 ± 0.52 0.787 ± 0.020 2.23 ± 0.09 0.799 ± 0.022 1.99 ± 0.06 0.967 ± 0.006 3.65 ± 0.18 0.719 ± 0.019 1.67 ± 0.11 0.919 ± 0.007 6.91 ± 0.34 0.761 ± 0.030 2.22 ± 0.09 0.980 ± 0.010 1.89 ± 0.25 0.778 ± 0.041 0.35 ± 0.03|0.81 ± 0.004 83.5 ± 4.9 0.9986 ± 3.5e−4 61.3 ± 0.04 0.892 ± 0.008 5.83 ± 0.06 0.937 ± 0.008 6.78 ± 0.20 0.808 ± 0.011 5.40 ± 0.63 0.787 ± 0.038 5.61 ± 0.26 0.776 ± 0.035 5.24 ± 0.05 0.967 ± 0.005 6.30 ± 0.30 0.727 ± 0.018 5.20 ± 0.29 0.924 ± 0.004 7.41 ± 0.04 0.755 ± 0.021 5.84 ± 0.70 0.954 ± 0.009 4.97 ± 0.03 0.790 ± 0.024 5.06 ± 0.04|0.803 ± 0.004 85.8 ± 4.7 0.9984 ± 4.6e−4 63.4 ± 0.1 0.900 ± 0.008 5.96 ± 0.12 0.934 ± 0.007 6.17 ± 0.21 0.803 ± 0.014 6.51 ± 0.64 0.756 ± 0.021 7.09 ± 0.27 0.774 ± 0.022 6.97 ± 0.05 0.967 ± 0.005 6.89 ± 0.15 0.730 ± 0.031 6.54 ± 0.21 0.925 ± 0.005 6.17 ± 0.07 0.756 ± 0.037 6.39 ± 0.69 0.932 ± 0.025 6.63 ± 0.04 0.784 ± 0.026 6.30 ± 0.26|0.798 ± 0.004 92.1 ± 4.9 0.9985 ± 2.9e−4 75.5 ± 0.1 0.899 ± 0.008 8.41 ± 0.12 0.945 ± 0.007 7.44 ± 0.10 0.801 ± 0.013 5.76 ± 0.185 0.723 ± 0.006 6.22 ± 0.12 0.775 ± 0.027 5.89 ± 0.04 0.967 ± 0.005 7.86 ± 0.06 0.734 ± 0.025 5.72 ± 0.07 0.921 ± 0.003 8.78 ± 0.20 0.753 ± 0.039 5.89 ± 0.07 0.926 ± 0.016 5.61 ± 0.04 0.796 ± 0.017 6.24 ± 0.23|0.808 ± 0.004 87.1 ± 4.5 0.9985 ± 2.9e−4 64.9 ± 0.07 0.897 ± 0.008 6.35 ± 0.11 0.941 ± 0.013 6.15 ± 0.47 0.817 ± 0.019 5.29 ± 0.13 0.732 ± 0.030 5.79 ± 0.04 0.783 ± 0.023 5.46 ± 0.04 0.967 ± 0.005 6.20 ± 0.10 0.729 ± 0.015 5.23 ± 0.02 0.926 ± 0.005 6.61 ± 0.11 0.758 ± 0.037 5.48 ± 0.12 0.951 ± 0.012 5.18 ± 0.03 0.784 ± 0.026 5.19 ± 0.01|


-----

#### 7 ACKNOWLEDGEMENTS

This work was partially supported by the National Science Foundation (NSF) under grants ECCS2037304, DMS-2134248, NSF CAREER award CCF-2236829, the U.S. Army Research Office Early
Career Award W911NF-21-1-0242, Stanford Precourt Institute, and the ACCESS – AI Chip Center
for Emerging Smart Systems, sponsored by InnoHK funding, Hong Kong SAR.

#### REFERENCES

Burak Bartan and Mert Pilanci. Neural spectrahedra and semidefinite lifts: Global convex optimization of polynomial activation neural networks in fully polynomial-time. arXiv preprint
_arXiv:2101.02429, 2021._

Peter L. Bartlett and Tom Downs. Using random weights to train multilayer networks of hardlimiting units. IEEE Trans. Neural Networks, 3(2):202–210, 1992. doi: 10.1109/72.125861. URL
[https://doi.org/10.1109/72.125861.](https://doi.org/10.1109/72.125861)

Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension
and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning
_[Research, 20(63):1–17, 2019. URL http://jmlr.org/papers/v20/17-612.html.](http://jmlr.org/papers/v20/17-612.html)_

Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through´
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

Sebastien Bubeck, Ronen Eldan, Yin Tat Lee, and Dan Mikulincer. Network size and size of´
the weights in memorization with two-layers neural networks. Advances in Neural Information
_Processing Systems, 33:4977–4986, 2020._

Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by
half-wave gaussian quantization. Computer Vision and Pattern Recognition, 2017.

Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing neural
networks with the hashing trick. In International conference on machine learning, pp. 2285–2294.
PMLR, 2015.

Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Ng Andrew. Deep learning
with cots hpc systems. In International conference on machine learning, pp. 1337–1345. PMLR,
2013.

Edward M. Corwin, Antonette M. Logar, and William J. B. Oldham. An iterative method for training
multilayer networks with threshold functions. IEEE Trans. Neural Networks, 5(3):507–508, 1994.
[doi: 10.1109/72.286926. URL https://doi.org/10.1109/72.286926.](https://doi.org/10.1109/72.286926)

Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training neural networks with weights and activations constrained to +1 or -1.
_arXiv preprint arXiv:1602.02830, 2016._

Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition. IEEE transactions on electronic computers, (3):326–334, 1965.

Jaume de Dios and Joan Bruna. On sparsity in overparametrised shallow relu networks. arXiv
_preprint arXiv:2006.10225, 2020._

[Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.](http://archive.ics.uci.edu/ml)
[ics.uci.edu/ml.](http://archive.ics.uci.edu/ml)

Herbert Edelsbrunner, Joseph O’Rourke, and Raimund Seidel. Constructing arrangements of lines
and hyperplanes with applications. SIAM Journal on Computing, 15(2):341–363, 1986.

Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. The
_Annals of statistics, 32(2):407–499, 2004._


-----

Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
_arXiv preprint arXiv:2002.11219, 2020._

Tolga Ergen and Mert Pilanci. Global optimality beyond two layers: Training deep relu networks via
convex programs. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
_Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp._
[2993–3003. PMLR, 18–24 Jul 2021a. URL https://proceedings.mlr.press/v139/](https://proceedings.mlr.press/v139/ergen21a.html)
[ergen21a.html.](https://proceedings.mlr.press/v139/ergen21a.html)

Tolga Ergen and Mert Pilanci. Path regularization: A convexity and sparsity inducing regularization
for parallel relu networks. arXiv preprint arXiv: Arxiv-2110.09548, 2021b.

Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality. In
Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
_Learning, volume 139 of Proceedings of Machine Learning Research, pp. 3004–3014. PMLR,_
[18–24 Jul 2021c. URL http://proceedings.mlr.press/v139/ergen21b.html.](http://proceedings.mlr.press/v139/ergen21b.html)

Tolga Ergen, Behnam Neyshabur, and Harsh Mehta. Convexifying transformers: Improving optimiza[tion and understanding of transformer networks, 2022a. URL https://arxiv.org/abs/](https://arxiv.org/abs/2211.11052)
[2211.11052.](https://arxiv.org/abs/2211.11052)

Tolga Ergen, Arda Sahiner, Batu Ozturkler, John M. Pauly, Morteza Mardani, and Mert Pilanci.
Demystifying batch normalization in reLU networks: Equivalent convex optimization models and
implicit regularization. In International Conference on Learning Representations, 2022b. URL
[https://openreview.net/forum?id=6XGgutacQ0B.](https://openreview.net/forum?id=6XGgutacQ0B)

Manuel Fernandez-Delgado, Eva Cernadas, Sen´ en Barro, and Dinani Amorim. Do we need hundreds´
of classifiers to solve real world classification problems? Journal of Machine Learning Research,
[15(90):3133–3181, 2014. URL http://jmlr.org/papers/v15/delgado14a.html.](http://jmlr.org/papers/v15/delgado14a.html)

Vikul Gupta, Burak Bartan, Tolga Ergen, and Mert Pilanci. Convex neural autoregressive models:
Towards tractable, expressive, and theoretically-backed models for sequential forecasting and
generation. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and
_Signal Processing (ICASSP), pp. 3890–3894, 2021. doi: 10.1109/ICASSP39728.2021.9413662._

Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, and Roeland
Nusselder. Latent weights do not exist: Rethinking binarized neural network optimization. Neural
_Information Processing Systems, 2019._

Geoffrey Hinton. Neural networks for machine learning. Coursera Video Lectures, 2012.

Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. The Journal
_of Machine Learning Research, 2017._

Anil K Jain, Jianchang Mao, and K Moidin Mohiuddin. Artificial neural networks: A tutorial.
_Computer, 29(3):31–44, 1996._

[Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset. http://www.cs.](http://www. cs. toronto. edu/kriz/cifar. html)
[toronto.edu/kriz/cifar.html, 2014.](http://www. cs. toronto. edu/kriz/cifar. html)

[Yann LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/](http://yann. lecun. com/exdb/mnist/)
[mnist/.](http://yann. lecun. com/exdb/mnist/)

Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in optimization, 1(3):
127–239, 2014.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026–8037, 2019.


-----

Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. International Conference on Machine
_Learning, 2020._

Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. European conference on computer
_vision, pp. 525–542, 2016._

Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 2015.

Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. L1 regularization in infinite
dimensional feature spaces. In International Conference on Computational Learning Theory, pp.
544–558. Springer, 2007.

Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John Pauly. Convex regularization
behind neural reconstruction. arXiv preprint arXiv:2012.05169, 2020.

Arda Sahiner, Tolga Ergen, John M. Pauly, and Mert Pilanci. Vector-output relu neural network
problems are copositive programs: Convex analysis of two layer networks and polynomial-time
[algorithms. In International Conference on Learning Representations, 2021. URL https:](https://openreview.net/forum?id=fGF8qAqpXXG)
[//openreview.net/forum?id=fGF8qAqpXXG.](https://openreview.net/forum?id=fGF8qAqpXXG)

Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John M. Pauly, Morteza Mardani, and
Mert Pilanci. Hidden convexity of wasserstein GANs: Interpretable generative models with
closed-form solutions. In International Conference on Learning Representations, 2022a. URL
[https://openreview.net/forum?id=e2Lle5cij9D.](https://openreview.net/forum?id=e2Lle5cij9D)

Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Unraveling attention via convex duality: Analysis and interpretations of vision transformers. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato
(eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of
_Proceedings of Machine Learning Research, pp. 19050–19088. PMLR, 17–23 Jul 2022b. URL_
[https://proceedings.mlr.press/v162/sahiner22a.html.](https://proceedings.mlr.press/v162/sahiner22a.html)

Richard P Stanley et al. An introduction to hyperplane arrangements. Geometric combinatorics, 13:
389–496, 2004.

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
_Society: Series B (Methodological), 58(1):267–288, 1996._

Roman Vershynin. Memory capacity of neural networks with threshold and relu activations. arXiv
_preprint arXiv:2001.06938, 2020._

Yifei Wang, Tolga Ergen, and Mert Pilanci. Parallel deep neural networks have zero duality gap. In
_[International Conference on Learning Representations, 2023. URL https://openreview.](https://openreview.net/forum?id=6zrOr_Rdhjs)_
[net/forum?id=6zrOr_Rdhjs.](https://openreview.net/forum?id=6zrOr_Rdhjs)

Xia Xiao, Zigeng Wang, and Sanguthevar Rajasekaran. Autoprune: Automatic network pruning by
regularizing auxiliary parameters. Advances in Neural Information Processing Systems.

Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Understanding straight-through estimator in training activation quantized neural nets. International
_Conference on Learning Representations, 2019a._

Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Blended
coarse gradient descent for full quantization of deep neural networks. Research in the Mathematical
_Sciences, 6:1–23, 2019b._


-----

# Appendix

### Table of Contents

**A Proofs of the results in the main paper** **13**

A.1 Lemma 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

A.2 Proposition 2.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

A.3 Theorem 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

A.4 Theorem 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

A.5 Lemma 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

A.6 Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

A.7 Lemma 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

A.8 Lemma 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

A.9 Corollary 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

A.10 Corollary 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

**B** **Additional experiments and details** **20**

B.1 Experiment in Figure 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

B.2 Experiment in Table 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

B.3 Two-layer experiments in Figure 6, 7, 8, and 9 . . . . . . . . . . . . . . . . . . 22

B.4 Three-layer experiments with a representation matrix in Figure 10, 11, 12, and 13 23

B.5 Standard three-layer experiments in Figure 1 and 2 . . . . . . . . . . . . . . . . 25

B.6 Standard ten-layer experiments in Figure 14 . . . . . . . . . . . . . . . . . . . 25

(a) 2-layer network (b) 3-layer network

Figure 3: The convex hull of D[(1)] = A(X) corresponding to the 2-layer network (a) and the convex

� �

hull of [�]|S|=m1 _[A]_ **D[(1)]S** corresponding to the 3-layer network (b) for the data in Example 3.1.

Here, we visualize the convex constraint in Proposition 2.4, which is the implicit representation
revealed by convex analysis. We observe that the hidden representation space becomes richer with an
additional nonlinear layer (from 2-layer to 3-layer), and the resulting symmetry in the convex hull
enables the simpler convex formulation (7).

#### A PROOFS OF THE RESULTS IN THE MAIN PAPER

A.1 LEMMA 2.1

**_Proof of Lemma 2.1. We first restate the original training problem below._**


1
_P2[noncvx]_ = min 2 [+][ β]
**W[(1)],s,w[(2)]** 2 2

_[∥][f][θ,][2][(][X][)][ −]_ **[y][∥][2]**


_m_
�

_j=1_


�
_∥wj[(1)][∥]2[2]_ [+][ s]j[2] [+][ |][w]j[(2)][|][2][�] _._ (19)


As already noted in the main paper the loss function is invariant to the norm of wj[(1)] and we may

have wj[(1)] _→_ 0 to reduce the regularization cost. Therefore, we can omit the regularization penalty


-----

(a) n = 2 (b) n = 3

Figure 4: Two and three dimensional visualizations of hidden layer representation space of the weight
decay regularized threshold networks. Here, we visualize the convex hull in Proposition 2.4, which is
the implicit representation space revealed by our analysis.

on wj[(1)] to simplify (19) as

_m_

1 �
_P2[noncvx]_ = **W[(1)]min,s,w[(2)]** 2 2 [+][ β]2 (s[2]j [+][ |][w]j[(2)][|][2][)][ .]

_[∥][f][θ,][2][(][X][)][ −]_ **[y][∥][2]** _j=1_

We now note that one can rescale the parameters as

_s¯j = αjsj, ¯wj[(2)]_ = _wαj[(2)]j_

without the changing the network’s output, i.e.,


_m_
� _sj1{Xwj[(1)][}][w]j[(2)]_ = fθ,2(X)

_j=1_


_fθ,¯_ 2(X) =


_m_
� _s¯j1{Xwj[(1)][}][ ¯][w]j[(2)]_ =

_j=1_


where αj > 0. We next note that


_αj[2][s]j[2]_ [+] _|wj[(2)][|][2]_

_αj[2]_


�


_m_
�

_j=1_


�


_m_
�

_|s¯j|| ¯wj[(2)][|][,]_
_j=1_


_β_

2


_m_
�

(¯s[2]j [+ ¯][w]j[(2)][2] ) = _[β]_

2

_j=1_


_β_
_≥_


_m_
�

_|sj||wj[(2)][|][ =][ β]_
_j=1_


�

where the equality holds when αj = _|w|sj[(2)]j_ _|_ _|_ [. Thus, we obtain the following reformulation of the]

objective function in (19) where the regularization term takes a multiplicative form


1
_P2[noncvx]_ = min 2 [+][ β]
**W[(1)],s,w[(2)]** 2

_[∥][f][θ,][2][(][X][)][ −]_ **[y][∥][2]**


_m_
�

_|sj||wj[(2)][|][.]_ (20)
_j=1_


Next, we apply a variable change for the new formulation in (20) as follows

_s[′]j_ [:=][ s][j] _wj[(2)][′]_ := wj[(2)]

_|sj|_ _[,]_ _[|][s][j][|][.]_

With the variable change above (20) can be equivalently written as


_m_
�

_|wj[(2)][′]_ _|._ (21)
_j=1_


_P2[noncvx]_ = min
**W[(1)],w[(2)][′]**

**s[′]:|s[′]j** _[|][=1]_


1
2 [+][ β]
2

_[∥][f][θ,][2][(][X][)][ −]_ **[y][∥][2]**


This concludes the proof and yields the following equivalent formulation of (21)


_P2[noncvx]_ = min
**W[(1)],w[(2)][′]**

**s[′]:|s[′]j** _[|][=1]_


1
2 [+][ β][∥][w][(2)][′] _[∥][1][,]_
2

_[∥][f][θ,][2][(][X][)][ −]_ **[y][∥][2]**


-----

A.2 PROPOSITION 2.4

**_Proof of Proposition 2.4. Using the reparameterization for Lasso problems, we rewrite (9) as_**


_P_
�

**dj(wj[+]** _j_ [) =][ y][.] (22)

_[−]_ _[w][−]_
_j=1_


min
_wj[+][,w]j[−][≥][0]_


_P_
�

(wj[+] [+][ w]j[−][)] s.t.
_j=1_


We next introduce a slack variable t 0 such that (22) can be rewritten as
_≥_

_P_ _P_
� �

mint≥0 _wj[+]min[,w]j[−][≥][0]_ _t_ s.t. _j=1_ **dj(wj[+]** _[−]_ _[w]j[−][) =][ y][,]_ _j=1(wj[+]_ [+][ w]j[−][) =][ t.] (23)

We now rescale wj[+][, w]j[−] [with][ 1][/t][ to obtain the following equivalent of (][23][)]


_P_
�

(wj[+] [+][ w]j[−][) = 1][.] (24)
_j=1_


min min _t_ s.t.
_t≥0_ _wj[+][,w]j[−][≥][0]_

The problem in (24) implies that


_P_
�

**dj(wj[+]** _j_ [) =][ y][/t,]

_[−]_ _[w][−]_
_j=1_


_P_
�

**dj(wj[+]** _j_ [) =][ y][/t][ ⇐⇒] **[y][ ∈]** _[t][Conv][{±][d][j][,][ ∀][j][ ∈]_ [[][P] []][}][,]

_[−]_ _[w][−]_
_j=1_


_∃wj[+][, w]j[−]_ _[≥]_ [0 s][.][t][.]


_P_
�

(wj[+] [+][ w]j[−][) = 1]
_j=1_


where Conv denotes the convex hull operation. Therefore, (9) can be equivalenly formulated as
mint≥0 _[t]_ s.t. **y ∈** _tConv{±dj, ∀j ∈_ [P ]}.

A.3 THEOREM 2.2

**_Proof of Theorem 2.2. We first remark that the activations can only be either zero or one, i.e.,_**
_σsj_ (Xwj[(1)][)][ ∈{][0][,][ 1][}][n][,][ since][ |][s][j][|][ = 1][,][ ∀][j][ ∈] [[][m][]][. Therefore, based on Lemma][ 2.1][, we reformulated]
(4) as

1

_P2[noncvx]_ = min 2 [+][ β][∥][w][∥][1][,] (25)
**dij ∈{dw1...dP }** 2 _[∥][[][d][i][1]_ _[, ...,][ d][i][m][]][ w][ −]_ **[y][∥][2]**

where we denote the elements of the set H(X) by d1, . . ., dP ∈{0, 1}[n] and P is the number of
hyperplane arrangements. The above problem is similar to Lasso (Tibshirani, 1996), although it is
non-convex due to the discrete variables di1 _, ..., dim_ . These m hyperplane arrangement patterns are
discrete optimization variables along with the coefficient vector w. In fact, the above problem is
equivalent to a cardinality-constrained Lasso problem.

We have


1

_P2[noncvx]_ = min 2 [+][ β][∥][w][∥][1]
**r∈R[n], d1,...,dm∈H(X), w∈R[m]** 2 _[∥][r][∥][2]_

**r=[d1,...,dm]w−y**

_m_
1 �
= min 2 [+][ β][∥][w][∥][1] [+][ z][T][ (][r][ +][ y][)][ −] **[z][T]** **diwi**
**r∈R[n],d1,...,dm∈H(X),w∈R[m][ max]z∈R[n]** 2 _[∥][r][∥][2]_ _i=1_


(i)
_≥_ **d1,...,mindm∈H(X)** **z[max]∈R[n]** **r∈R[n]min,w∈R[m][ z][T][ y][ + 1]2** _[∥][r][∥]2[2]_ [+][ z][T][ r][ +]


_m_
�

_β|wi| −_ _wiz[T]_ **di**
_i=1_


= min max
**d1,...,dm∈H(X)** **z∈R[n]**

_|z[T]_ **di|≤β, ∀i∈[m]**


1
2 _[∥][y][∥]2[2]_ _[−]_ [1]2 _[∥][z][ −]_ **[y][∥]2[2]**


(ii)
max
_≥_
**z∈R[n]**

maxd∈H(X) |z[T] **d|≤β**


1
2 _[∥][y][∥]2[2]_ _[−]_ [1]2 _[∥][z][ −]_ **[y][∥]2[2]**


(iii)
max
_≥_
**z∈R[n]**

_|z[T]_ **di|≤β,∀i∈[P ]**


1
2 2 [=][ D]2[cvx] _,_
2 _[∥][y][∥][2]_ _[−]_ [1]2 _[∥][z][ −]_ **[y][∥][2]**


-----

where inequality (i) holds by weak duality, inequality (ii) follows from augmenting the set of
constraints |z[T] **di| ≤** _β to hold for any sign pattern d ∈H(X), and inequality (iii) follows from_
enumerating all possible sign patterns, i.e., H(X) = {d1, d2, . . ., dP }.

We now prove that strong duality in fact holds, i.e., P2[noncvx] = D2[cvx]. We first form the Lagrangian
of the dual problem D2[cvx]


1
_D2[cvx]_ = maxz∈R[n] _wimin,wi[′]_ _[≥][0]_ 2 _[∥][y][∥]2[2]_ _[−]_ [1]2 _[∥][z][ −]_ **[y][∥]2[2]** [+]

(i) 1
= min 2 2 [+]
_wi,wi[′]_ _[≥][0][ max]z∈R[n]_ 2 _[∥][y][∥][2]_ _[−]_ [1]2 _[∥][z][ −]_ **[y][∥][2]**


_P_
�

_i=1_

_P_
�

_i=1_


�wi(β − **z[T]** **di) + wi[′][(][β][ +][ z][T][ d][i][)]�**

�wi(β − **z[T]** **di) + wi[′][(][β][ +][ z][T][ d][i][)]�**


2

_P_ _P_

1 � �
= _wimin,wi[′]_ _[≥][0]_ 2 **di(wi −** _wi[′][)][ −]_ **[y]** + _β (wi + wi[′][)]_

����� _i=1_ �����2 _i=1_

1
= min 2 [+][ β][∥][w][∥][1][ =][ P]2[cvx] (26)
**w∈R[P]** 2 _[∥][Dw][ −]_ **[y][∥][2]**

where, in equality (i) follows from the fact that D2[cvx] is a convex optimization problem satisfying
Slater’s conditions so that strong duality holds (Boyd & Vandenberghe, 2004), i.e., D2[cvx] = P2[cvx],
and the matrix D 0, 1 is defined as
_∈{_ _}[n][×][P]_

**D** := [d1, d, . . ., dP ].

Now, based on the strong duality results in Ergen & Pilanci (2020); Pilanci & Ergen (2020), there
exist a threshold for the number of neurons, i.e., denoted as m[∗], such that if m ≥ _m[∗]_ then strong
duality holds for the original non-convex training problem (25), i.e., P2[noncvx] = D2[cvx] = P2[cvx]. Thus,
(26) is exactly equivalent to the original non-convex training problem (3).

A.4 THEOREM 2.3

**_Proof of Theorem 2.3. Following the proof of Theorem 2.2, we first note that strong duality holds_**
for the problem in (25), i.e., P2[noncvx] = D2[cvx].

Under the assumption that H(X) = {0, 1}[n], the dual constraint maxd∈H(X) |z[T] **d| ≤** _β is equivalent_
to {maxd∈[0,1]n z[T] **d ≤** _β} ∪{maxd′∈[0,1]n −z[T]_ **d[′]** _≤_ _β}. Forming the Lagrangian based on this_
reformulation of the dual constraint, we have

1
_D2[cvx]_ = maxz∈R[n][ min]t,t[′]≥0 2 _[∥][y][∥]2[2]_ _[−]_ [1]2 _[∥][z][ −]_ **[y][∥]2[2]** [+][ t][(][β][ −] **d∈max[0,1][n][ z][T][ d][) +][ t][′][(][β][ +]** **d[′]max∈[0,1][n][ z][T][ d][′][)]**

1

= max min 2 2 [+][ z][T][ (][t][′][d][′][ −] _[t][d][) +][ β][(][t][′][ +][ t][)]_
**z∈R[n]** _t,t[′]≥0_ 2 _[∥][y][∥][2]_ _[−]_ [1]2 _[∥][y][ −]_ **[z][∥][2]**
**d,d[′]∈[0,1][n]**

(i) 1
= max min 2 2 [+][ z][T][ (][d][′][ −] **[d][) +][ β][(][t][′][ +][ t][)]**
**z∈R[n]** _t,t[′]≥0_ 2 _[∥][y][∥][2]_ _[−]_ [1]2 _[∥][y][ −]_ **[z][∥][2]**
**d∈[0,t][n],d[′]∈[0,t[′]][n]**

(ii) 1
= min max 2 2 [+][ z][T][ (][d][′][ −] **[d][) +][ β][(][t][′][ +][ t][)]**
_t,t[′]≥0_ **z∈R[n]** 2 _[∥][y][∥][2]_ _[−]_ [1]2 _[∥][y][ −]_ **[z][∥][2]**
**d∈[0,t][n],d[′]∈[0,t[′]][n]**

1

= min 2 [+][ β][(][t][ +][ t][′][)][ .]
_t,t[′]≥0_ 2 _[∥][d][′][ −]_ **[d][ −]** **[y][∥][2]**
**d∈[0,t][n],d[′]∈[0,t[′]][n]**


where, in equality (i), we used the change of variables d ≡ _td and d[′]_ _≡_ _t[′]d[′], and, in equality (ii), we_
used the fact that the objective function [1]2 _[∥][y][∥]2[2]_ _[−]_ [1]2 _[∥][y][ −]_ **[z][∥]2[2]** [+][ z][T][ (][d][′][ −] **[d][) +][ β][(][t][′][ +][ t][)][ is strongly]**

concave in z and convex in (d, d[′], t, t[′]) and the constraints are linear, so that strong duality holds and
we can switch the order of minimization and maximization.


-----

Given a feasible point (d, d[′], t, t[′]), i.e., d [0, t][n] and d[′] [0, t[′]][n], we set δ = d[′] **d. Note that**
_∈_ _∈_ _−_
_∥(δ)+∥∞_ _≤∥(d[′])+∥∞_ = ∥d[′]∥∞ _≤_ _t[′]. Similarly, ∥(−δ)+∥∞_ _≤_ _t. This implies that_

1 1

min 2 [+][ β][(][t][ +][ t][′][)][ ≥] [min] 2 [+][ β][(][∥][(][δ][)][+][∥][∞] [+][ ∥][(][−][δ][)][+][∥][∞][) =][ P]v[cvx]2 _[.]_
_t,t[′]≥0_ 2 _[∥][d][′][ −]_ **[d][ −]** **[y][∥][2]** **_δ∈R[n]_** 2 _[∥][δ][ −]_ **[y][∥][2]**
**d∈[0,t][n],d[′]∈[0,t[′]][n]**

Conversely, given δ ∈ R[n], we set d = (δ)+, t = ∥d∥∞, d[′] = (−δ)+ and t[′] = ∥d[′]∥∞. It holds
that (d, d[′], t, t[′]) is feasible with same objective value, and consequently, the above inequality is an
equality, i.e., D2[cvx] = Pv[cvx]2 [.]

**Optimal threshold network construction: We now show how to construct an optimal threshold net-**
work given an optimal solution to the convex problem (7). Let δ ∈ R[n] be an optimal solution. Set d =
(δ)+ and d[′] = (−δ)+. We have d ∈ [0, ∥d∥∞][n] and d[′] _∈_ [0, ∥d[′]∥∞][n]. It is easy to show that we
can transform δ such that, for each index i [n], either the i-th coordinate of d is active or the i-th co_∈_
ordinate of d[′] is active. Therefore, by Caratheodory’s theorem, there exist n+, n− _≥_ 1 such that n− +
_∥nd+∥ ≤∞_ �n, andni=1++1 dγ1i, . . .,di, and, dn d++1[′]1[, . . .,] ∈{[ d]0,[′]n 1−}+1[n] and[∈{] γ[0]1[,][ 1], . . ., γ[}][n][ and]n[ γ]++11[′] _[, . . ., γ] ≥_ 0 such thatn[′] _−+1_ _[≥]_ [0][�][ such that]i[n]=1[+][+1] _γ[ �]i = 1i[n]=1[−][+1] andγi[′] d[= 1] =_

and d[′] = ∥d[′]∥∞ �ni=1−+1 _γi[′][d][′]i[, with][ n][−]_ [+][ n][+][ ≤] _[n][.]_ Then, we can pick w1[(1)][, . . .,][ w]n[(1)]++1[,]

_w1[(2)][, . . ., w]n[(2)]++1[, s][1][, . . ., s][n]+[+1][ such that][ 1][{][Xw]i[(1)]_ _≥_ 0} = di, wi[(1)] = ∥d∥∞γi and si = −1, and,

**wn[(1)]++2[, . . .,][ w]n[(1)]++n−+2[,][ w]n[(2)]++2[, . . ., w]n[(2)]++n−+2[, s][n]+[+2][, . . ., s][n]+[+][n]−[+2][ such that][ 1][{][Xw]i[(1)]** _≥_
0} = d[′]i−(n++1)[,][ α][i][ =][ ∥][d][′][∥][∞][γ][i][−][(][n]+[+1)][ and][ s][i][ = 1][.] Note that, given δ ∈ R[n], finding
the corresponding d1, . . ., dn++1, γ1, . . ., γn++1 and d[′]1[, . . .,][ d][′]n++1[, γ]1[′] _[, . . ., γ]n[′]_ ++1 [takes time]
_O(n+ + n+) = O(n)._

A.5 LEMMA 3.1

**_Proof of Lemma 3.1. We first restate the standard weight decay regularized training problem below_**


_L_
�

(∥Wk[(][l][)][∥]F[2] [+][ ∥][s]k[(][l][)][∥]2[2][)][.] (27)
_l=1_


1
minθ∈Θ 2 _[∥][f][θ,L][(][X][)][ −]_ **[y][∥]2[2]** [+][ β]2


_mL−1_
�

_k=1_


Then, we remark that the loss function in (27), i.e., [1]2 2 [is invariant to the norms of]

_[∥][f][θ,L][(][X][)][ −]_ **[y][∥][2]**
hidden layer weights {{Wk[(][l][)][}]l[L]=1[−][1][}][m]k=1[L][−][1] and amplitude parameters {{s[(]k[l][)][}][L]l=1[−][2][}]k[m]=1[L][−][1][. Therefore,]
(27) can be rewritten as


1
minθ∈Θ 2 _[∥][f][θ,L][(][X][)][ −]_ **[y][∥]2[2]** [+][ β]2


_mL−1_
�

_k=1_


�
(wk[(][L][)])[2] + (s[(]k[L][−][1)])[2][�] _._ (28)


Then, we can directly apply the scaling in the proof of Lemma 2.1 to obtain the claimed ℓ1-norm
regularized optimization problem.

A.6 THEOREM 3.2

**_Proof of Theorem 3.2. We first remark that the last hidden layer activations can only be either zero_**
or one, i.e., X[(]k[L][−][1)] _∈{0, 1}[n][×][m][L][−][1]_ _, since |s[(]k[L][−][1)]| = 1, ∀k ∈_ [mL−1]. Therefore, based on
Lemma 3.1, we reformulated (12) as


� 2
**w** **y** (29)
_−_ ���2 [+][ β][∥][w][∥][1][,]


_PL[noncvx]_ = min
**dij ∈HwL(X)**


1
2


�
��� **di1** _, ..., dimL−1_


As in (25), the above problem is similar to Lasso, although it is non-convex due to the discrete
variables di1 _, ..., dimL−1_ . These mL−1 hyperplane arrangement patterns are discrete optimization
variables along with the coefficient vector w.

We now note that (29) has following dual problem

_DL[cvx]_ = D2[cvx] = max 2 [+ 1] 2[.] (30)
maxd∈HL (X) |z[T] **d|≤β** _[−]_ 2[1] _[∥][z][ −]_ **[y][∥][2]** 2 _[∥][y][∥][2]_


-----

Since the above dual form is the same with the dual problem of two-layer networks, i.e., DL[cvx] = D2[cvx],
we directly follow the proof of Theorem 2.2 to obtain the following bidual formulation.


1
_DL[cvx]_ = PL[cvx] = min
**w∈R[PL][−][1]** 2

where D[(][L][−][1)] = [d1, d, . . ., dPL−1 ].


2
**D(L−1)w** **y**
��� _−_ ���2 [+][ β][∥][w][∥][1][,]


Hence, based on the strong duality results in Ergen & Pilanci (2020); Pilanci & Ergen (2020); Ergen
& Pilanci (2021c), there exist a threshold for the number of neurons, i.e., denoted as m[∗], such that
if mL−1 ≥ _m[∗]_ then strong duality holds for the original non-convex training problem (25), i.e.,
_PL[noncvx]_ = DL[cvx] = PL[cvx][.]

A.7 LEMMA 3.3

**_Proof of Lemma 3.3. Based on the construction in (Bartlett et al., 2019), a deep network with L_**
layers and W parameters, where W = [�]i[L]=1 _[m][i][−][1][m][i][ in our notation, can shatter a dataset of]_
the size C1WL log(W ), where C1 is a constant. Based on this complexity results, if we choose
_ml = m = C[√]n/L, ∀l ∈_ [L], the number of samples that can be shattered by an L-layer threshold
network is

# of data samples that can be shattered = C1WL log(W )

= C1m[2]L[2] log(m[2]L)


� _C_ 2n
= C1C [2]n log

_L_


�


� _n_
= n log

_LC1_


�
_n_
_≫_


provided that n _L, which is usually holds in practice and we choose the constant C such that_
_≫_
_C = 1/C1[2][. Therefore, as we make the architecture deeper, we can in fact improve the assumption]_
_m_ _n + 2 assumption in the two-layer networks case by benefiting from the depth._
_≥_

A.8 LEMMA 3.5

**_Proof of Proposition 3.5. Let us start with the L = 2 case for which hyperplane arrangement set_**
can described as detailed in (5)

_H2(X) = {1{Xw[(1)]_ _≥_ 0} : θ ∈ Θ}
= (X)
_H_
= {d1, . . ., dP1 _} ⊂{0, 1}[n]._

Now, in order to construct H3(X) (with mL−1 = 1 so we drop the index k), we need to choose m1
arrangements from H2(X) and then consider all hyperplane arrangements for each of these choices.
Particularly, since |H2(X)| = P1, where


�r
(n[r])
_≈O_


_P1 ≤_ 2


_r−1_
�

_k=0_


�n 1� � _e(n_ 1)
_−_ _−_

2r
_≤_

_k_ _r_


from (8), we have �mP11� choices and each of them yields a different activation matrix denoted as

**X[(1)]i** _∈_ R[n][×][m][1] . Based on the upperbound (8), each X[(1)]i can generate O(n[m][1] ) patterns.

Therefore, overall, the set of possible arrangement in the second layer is as follows


(mP11[)]
_H3(X) =_ _i�=1_ _{1{X[(1)]w[(2)]_ _≥_ 0} : θ ∈ Θ}���X[(1)]=X[(1)]i


-----

which implies that


� _P1_
_|H3(X)| = P2 ≲_
_m1_


�
(n[m][1] ) (n[m][1][r]).
_O_ _≈O_


This analysis can be recursively extended to the l[th] layer where the hyperplane arrangement set and
the corresponding cardinality values can be computed as follows

(mlPl−−22[)]

_Hl(X) =_ _i�=1_ _{1{X[(][l][−][2)]w[(][l][−][1)]_ _≥_ 0} : θ ∈ Θ}���X[(][l][−][2)]=X[(]i[l][−][2)]

_|Hl(X)| = Pl−1 ≲_ � _Pl−2_ �O(n[m][l][−][2] ) ≈O(n[r][ �]k[l][−]=1[2] _[m][k]_ ).
_ml−2_

A.9 COROLLARY 3.4


**_Proof of Corollary 3.4. We first remark that the last hidden layer activations can only be either_**
zero or one, i.e., X[(]k[L][−][1)] _∈{0, 1}[n][×][m][L][−][1]_ _, since |s[(]k[L][−][1)]| = 1, ∀k ∈_ [mL−1]. Therefore, based on
Lemma 3.1, we reformulated (12) as


� 2
**w** **y** (31)
_−_ ���2 [+][ β][∥][w][∥][1][,]


min
**dij ∈HwL(X)**


1
2


�
��� **di1** _, ..., dimL−1_


As in (25), the above problem is similar to Lasso, although it is non-convex due to the discrete
variables di1 _, ..., dimL−1_ . These mL−1 hyperplane arrangement patterns are discrete optimization
variables along with the coefficient vector w.

We now note that (31) has following dual problem

max 2 [+ 1] 2[.]
maxd∈HL (X) |z[T] **d|≤β** _[−]_ 2[1] _[∥][z][ −]_ **[y][∥][2]** 2 _[∥][y][∥][2]_


Since HL(X) = {0, 1}[n] by Lemma 3.3, all the steps in the proof of Theorem 2.3 directly follow.

**Optimal deep threshold network construction: For the last two layers’ weights, we exactly follow**
the weight construction procedure in the Proof of Theorem 2.3 as detailed below.

Let δ ∈ R[n] be an optimal solution. Set d = (δ)+ and d[′] = (−δ)+. We have d ∈ [0, ∥d∥∞][n] and
**d[′]** _∈_ [0, ∥d[′]∥∞][n]. It is easy to show that we can transform δ such that, for each index i ∈ [n], either
the i-th coordinate of d is active or the i-th coordinate of d[′] is active. Therefore, by Caratheodory’s
theorem, there exist n+, n− _≥_ 1 such that n− + n+ ≤ _n, and d1, . . ., dn++1 ∈{0, 1}[n]_ and
_γ1, . . ., γn++1 ≥_ 0 such that [�]i[n]=1[+][+1] _γi = 1 and d = ∥d∥∞_ �ni=1++1 _γidi, and, d[′]1[, . . .,][ d][′]n−+1_ _[∈]_

_{0, 1}[n]_ and γ1[′] _[, . . ., γ]n[′]_ _−+1_ _[≥]_ [0][ such that][ �]i[n]=1[−][+1] _γi[′]_ [= 1][ and][ d][′][ =][ ∥][d][′][∥][∞] �in=1−+1 _γi[′][d][′]i[, with]_

_n−_ + n+ ≤ _n. Then, we can pick w1[(][L][−][1)], . . ., wn[(][L]+[−]+1[1)][,][ w]1[(][L][)], . . ., wn[(][L]+[)]+1[, s][(]1[L][−][1)], . . ., s[(]n[L]+[−]+1[1)]_ [such]

that 1{X[(][L][−][2)]wi[(][L][−][1)] _≥_ 0} = di, wi[(][L][−][1)] = ∥d∥∞γi and si = −1, and, wn[(][L]+[−]+2[1)][, . . .,][ w]n[(1)]++n−+2[,]

_wn[(][L]+[)]+2[, . . ., w]n[(][L]+[)]+n−+2[, s][(]n[L]+[−]+2[1)][, . . ., s][(]n[L]+[−]+[1)]n−+2_ [such that][ 1][{][X][(][L][−][2)][w]i[(][L][−][1)] _≥_ 0} = d[′]i−(n++1)[,]
_αi = ∥d[′]∥∞γi−(n++1) and si = 1._ Note that, given δ ∈ R[n], finding the corresponding
**d1, . . ., dn++1, γ1, . . ., γn++1 and d[′]1[, . . .,][ d][′]n++1[, γ]1[′]** _[, . . ., γ]n[′]_ ++1 [takes time][ O][(][n][+] [+] _[n][+][) =][ O][(][n][)][.]_

Then, the rest of the layer weights can be reconstructed using the construction procedure detailed in
(Bartlett et al., 2019).


-----

A.10 COROLLARY 4.1

**_Proof of Corollary 4.1. We first apply the scaling in Lemma 3.1 and then follow the same steps in_**
the proof of Theorem 2.3 to get the following dual problem

max (32)
maxd∈H(X) |z[T] **d|≤β** _[−L][∗][(][z][)][,]_

where is the Fenchel conjugate of and defined as
_L[∗]_ _L_

(z):= max **v[T]** **z** (v, y).
_L[∗]_ **v** _−L_

Therefore, we obtain a generic version of the dual problem in (30), where we can arbitrarily choose
the network’s depth and the convex loss function. Then the rest of the proof directly follows from
Theorem 3.2 and Corollary 3.4.

#### B ADDITIONAL EXPERIMENTS AND DETAILS

In this section, we present additional numerical experiments and further experimental details that are
not presented in the main paper due to the page limit.

We first note that all of the experiments in the paper are run on a single laptop with Intel(R) Core(TM)
i7-7700HQ CPU and 16GB of RAM.

B.1 EXPERIMENT IN FIGURE 5

Figure 5: Comparison of our convex training method in (6) with standard non-convex training
heuristic for threshold networks, known as Straight-Through Estimator (STE) (Bengio et al., 2013).
For the non-convex heuristic, we repeat the training process using 5 independent initializations,
however, all trials fail to converge to the global minimum obtained by our convex optimal method,
and lack stability. We provide experimental details in Appendix B.

For the experiment in Figure 5, we consider a simple one dimensional experiment, where the data
matrix is X = [ 2, 1, 0, 1, 2][T] . Using this data matrix, we generate the corresponding labels y
_−_ _−_
by simply forward propagating the data through randomly initialized two-layer threshold networks
with m1 = 2 neurons as described in (2). We then run our convex training method in (6) and the
non-convex training heuristic STE (Bengio et al., 2013) on the objective with the regularization
coefficient β = 1e − 2. For a fair comparison, we used P1 = 24 for the convex method and
_m1 = 24 for STE. We also tune the learning rate of STE by performing a grid search on the set_
5e 1, 1e 1, 5e 2, 1e 2, 5e 3, 1e 3 . As illustrated in Figure 5, the non-convex training
_{_ _−_ _−_ _−_ _−_ _−_ _−_ _}_
heuristic STE fails to achieve the global minimum obtained by our convex training algorithm for 5
different initialization trials.

B.2 EXPERIMENT IN TABLE 2

Here, we provide a test performance comparison on on CIFAR-10 (Krizhevsky et al., 2014), MNIST
(LeCun), and the datasets taken from UCI Machine Learning Repository (Dua & Graff, 2017), where


-----

Table 3: Dataset sizes for the experiments in Table 2.

##### Dataset n d

 CIFAR-10 10000 3072 MNIST 12665 784 bank 4521 16 chess-krvkp 3196 36 mammographic 961 5 oocytes-4d 1022 41 oocytes-2f 912 25 ozone 2536 72 pima 768 8 spambase 4601 57 statlog-german 1000 24 tic-tac-toe 958 9 titanic 2201 3

we follow the preprocesing in Fernandez-Delgado et al.´ (2014) such that 750 _n_ 5000 (see
_≤_ _≤_
Table 3 for the exact dataset sizes). We particularly consider a conventional binary classification
framework with two-layer networks and performed simulations over multiple seeds and compare
the mean/std of the test accuracies of non-convex heuristics trained with SGDnamely Nonconvex**STE (Bengio et al., 2013), Nonconvex-ReLU (Yin et al., 2019a), Nonconvex-LReLU (Xiao et al.)**
and Nonconvex-CReLU (Cai et al., 2017), with our convex program in (6), i.e., Convex-Lasso.
For the non-convex training, we use the SGD optimizer. We also use the 80% 20% splitting
_−_
ratio for the training and test sets of the UCI datasets. We tune the regularization coefficient
_β and the learning rate µ for the non-convex program by performing a grid search on the sets_
_βlist = {1e −_ 6, 1e − 3, 1e − 2, 1e − 1, 0.5, 1, 5} and µlist = {1e − 3, 5e − 3, 1e − 2, 1e − 1},
respectively. In all experiments, we also decayed the selected learning rate systematically using
PyTorch’s (Paszke et al., 2019) scheduler ReduceLROnPlateau. Moreover, we choose the number
of neurons, number of epochs (ne), batch size (bs) as m = 1000, bs = 5000, bs = n, respectively.
Our convex approach achieves highest test accuracy for precisely 9 of 13 datasets whereas the best
non-convex heuristic achieves the highest test accuracy only for 4 datasets. This experiment verifies
that our convex training approach not only globally optimize the training objective but also usually
generalizes well on the test data. In addition, our convex training approach is shown to be significantly
more time efficient than standard non-convex training.

(a) (n, d) = (20, 100) (b) (n, d) = (50, 50) (c) (n, d) = (100, 20)

Figure 6: Training performance comparison of our convex program for two-layer networks in (6)
with four standard non-convex training heuristics (STE and its variants in Section 5). To generate the
dataset, we randomly initialize a two-layer network and then sample a random i.i.d. Gaussian data
matrix X ∈ R[n][×][d]. Then, we set the labels as y = sgn(tanh(XW[(1)])w[(2)]) where sgn and tanh
are the sign and hyperbolic tangent functions. We specifically choose the regularization coefficient
_β = 1e_ 3 and run the training algorithms with m = 1000 neurons for different (n, d) combinations.
_−_
For each non-convex method, we repeat the training with 5 different initialization and then plot the
best performing non-convex method for each initialization trial. In each case, our convex training
algorithm achieves significantly lower objective than all the non-convex heuristics. We also indicate
the time taken to solve the convex program with a marker.


-----

(a) (b) (c)

Figure 7: In this figure, we compare the classification performance of the simulation in Figure 6c
for a single initialization trial, where n = 100 and d = 20. This experiment shows that our convex
training approach not only provides the optimal training performance but also generalizes on the test
data as demonstrated in (c).

(a) (b) (c)

Figure 8: The setup for these figures is completely the same with Figure 7 except that we consider
the (n, d) = (50, 50) case in Figure 6b. Unlike Figure 7, here even though our convex training
approach provides the optimal training, the non-convex heuristic methods that are stuck at a local
minimum yield a better test accuracy. This is due to the fact that we have less data samples with
higher dimensionality compared to Figure 7.

B.3 TWO-LAYER EXPERIMENTS IN FIGURE 6, 7, 8, AND 9

Here, we compare two-layer threshold network training performance of our convex program (6),
which we call Convex-Lasso, with standard non-convex training heuristics, namely Nonconvex-STE
(Bengio et al., 2013), Nonconvex-ReLU (Yin et al., 2019a), Nonconvex-LReLU (Xiao et al.) and
**Nonconvex-CReLU (Cai et al., 2017). For the non-convex heuristics, we train a standard two-**
layer threshold network with the SGD optimizer. In all experiments, learning rates are initialized
to be 0.01 and they are decayed systematically using PyTorch’s (Paszke et al., 2019) scheduler
ReduceLROnPlateau. To generate a dataset, we first sample an i.i.d. Gaussian data matrix X and
then obtain the corresponding labels as y = sgn(tanh(XW[(1)])w[(2)]) where sgn and tanh are the
sign and hyperbolic tangent functions, respectively. Here, we denote the the ground truth parameters
as W[(1)] _∈_ R[d][×][m][∗] and w[(2)] _∈_ R[m][∗], where m[∗] = 20 is the number of neurons in the ground truth
model. Notice that we use tanh and sign in the ground truth model to have balanced label distribution
**y** +1, 1 . We also note that for all the experiments, we choose the regularization coefficient
_∈{_ _−_ _}[n]_
as β = 1e 3.
_−_

We now emphasize that to have a fair comparison with the non-convex heuristics, we first randomly
sample a small subset of hyperplane arrangements and then solve (6) with this fixed small subset.
Specifically, instead of enumerating all possible arrangements {di}i[P]=1[, we randomly sample a]
subset {dij _}j[m]=1_ [to have a fair comparison with the non-convex neural network training with][ m]
hidden neurons. So, Convex-Lasso is an approximate way to solve the convex program (6) yet it
still performs extremely well in our experiments. We also note that the other convex approaches
**Convex-PI and Convex-SVM exactly solve the proposed convex programs.**

In Figure 6, we compare training performances. We particularly solve the convex optimization
problem (6) once. However, for the non-convex training heuristics, we try five different initializations.


-----

(a) (b) (c)

Figure 9: The setup for these figures is completely the same with Figure 7 except that we consider the
(n, d) = (20, 100) case in Figure 6a. As in Figure 8, even though the non-convex heuristic training
methods fail to globally optimize the objective, they yield higher test accuracies than the convex
program due to the low data regime (n _d)._
_≤_

We then select the trial with the lowest objective value to plot. In the figure, we plot the objective
value defined in (3). As it can be seen from the figures, Convex-Lasso achieves much lower objective
value than the non-convex heuristics in three different regimes, i.e., overparameterized (n < d),
underparameterized (n > d) and moderately parameterized ((n = d)). This is mainly because of the
non-convex and heuristic nature of the standard optimizers.

Moreover, we illustrate training and test accuracies of these training algorithms in Figure 7, 8, and
9 for different (n, d) combinations. To generate the test data with 3000, samples we use the same
ground truth model defined above. Here we observe that in some cases even though Convex-Lasso
provides a better training performance, it might yield worse test accuracies than the non-convex
heuristic especially in the low data regime (n _d). Therefore, understanding the generalization_
_≤_
performance and optimal weight reconstruction (discussed in Section 2.4) remains to be an open
problem.


(a) (n, d, m1, m2) =
(20, 100, 1000, 20)


(b) (n, d, m1, m2) =
(50, 50, 1000, 50)


(c) (n, d, m1, m2) =
(100, 20, 1000, 100)


Figure 10: Training comparison of our convex program in (7) with three-layer threshold networks
trained with the non-convex heuristics. Here, we directly follow the setup in Figure 6 except the
following differences. This time we randomly initialize a three-layer network and then sample i.i.d.
Gaussian data matrix. We then set the labels as y = sgn(tanh(tanh(XW[(1)])W[(2)])w[(3)]). To have
our convex formulation, we require (X) to be complete. Thus, we first use a random representation
_H_
matrix H ∈ R[d][×][M], where M = 1000, and then apply the transformation **X[˜]** = σ(XH). We also
apply this transformation to the non-convex methods as if we train a two-layer networks on the
modified data matrix **X[˜]** . We also indicate the time taken to solve the convex programs with markers.
We again observe that our convex training approach achieves lower objective value than all the
non-convex heuristic training methods in all initialization trials.

B.4 THREE-LAYER EXPERIMENTS WITH A REPRESENTATION MATRIX IN FIGURE 10, 11, 12,
AND 13

We also compare our alternative convex formulation in (7) with non-convex approaches for three-layer
network training. To do so we first generate a dataset via the following ground truth model y =


-----

(a) (b) (c)

Figure 11: In this figure, we compare the classification performance of the simulation in Figure 10c
for a single initialization trial, where (n, d) = (100, 20). This experiment shows that our convex
training approach not only provides the optimal training performance but also generalizes on the test
data as demonstrated in (c).

(a) (b) (c)

Figure 12: The setup for these figures is completely the same with Figure 11 except that we consider
the (n, d) = (50, 50) case in Figure 10b. This experiment shows that when we have all possible
hyperplane arrangements, our convex training approach generalizes better even in the low data regime
(n _d)._
_≤_

sgn(tanh(tanh(XW[(1)])W[(2)])w[(3)]), where W[(1)] _∈_ R[d][×][m]1[∗] _, W[(2)]_ _∈_ R[m]1[∗][×][m][∗]2 and w[(3)] _∈_ R[m]2[∗]
and we choose m[∗]1 [=][ m]2[∗] [= 20][ for all the experiments. As it is described in Theorem][ 2.3][, we require]
(X) to be complete in this case. To ensure that, we transform the data matrix X using a random
_H_
representation matrix. In particular, we first generate a random representation matrix H ∈ R[d][×][M]
and then multiply it with the data matrix followed by a threshold function. Therefore, effectively, we
obtain a new data matrix **X[˜]** = σ(XH). By choosing M large enough, which is M = 1000 in our
experiments, we are able to enforce **X[˜]** to be full rank and thus the set of hyperplane arrangements
( X[˜] ) is complete as assumed in Theorem 2.3. We also apply the same steps for the non-convex
_H_
training, i.e., we train the networks as if we perform a two-layer network training on the data matrix
**X˜** . Notice that we also provide standard three-layer network training comparison, where all of three
layers are trainable, in Section B.5. More importantly, after solving the convex problem (7), we
need to construct a neural network that gives the same objective value. As discussed in Section
2.4, there are numerous ways to reconstruct the non-convex network weights and we particularly
use Convex-PI and Convex-SVM as detailed in Section 5, which seem to have good generalization
performance. We also note that these approaches are exact in the sense that they globally optimize
the objective in B.3. Additionally, we have n neurons in our reconstructed neural network to have
fair comparison with the non-convex training with n hidden neurons.

In Figure 10, we compare objective values and observe that Convex-PI and Convex-SVM, which
solve the convex problem 7, obtain the globally optimal objective value whereas all trials of the
non-convex training heuristics are stuck at a local minimum. Figure 11, 12, and 13 show that our
convex training approaches also yield better test performance in all cases unlike the two-layer training
in Section B.3. Again, for the testing phase we generate 3000 samples via the ground truth model
defined above.


-----

(a) (b) (c)

Figure 13: The setup for these figures is completely the same with Figure 11 except that we consider
the (n, d) = (20, 100) case in Figure 10a. This experiment also confirms better generalization of our
convex training approach as in Figure 11 and 12.

B.5 STANDARD THREE-LAYER EXPERIMENTS IN FIGURE 1 AND 2

Here, we compare our convex program in (7) with 3-layer networks trained with non-convex
heuristics. Similar to the case in Section B.4 we generate a dataset via the ground truth model
**y = sgn(tanh(tanh(XW[(1)])W[(2)])w[(3)]), where X ∈** R[n][×][d], W[(1)] _∈_ R[d][×][m]1[∗] _, W[(2)]_ _∈_ R[m]1[∗][×][m]2[∗]
and w[(3)] _∈_ R[m]2[∗] and we choose m[∗]1 [=][ m]2[∗] [= 20][ for all the experiments.]

In contrast to Section B.4, we use a representation matrix H ∈ R[d][×][M], where M = 1000, and
then apply the transformation **X[˜]** = σ(XH) for the convex methods. We perform standard training
procedure for three-layer non-convex networks, i.e., we train a fully three-layer network without any
sort of representation matrix. To have fair comparison, we choose m1 = M and m2 = n for both
non-convex and convex settings.

In Figure 1, we compare the objective values and observe that our convex training approaches achieve
a global optimum in all cases unlike the non-convex training heuristics. We also provide the test and
training accuracies for three-layer networks trained with different (n, d) pairs in Figure 2. In all cases
our convex approaches outperform the non-convex heuristics in terms of test accuracy.

(a) (b) (c)

Figure 14: Performance comparison of our convex training approaches trained via (7) and non-convex
training heuristics for a 10-layer threshold network training problem. Here, we use the same setup
in Figure 1. As in the previous experiment, our convex training approaches yields outperforms the
non-convex heuristics in both training and test metrics.

B.6 STANDARD TEN-LAYER EXPERIMENTS IN FIGURE 14

In order to verify the effectiveness of the proposed convex formulations in training deeper networks,
we consider a threshold network training problem for a 10-layer network, i.e., (10) with L = 10. We
directly follow the same setup with Section B.5 for (n, d, β) = (100, 20, 1e 3). Since in this case
_−_
the network is significantly deeper than the previous cases, all of the non-convex training heuristics
failed to fit the training data, i.e., they couldn’t achieve 100% training accuracy. Therefore, for the
non-convex training, we also include Batch Normalization in between hidden layers to stabilize and
improve the training performance. In Figure 14, we present the results for this experiment. Here, we
observe that our convex training approaches provide a globally optimal training performance and


-----

yield higher test accuracies than all of the non-convex heuristics that are further supported by Batch
Normalization.


-----


# WOULD DECENTRALIZATION HURT GENERALIZATION?

**Anonymous authors**
Paper under double-blind review

### ABSTRACT

Decentralized stochastic gradient descent (D-SGD) allows collaborative learning
on massive devices without the control of a central server. Existing theory suggests
that decentralization degrades generalizability, which conflicts with experimental
results in large-batch settings that D-SGD generalizes better than centralized SGD
(C-SGD). This work presents a new theory that reconciles the conflict between
the two perspectives. We prove that D-SGD introduces an implicit regularization
that simultaneously penalizes (1) the sharpness of the learned minima and (2) the
consensus distance between the global averaged model and local models. We then
prove that the implicit regularization is amplified in large-batch settings when
the linear scaling rule is applied. We further analyze the escaping efficiency of
D-SGD and show that D-SGD favors super-quadratic flat minima. Experiments
are in full agreement with our theory. The code will be released publicly. To our
best knowledge, this is the first work on the implicit regularization and escaping
efficiency of D-SGD.

### 1 INTRODUCTION

Decentralized stochastic gradient descent (D-SGD) enables simultaneous model training on massive
workers without being controlled by a central server, where every worker communicates only with its
directly connected neighbors (Xiao & Boyd, 2004; Lopes & Sayed, 2008; Nedic & Ozdaglar, 2009;
Lian et al., 2017; Koloskova et al., 2020). This decentralization avoids the requirements of a costly
central server with heavy communication and computation burdens. Despite the absence of a central
server, existing theoretical results demonstrate that the massive models on the edge converge to a
unique steady consensus model (Shi et al., 2015; Lian et al., 2017; Lu et al., 2011), with asymptotic
linear speedup in convergence rate (Lian et al., 2017) as the distributed centralized SGD (C-SGD)
does (Dean et al., 2012; Li et al., 2014). Consequently, D-SGD offers a promising distributed learning
solution with significant advantages in privacy (Nedic, 2020), scalability (Lian et al., 2017), and
communication efficiency (Ying et al., 2021b).

However, existing theoretical studies show that the decentralization nature of D-SGD introduces an
additional positive term into the generalization error bounds, which suggests that decentralization
may hurt generalization (Sun et al., 2021; Zhu et al., 2022). This poses a crippling conflict with
empirical results by Zhang et al. (2021) which show that D-SGD generalizes better than C-SGD by a
large margin in large batch settings; see Figure 1. This conflict signifies that the major characteristics
were overlooked in the existing literature. Therefore,

_would decentralization hurt generalization?_

This work reconciles the conflict. We prove that decentralization introduces implicit regularization in D-SGD, which promotes the generalization. To our best knowledge, this is the first paper
that surprisingly shows the advantages of D-SGD in generalizability, which redresses the former
misunderstanding. Specifically, our contributions are in twofold.

- We prove that the mean iterate of D-SGD closely follows the path of C-SGD on a regularized loss,
which is the addition of the original loss and a regularization term introduced by decentralization.
This regularization term penalizes the largest eigenvalue of the Hessian matrix, as well as the
consensus distance (see Theorem 1). These regularization effects are shown to be considerably
amplified in large-batch settings (see Theorem 2), which is consistent with our visualization (see


-----

0.9

0.8


0.9

0.8


0.9

0.8


0.7

0.6


0 2000 4000 6000


0.7


0.7


0 2000 4000 6000


0 2000 4000 6000

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
||||D_1024 D_8196|
|||D-SG D-SG|D_1024 D_8196|
|||C-SG C-SG|D_1024 D_8196|
|||||
|0|2000 4000 60 iteration|||

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
|||D-SG D-SG C-SG|D_1024 D_8196 D_1024||
|||C-SG|D_8196||
||||||
|0|2000 4000 60 iteration||||

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
|||D-SG D-SG C-SG|D_1024 D_8196 D_1024||
|||C-SG|D_8196||
||||||
|0|2000 4000 60 iteration||||


(a) AlexNet


(b) ResNet-18


(c) ResNet-34


Figure 1: Comparison of the validation accuracy of C-SGD and D-SGD on CIFAR-10. The number
of workers (one GPU as a worker) is set as 16; and the local batch size is set as 64, and 512 per
worker (1024 and 8196 total batch size). The training setting is included in Section 5.

Figure 4) and the empirical results in (Zhang et al., 2021). To prove the above results, we apply
second-order multivariate Taylor approximation (Konigsberger¨, 2013) on the gradient diversity (see
Equation (5)) to derive the regularized loss. Then, we prove that the regularization term contained
in the regularized loss scales positively with the largest Hessian eigenvalue, which suggests that
D-SGD implicitly minimizes the sharpness of the learned minima (see Lemma C.2).



- We prove the first result on the expected escaping speed of D-SGD from local minima (see

Theorem 3). Our results show that D-SGD prefers super-quadratic flat minima to sub-quadratic
minima with higher probability (see Proposition 4). The proof is based on the construction of a
stochastic differential equation (SDE) approximation (Jastrzebski et al., 2017; M et al., 2017; Li
et al., 2021) of D-SGD.

### 2 RELATED WORK


**Flatness and generalization. The flatness of minimum is a commonly used concept in the opti-**
mization and machine learning literature and has long been regarded as a proxy of generalization
(Hochreiter & Schmidhuber, 1997; Izmailov et al., 2018; Jiang et al., 2020). Intuitively, the loss
around a flat minimum varies slowly in a large neighborhood, while a sharp minimum increases
rapidly in a small neighborhood (Hochreiter & Schmidhuber, 1997). Through the lens of the minimum description length theory (Rissanen, 1983), flat minimizers tend to generalize better than sharp
minimizers, since they are specified with lower precision (Keskar et al., 2017). From a Bayesian perspective, sharp minimizers have posterior distributions highly concentrated around them, indicating
that they are more specialized on the training set and thus are less robust to data perturbations than
flat minimizers (MacKay, 1992; Chaudhari et al., 2019).

**Generalization of large-batch training. Large-batch training is of significant interest for deep**
learning deployment, which can contribute to a significant speed-up in training neural networks
(Goyal et al., 2017; You et al., 2018; Shallue et al., 2019). Unfortunately, it is widely observed that
in the centralized learning setting, large-batch training often suffers from a drastic generalization
degradation, even with fine-tuned hyper-parameters, from both empirical (Chen & Huo, 2016; Keskar
et al., 2017; Hoffer et al., 2017; Shallue et al., 2019; Smith et al., 2020) and theoretical (Li et al.,
2021) aspects. An explanation of this phenomenon is that large-batch training leads to “sharper”
minima (Keskar et al., 2017), which are more sensitive to perturbations (Hochreiter & Schmidhuber,

1997).

**Development of D-SGD. The earliest work of classical decentralized optimization can be traced**
back to Tsitsiklis (1984), Tsitsiklis et al. (1986) and Nedic & Ozdaglar (2009). D-SGD, a typical
decentralized optimization algorithm, has been extended to various settings in deep learning, including
time-varying topologies (Lu & Wu, 2020; Koloskova et al., 2020), asynchronous settings (Lian et al.,
2018; Xu et al., 2021; Nadiradze et al., 2021), directed topologies (Assran et al., 2019; Taheri et al.,
2020), and data-heterogeneous scenarios (Tang et al., 2018; Vogels et al., 2021).

**Generalization of D-SGD. Recently, Sun et al. (2021) and Zhu et al. (2022) have established**
generalization bounds of D-SGD and have shown that decentralized training hurts generalization.


-----

Figure 2: An illustration of C-SGD and D-SGD.

However, these works do not analyze the sharpness reduction effect of D-SGD and cannot explain
why D-SGD can generalize better than C-SGD in large batch settings. Another work by Zhang
et al. (2021) demonstrates that D-SGD introduces an “additional” landscape-dependent noise, which
improves the convergence of D-SGD. However, the direction, magnitude, and shape of the noise
remain unexplored. In contrast, we rigorously prove that the additional noise of D-SGD (i.e., the
gradient diversity in Equation (4)) biases the trajectory of D-SGD towards flatter minima, which may
play a distinct role in shaping the generalizability of D-SGD.

### 3 PRELIMINARIES

Suppose that X ⊆ R[d][x] and Y ⊆ R are the input and output spaces, respectively. We denote the
training set as µ = {z1, . . ., zN _}, where zζ = (xζ, yζ), ζ = 1, . . ., N are sampled independent_
and identically distributed (i.i.d.) from an unknown data distribution defined on = .
_D_ _Z_ _X × Y_
The goal of supervised learning is to learn a predictor (hypothesis) g( ; w), parameterized by w =

_·_
**w(z1, z2, . . ., zN** ) ∈ R[d], to approximate the mapping between the input variable x ∈X and the
output variable y ∈Y, based on the training set µ. Let c : Y × Y �→ R[+] be a function that evaluates
the prediction performance of hypothesis g. The loss of a hypothesis g with respect to (w.r.t.) the
example zζ = (xζ, yζ) is denoted by L(w; zζ) = c(g(xζ; w), yζ), which measures the effectiveness
of the learned model. Then, the empirical and population risks of w are defined as follows:


**_L[µ](w) = [1]_**

_N_


_N_
�

**_L(w; zζ), L(w) = Ez∼D[L(w; z)]._**
_ζ=1_


**Distributed learning. Distributed learning jointly trains a learning model w on multiple workers**
(Shamir & Srebro, 2014). In this framework, the j-th worker (j = 1, . . ., m) can access |µj|
independent and identically distributed (i.i.d.) training examples µj = {zj,1, . . ., zj,|µj _|}, drawn_
from the data distribution . In this case, the global empirical risk of w is
_D_


**_L[µ](w) = [1]_**

_m_


_m_
�

**_L[µ][j]_** (w),

_j=1_


where L[µ][j] (w) = _|µ1j_ _|_ �ζ|µ=1j _|_ **_[L][(][w][;][ z][j,ζ][)][ denotes the local empirical risk on the][ j][-th worker and]_**

_zj,ζ ∈_ _µj (ζ = 1, . . ., |µj|) stands for the local training data._

**Distributed centralized stochastic gradient descent (C-SGD).[1]** In C-SGD, there is only one
centralized model wa(t). C-SGD (Dean et al., 2012; Li et al., 2014) updates the model by


Local gradient computation
� �� �
_∇L[µ][j]_ [(][t][)](wa(t)), (1)


**wa(t+1) = wa(t) −** [1]

_m_


_m_
�

_η_
_·_
_j=1_


where η denotes the learning rate, µj (t) = {zj,1, . . ., zj,|µj (t)|} denotes the local training batch
independent and identically distributed (i.i.d.) drawn from the data distribution at the t-th iteration,
_D_

1The word “centralized” indicates that in C-SGD, there is a central server receiving gradient information
from local workers (see Figure 2).


-----

and ∇L[µ][j] [(][t][)](w) = _|µj1(t)|_ �|ζµ(tj)=1(t)| _[∇][L][(][w][;][ z][j,ζ][(][t][)][)][ stands for the local mini-batch gradient of][ L]_

w.r.t. the first argument w. The total batch size of C-SGD at t-th iteration is |µ(t)| = [�]j[m]=1 _[|][µ][j]_ [(][t][)][|][.]
In the next section, we will show that C-SGD equals the single-worker SGD with a larger batch size.

**Decentralized stochastic gradient descent (D-SGD). The goal of D-SGD is to learn a consensus**
model wa(t) = _m[1]_ �mj=1 **[w][j]** [(][t][)][ on][ m][ workers, where][ w][j] [(][t][)][ stands for the][ d][-dimensional local model]

on the j-th worker. We denote P = [Pj,k] ∈ R[m][×][m] as a doubly stochastic gossip matrix (see
Definition A.1) that characterizes the underlying topology . The vanilla Adapt-While-Communicate
_G_
(AWC) version of the mini-batch D-SGD (Nedic & Ozdaglar, 2009; Lian et al., 2017) updates the
model on the j-th worker by


**wj** (t+1) =


Communication
�m �� �
�

**Pj,kwk(t) −η ·**
_j=1_


Local gradient computation
� �� �
_∇L[µ][j]_ [(][t][)](wj (t)) _._ (2)


For a more detailed background of D-SGD, please refer to Appendix A .

### 4 THEORETICAL RESULTS

This section shows the implicit regularization effect and the escaping efficiency of D-SGD. We start
by showing that D-SGD can be interpreted as C-SGD on a regularized loss. Then we prove that
the regularization term in the new loss scales positively with the largest Hessian eigenvalue (see
Theorem 1), which suggests that D-SGD implicitly minimizes the sharpness. Next, we prove that
the regularization effect will increase with the total batch size if we apply the linear scaling rule
(see Theorem 2), which justifies the superiority of D-SGD in large-batch settings. Finally, we prove
the escaping efficiency of D-SGD beyond the quadratic assumption (see Theorem 3) and show that
D-SGD favors super-quadratic minima (see Proposition 4).

4.1 D-SGD IS EQUIVALENT TO C-SGD ON A REGULARIZED LOSS

In this subsection, we theoretically compare D-SGD and C-SGD. We prove that D-SGD is equivalent
to C-SGD on regularized loss with an extra positive regularization term, as shown in the following
theorem.
**Theorem 1 (Implicit regularization of D-SGD).** _Given that the loss L is continuous_
_and has fourth-order partial derivatives, denote the weight diversity matrix as Ξ(t)_ =
1 �m

_m_ _j=1[(][w][j]_ [(][t][)][−][w][a][(][t][)][)(][w][j] [(][t][)][−][w][a][(][t][)][)][T][, its diagonal matrix as][ Ξ][∗][(][t][)][, and the][ d][-dimensional all-ones]
_vector as 1. With a probability greater than 1_ (η), the mean iterate of D-SGD becomes
_−O_
Eµj (t)∼D
_j=1,...,m[[][w][a][(][t][+1)][]]_


= wa(t)−η∇ �L (wa(t)) + [1] �

2 [Tr(][H][(][w][a][(][t][)][)][Ξ][∗][(][t][)][)]

� �� �
_the regularized loss_


+O(η[2]1)+O (η∥wj (t)−wa(t)∥2[3] **[1][)][,]** (3)


_Under mild assumptions in Lemma C.2, D-SGD implicitly regularizes_
_reg( wj_ (t) ) = _λH(wa(t)),1_ _·_ Tr(Ξ(t))
_j=1,...,m_ _maximum Hessian eigenvalue�_ �� � _consensus distance�_ �� �


_._


The first term λH(wa(t)),1 is commonly regraded as a sharpness measure (Jastrzebski et al., 2017;
Wen et al., 2020). It is related to the (Cϵ, A)-sharpness (i.e., maxw′∈Cϵ L(w + Aw[′]) − **_L(w)) in_**
Keskar et al. (2017) and is an equivalent measure to the Sharpness Aware Minimization (SAM) loss
proposed by Foret et al. (2021) at a local minimum (Zhuang et al., 2022). Theorem 1 shows that the
decentralization navigates D-SGD towards the flatter directions, in order to lower the regularization
term λH(wa(t)),1. The second term, the trace of Ξ(t), equals to the consensus distance, a key
component measuring the overall effect of decentralized learning (Kong et al., 2021),


consensus distance = [1]

_m_


_m_
�

(wj (t)−wa(t))[T] (wj (t)−wa(t)).
_j=1_


-----

Consequently, Theorem 1 also suggests that D-SGD implicitly controls the discrepancy between the
global averaged model wa(t) and the local models wj (t) (j = 1, . . ., m) during training.

Our derived implicit regularization on the sharpness of learned minima is similar to how label noise
(Blanc et al., 2020; Damian et al., 2021) and artificial noise (Orvieto et al., 2022) smooth the loss
function in centralized gradient methods, including distributed centralized gradient methods (C-SGD)
and single-worker gradient methods. To the best of our knowledge, this is the first work that shows
D-SGD is equivalent to C-SGD on a regularized loss with implicit sharpness regularization. In the
existing literature, initial efforts have viewed D-SGD as C-SGD in a higher-dimensional space that
penalizes the weight norm ∥W∥I[2]−P[, where][ W][ = [][w][1][,][ · · ·][,][ w][m][]][T][ ∈] [R][m][×][d][ stands for all local]
models across the network (Yuan et al., 2021; Gurbuzbalaban et al., 2022).

We summarize the proof sketch below. The full proof is given in Appendix C.

**Proof sketch.**

**(1) Deriving the dynamics of the global averaged model** [2]. We first start by rewriting the update of
the global averaged model wa(t) of D-SGD as follows,

**wa(t+1) =wa(t)−η�** _∇L (wa(t))_ + ∇L (wa(t)) −∇L[µ][(][t][)] (wa(t))
unbiased gradient� �� � �gradient noise over the superbatch�� _µ(t)�_


+ [1]

_m_


_m_
�

[∇L[µ][j][ (][t][)] (wj (t)) −∇L[µ][j][ (][t][)] (wa(t))]
_j=1_


�. (4)


� �� �
gradient diversity among workers

**Remark. The equality shows that decentralization introduces an additional noise, which char-**
acterizes the gradient diversity between the global averaged model wa(t) and the local models
**wj** (t) (j = 1, . . ., m). It implies that distributed centralized SGD, which has constant zero gradient
diversity, is equivalent to standard single-worker SGD with larger batch size. Note that the gradient
diversity also equals to zero on quadratic loss L (see Corollary C.1). Consequently, the quadratic
approximation in the analysis of mini-batch SGD (Zhu et al., 2019b; Ibayashi & Imaizumi, 2021; Liu
et al., 2021) fails to capture how decentralization affects the training dynamics of D-SGD.

**(2) Performing Taylor expansion on the gradient diversity. Analyzing the effect of the gradient**
diversity on the training dynamics of D-SGD on the general non-convex losses is highly non-trivial.
Technically, we perform a second-order Taylor expansion on the gradient diversity around wa(t),
omitting the high-order residuals R:


_m_
� **_T_** _[µ][j][ (][t][)](wa(t)) ⊗_ [(wj (t)−wa(t))(wj (t)−wa(t))[T] ].

_j=1_


1

_m_

= [1]

_m_


_m_
�

[∇L[µ][j][ (][t][)] (wj (t)) −∇L[µ][j][ (][t][)] (wa(t))]
_j=1_

_m_
�

**_H_** _[µ][j][ (][t][)](wa(t))(wj_ (t)−wa(t)) + [1]

2m

_j=1_


Here H _[µ][j][ (][t][)](wa(t)) ≜_ _|µj1(t)|_ �|ζµ(tj)=1(t)| **_[H][(][w][a][(][t][)][;][ z][j,ζ][(][t][)][)][ stands for the empirical Hessian at][ w][a][(][t][)]_**

and T _[µ][j][ (][t][)](wa(t)) ≜_ _|µj1(t)|_ �|ζµ(tj)=1(t)| **_[T][ (][w][a][(][t][)][;][ z][j,ζ][(][t][)][)][ denotes the empirical third-order partial]_**

derivative tensor at wa(t), where µj (t) and zj,ζ(t) follows the notation in Equation (1).

Analogous to the works investigating the SGD dynamics (M et al., 2017; Zhu et al., 2019b; Ziyin et al.,
2022; Wu et al., 2022), we will calculate the expectation and covariance of the gradient diversity. The
expectation of gradient diversity is calculated first as follows. We defer the analysis of its covariance
to Subsection 4.3. Taking expectation over all local mini-batches µj (t) (j = 1, . . ., m) provides[3]


_m_
� [∇L[µ][j][ (][t][)] (wj (t)) −∇L[µ][j][ (][t][)] (wa(t))] �

_j=1_


Eµj (t)∼D
_j=1,...,m_


1
�

_m_


2Note that there is no central server In D-SGD. In the following we analyze the training dynamics of
the global averaged model wa(t) of D-SGD, which has been proved to be close to the individual models
**wj** (t)(j = 1, ..., m) (Yuan et al., 2016; Fallah et al., 2022).
3
Taking expectation over µj (t) means taking expectation over all zj,ζ(t) (ζ (t) = 1, . . ., |µj|).


-----

_m_
�

(wj (t)−wa(t))(wj (t)−wa(t))[T] ]+R.
_j=1_


=H(wa(t)) [1]

_m_


_m_
�

(wj (t)−wa(t))
_j=1_


+ [1]

2 **_[T][ (][w][a][(][t][)][)][ ⊗]_** [[ 1]m


� =0�� �

The i-th entry of the above equation will be


1

Eµj (t)∼D�
_j=1,...,m_ _m_


_m_
� [∂iL[µ][j][ (][t][)] (wj (t)) −∂iL[µ][j][ (][t][)] (wa(t))] �

_j=1_


= [1]

2


�

_∂ikl[3]_ **_[L][(][w][a][(][t][)][) 1]_**

_m_

_k,l_


_m_
�

(wj (t)−wa(t))k(wj (t)−wa(t))l
_j=1_


+O (∥wj (t)−wa(t)∥2[3][)][,] (5)


� =∂i �kl _[∂]kl[2]_ **_[L][(][z][n][)][ 1]m_** �mj=1 [(]��[w][j] [(][t][)][−][w][a][(][t][))]k[(][w][j] [(][t][)][−][w][a][(][t][))]l �

where (wj (t)−wa(t))k denotes the k-th entry of the vector wj (t)−wa(t). The equality in the brace is
due to Clairaut’s theorem (Rudin et al., 1976).

Then we prove that with probability greater than 1−O(η), the iterate of D-SGD can be written as
Eµj (t)∼D
_j=1,...,m[[][w][a][(][t][+1)][]]_


1

=wa(t)−η∇ �L (wa(t)) + [1] � +O(η 2 1)+O (η∥wj (t)−wa(t)∥2[3] **[1][)][.]**

2 [Tr(][H][(][w][a][(][t][)][)][Ξ][∗][(][t][)][)]

� �� �
the regularized loss


**(3) Controlling the top Hessian eigenvalue with Tr(H(wa(t))Ξ[∗](t)). According to Lemma C.2,**
we obtain
0 ≤ Tr(H(wa(t))Ξ[∗](t)) ≤ _λH(wa(t)),1_ _·_ Tr(Ξ(t)) _≤_ _d1 Tr(H(wa(t))Ξ[∗](t)),_
� sharpness�� � consensus distance� �� �

where λH(wa(t)),1 denotes the largest eigenvalue of H(wa(t)) and d1 stands for the marginal contribution of λH(wa(t)),1 on the full spectrum of H(wa(t)) (i.e., λH(wa(t)),1 = _dd1_ [Tr(][H][(][w][a][(][t][)][))][).]

Therefore, combined with Equation (3), we conclude that D-SGD also implicitly regularizes
_λH(wa(t)),1_ Tr(Ξ(t)).
_·_

4.2 AMPLIFIED REGULARIZATION OF D-SGD IN LARGE-BATCH SETTING

In practice, the decentralization (and also distribution) ordinarily implies an equivalent large total
batch size, since a massive number of workers are involved in the system in many practical scenarios.
Moreover, large-batch training can enhance the utilization of super computing facilities and further
speeds up the entire training process. Thus, studying the large-batch setting is of significant interest
for fully understanding the application of D-SGD.

Despite the importance, theoretical understanding of the generalization of large-batch training in
D-SGD remains an open problem. This subsection examines how the total batch size affects the
sharpness reduction effect of D-SGD if the linear scaling rule, as presented below, is applied.

**Linear scaling rule (LSR). The linear scaling rule is a widely used hyper-parameter-free rule for**
deep learning (Krizhevsky, 2014; He et al., 2016a; Goyal et al., 2017; Bottou et al., 2018; Smith et al.,
2020), which states that a fixed learning rate to total batch size ratio allows maintaining generalization
performance when the total batch size increases.

**Theorem 2. Suppose that the averaged gradient norm satisfies** _m1_ �mj=1 _[∥∇][L][ (][w][j]_ [(][t][)][)][∥][2] _≤_

(1+ [1][−]4 _[λ]_ [)][ 1]m �mj=1 _[∥∇][L][ (][w][j]_ [(][t][+1)][)][∥][2][, where][ 1][−][λ][ denotes the spectral gap (see][ Definition A.2][).]

_The sharpness regularization coefficient[4]_ _of D-SGD (i.e.,_ Tr(Ξ(t))) at t-th iteration is
_O(|µ(t)|[2](1 +_ _m[1]_ �mj=1 _|µj1(t)|_ [))][, which increases with the total batch size][ |][µ][(][t][)][|][ if we apply the]

_linear scaling rule._

4Recall that Theorem 1 implies that the loss function D-SGD optimizes is close to the original loss L plus
12 [Tr] �Ξ(t)� _· λH(wa(t)),1. The second term λH(wa(t)),1 is a sharpness measure, and the first term Tr(Ξ(t)) is_
the “regularization coefficient” which characterizes the strength of the sharpness regularization.


-----

Theorem 2 states that the sharpness regularization effect of D-SGD is amplified in large-batch settings
if we apply the linear scaling rule. It is worth noting that this amplified sharpness regularization effect
requires no additional communication and computation, which verifies that significant advantages in
generalizability surprisingly exist in the large-batch D-SGD. The proof is included in Appendix C.

4.3 ESCAPING EFFICIENCY OF D-SGD FROM LOCAL MINIMA

This subsection presents an analysis of the escaping efficiency of D-SGD, based on the construction
of a stochastic differential equation (SDE) approximation (Jastrzebski et al., 2017; M et al., 2017; Li
et al., 2021) of D-SGD. This escaping efficiency analysis shows that D-SGD favors super-quadratic
minima.

To construct the SDE approximation of D-SGD, we combine Equation (3) and Equation (4) and write
the iterates of D-SGD as follows,
**wa(t+1)**

1

= wa(t)−η∇�L (wa(t)) + [1] �+ηϵ[0](t)+O(η 2 1)+O (η∥wj (t)−wa(t)∥2[3] **[1][)][,]** (6)

2 [Tr(][H][(][w][a][(][t][)][)][Ξ][∗][(][t][)][)]

where ϵ[0](t) denotes the zero-mean noise in D-SGD. Applying Lemma C.4, Equation (6) can be
viewed as the discretization of the following SDE

dwa(t) = −�∇L (wa(t)) + [1] �dt + �ηΣD(t)dW (t),

2 **_[T][ (][w][a][(][t][)][)][ ⊗]_** **[Ξ][∗][(][t][)]**

wherethe total noise ⊗ denotes the tensor product (see ϵD(t) = _m1_ �mj=1[[][∇][L][µ][j][ (][t][)][ (] Appendix A.2[w][j] [(][t][)][)][ −][∇][L][ (][w]), Σ[a][(][t]D[)][)]](t[, and]) denotes the covariance matrix of[ W][ (][t][)][ is a standard Brownian]

motion (Feynman, 1964) in R[d]. We then utilize the SDE approximation of D-SGD to study the
escaping efficiency of D-SGD, defined as follows.
**Definition 1 (Escaping efficiency). Let w[∗]** _denote one of the local minimum of the loss function L._
_Then, we call Ewa(t)[L(wa(t)) −_ **_L(w[∗])] the escaping efficiency of the dynamic wa(t+1) from w[∗],_**
_where Ewa(t) denotes the expectation with respect to the distribution of wa(t)._

Suppose that wa(t+1) gets stuck in a minimum w[∗][5], the escaping efficiency characterizes the
probability that the dynamics wa(t+1) escapes w[∗], since Markov’s inequality guarantees ∀δ,
_P_ (L(wa(t+1)) − **_L(w[∗]) ≥_** _δ) ≤_ �Ewa(t)[L(wa(t+1)) − **_L(w[∗])]�_** _/δ._

We then have the following theorem on the escaping efficiency of D-SGD.
**Theorem 3 (Escaping efficiency of D-SGD). If the loss L is continuous and has fourth-order partial**
_derivatives, the escaping efficiency of D-SGD from minimum w[∗]_ _satisfies_
Ewa(t)[L(wa(t)) − **_L(w[∗])]_**

� _t_
= − Ewa(t)[∇L(wa(t))[T] _∇L(wa(t)) −_ [1]

0 2 _[grandsum][((][T][ (][w][a][(][t][)][)][∇][L][(][w][a][(][t][)][))][ ⊙]_ **[Ξ][∗][(][t][)][)]d][t]**

� _t_ _η_
+

0 2 [Tr (][H][(][w][a][(][t][)][)][Σ][D][(][t][)][) d][t,]

_where_ _denotes the Hadamard product (Davis, 1962), and grandsum(_ ) (Merikoski, 1984) of a
_⊙_ _·_
_matrix_ **_M[˜]_** _satisfies grandsum( M[˜]_ ) = [�]i,j **_M[˜]_** _ij._

A detailed proof and the escaping efficiency of C-SGD (see Proposition C.5) are given in Appendix C.

Comparing Theorem 3 and Proposition C.5, we can see that the main difference between the escaping
efficiency of D-SGD and C-SGD lies in the integral of grandsum((T (wa(t))∇L(wa(t))) ⊙ **Ξ[∗](t)),**
which correlates with the gradient diversity in Equation (4). We then study how this term affects the
escaping efficiency of D-SGD on super-quadratic minima, a typical class of minima as defined below.
**Definition 2 (Super-quadratic minimum). Given that the loss L is continuous and has second-order**
_partial derivatives, we call the mimimum w[∗]_ _of L δ-locally super-quadratic if for any w in the open_
_punctured neighbourhood of w[∗]_ _(i.e., w ∈_ _U[˚](w[∗], δ)), the following condition holds: (1) H(w[∗]) ≼_
**_H(w); and (2) ∃_** _α(w), β(w) ∈_ R[+] _s.t. H(w)(w −_ **w[∗]) = α(w)(∥w −** **w[∗]∥2[β][(][w][)](w −** **w[∗])).**

5Note that there is no guarantee that D-SGD can converge to any local minimum in the non-convex settings.


-----

Figure 3: An illustration of super-quadratic and sub-quadratic minimum.

The super-quadratic growth implies that the losses become flatter when the parameters get closer to
minima. We then present the intuition of the second condition in Definition 2. A second-order Taylor
approximation of L around w[∗] reads,

**_L(w)_** **_L(w[∗]) =_** **_L(w)[T]_** (w **w[∗]) + (w** **w[∗])[T]** **_H(w)(w_** **w[∗]),**
_−_ _∇_ _−_ _−_ _−_
and the second condition in Definition 2 further guarantees that,

**_L(w) −_** **_L(w[∗]) = ∇L(w)[T]_** (w − **w[∗]) + α(w)∥w −** **w[∗]∥2[β][(][w][)]** (w − **w[∗])[T]** (w − **w[∗]),**
� �� �
quadratic growth

which suggests that the growth of L(w) is δ-locally super-quadratic as long as α(w), β(w) > 0.

A related study by Ma et al. (2022) observes that the minima learned by centralized gradient descent
methods obey a “sub-quadratic growth” (i.e., the loss becomes sharper as parameters get closer to the
minimum). We also give a formalization of the sub-quadratic minima in Definition C.1. Intuitively,
super-quadratic minima are flatter than sub-quadratic minima with the same depth, as illustrated in
Figure 3. The following proposition studies the sign of grandsum((T (wa(t))∇L(wa(t))) ⊙ **Ξ[∗](t))**
on the super-quadratic and sub-quadratic minima.
**Proposition** **4.** _Suppose_ _that_ **wa(t)** _is_ _sufficiently_ _close_ _to_ _a_ _local_ _minimum_ **w[∗],**
_grandsum((T (wa(t))∇L(wa(t))) ⊙_ **Ξ[∗](t)) is (1) zero if w[∗]** _is a quadratic minima, (2) posi-_
_tive if w[∗]_ _is a δ-locally super-quadratic minima, and (3) negative if w[∗]_ _is a δ-locally sub-quadratic_
_minima._

Combined with Theorem 3, Proposition 4 shows that D-SGD favors super-quadratic minima over
sub-quadratic minima with a higher probability. The proof is included in Appendix C.

Theorem 1 and Proposition 4 indicate that the additional noise (i.e., the gradient diversity in Equation (4)) of D-SGD may play a distinct role in shaping the generalizability of D-SGD.

### 5 EMPIRIAL RESULTS

This section empirically validates our theory. We first introduce the experimental setup and then
study how decentralization favours the flatness of minima.

**Implementation settings. Vanilla D-SGD and C-SGD are employed to train image classifiers on**
CIFAR-10 (Krizhevsky et al., 2009) with AlexNet (Krizhevsky et al., 2017), ResNet-18 and ResNet34 (He et al., 2016b), three popular neural networks. Batch normalization (Ioffe & Szegedy, 2015)
is employed in training AlexNet. The number of workers (one GPU as a worker) is set as 16; and
the local batch size is set as 8, 64, and 512 per worker in three different cases. For the case of local
batch size 64, the initial learning rate is set as 0.1 for ResNet-18 and 0.01 for AlexNet. The learning
rate is divided by 10 when the model has passed the 2/5 and 4/5 of the total number of iterations
(He et al., 2016a). We apply the linear scaling law to avoid different total batch sizes caused by the
different local batch size (see Subsection 4.2). In order to understand the effect of decentralization on
the flatness of minima, all other training techniques are strictly controlled. The code is written based
on PyTorch (Paszke et al., 2019).

**Hardware enviornment. The experiments are conducted on a computing facility with NVIDIA[®]**
Tesla[™] V100 16GB GPUs and Intel[®] Xeon[®] Gold 6140 CPU @ 2.30GHz CPUs.

We plot the minima learned by C-SGD and D-SGD in Figure 4 using the loss landscape 3D visualization tool in Li et al. (2018). See more plots in Appendix B. Two observations are obtained from
these figures: (1) the minima of D-SGD are flatter than those of C-SGD; and (2) the gap in flatness
becomes larger as the total batch size increases. These observations support the claims in Theorem 1
and Theorem 2 that D-SGD favors flatter minima than C-SGD, especially in the large-batch settings.


-----

(a) C-SGD, 128 total batch size

2.5

0.0

0.4

0.2

0.4 0.0

0.2 0.0 0.2

0.2 0.4

0.4

(d) D-SGD, 128 total batch size


(b) C-SGD, 1024 total batch size

17.5

15.0

12.5

10.0

7.5

5.0

2.5

0.0

0.4

0.2

0.4 0.0

0.2 0.0 0.2

0.2 0.4

0.4

(e) D-SGD, 1024 total batch size


(c) C-SGD, 8196 total batch size

17.5

15.0

12.5

10.0

7.5

5.0

2.5

0.0

0.4

0.2

0.4 0.0

0.2 0.0 0.2

0.2 0.4

0.4

(f) D-SGD, 8196 total batch size


Figure 4: Minima 3D visualization of C-SGD and D-SGD with ResNet-18 on CIFAR-10.

### 6 DISCUSSION AND FUTURE WORK

**Scalability to complex or sparse topologies. Our theory holds for arbitrary topologies (see Defini-**
tion A.1). We also conduct experiments on grid-like and static exponential topologies (Ying et al.,
2021a) and obtain results similar to Figure 4 and Figure B.1. For spare topologies, which has a very
small spectral gap, the regularization term in Theorem 1 would be extremely large during training,
which may hinder optimization and lead to a large total excess risk of D-SGD. Can we design a new
decentralized training algorithm that can alleviate the optimization issue on spare topologies while
maintaining the generalization advantage in large-batch setting?

**Non-IIDness and the flatness of minima. In real-world settings, a fundamental challenge in**
distributed learning is that data may not be i.i.d. across workers (Tang et al., 2018; Vogels et al.,
2021; Mendieta et al., 2022). In this case, different workers may collect distinct or even contradictory
samples (i.e., data-heterogeneity) (Criado et al., 2021). It is widely observed that the non-IIDness hurts
the generalizability of D-SGD. Can we rigorously analyze how the degree of data-heterogeneity affects
the flatness of minima and design theoretically motivated algorithms to promote the generalizability
of D-SGD in non-IID settings?

### 7 CONCLUSION

This work provides a new theory that reconciles the conflict between the empirical observations
showing that D-SGD can generalize better than centralized SGD (C-SGD) in large-batch settings
and the existing generalization theories of D-SGD which suggest that decentralization degrades
generalizability. We prove that D-SGD introduces an implicit regularization that penalizes the learned
minima’s sharpness and this effect will be amplified in large-batch settings if we apply the linear
scaling rule. We further analyze the escaping efficiency of D-SGD, which shows that D-SGD favors
super-quadratic flat minima. To our best knowledge, this is the first work on the implicit sharpness
regularization and escaping efficiency of D-SGD.


-----

### REFERENCES

Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat. Stochastic gradient push for
distributed deep learning. In International Conference on Machine Learning, 2019.

Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural
networks driven by an ornstein-uhlenbeck like process. In Conference on learning theory. PMLR,
2020.

Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223–311, 2018.

Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124018,
2019.

Kai Chen and Qiang Huo. Scalable training of deep learning machines by incremental block
training with intra-block parallel optimization and blockwise model-update filtering. In 2016 IEEE
_International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5880–5884._
IEEE Press, 2016.

Marcos F Criado, Fernando E Casado, Roberto Iglesias, Carlos V Regueiro, and Senen Barro. Non-iid´
data and continual learning processes in federated learning: A long road ahead. arXiv preprint
_arXiv:2111.13394, 2021._

Alex Damian, Tengyu Ma, and Jason D Lee. Label noise sgd provably prefers flat global minimizers.
_Advances in Neural Information Processing Systems, 2021._

Ch Davis. The norm of the schur product operation. Numerische Mathematik, 4:343–344, 1962.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks.
_Advances in neural information processing systems, 2012._

Alireza Fallah, Mert Gurbuzbalaban, Asuman Ozdaglar, Umut Simsekli, and Lingjiong Zhu. Robust
distributed accelerated stochastic gradient methods for multi-agent networks. Journal of Machine
_Learning Research, 23(220):1–96, 2022._

Richard P Feynman. The brownian movement. The Feynman Lectures of Physics, 1:41, 1964.

Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. In International Conference on Learning Representations,
2021.

Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients - how¨
easy is it to break privacy in federated learning? In Advances in Neural Information Processing
_Systems, 2020._

Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,´
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.

Mert Gurbuzbalaban, Yuanhan Hu, Umut Simsekli, Kun Yuan, and Lingjiong Zhu. Heavy-tail
phenomenon in decentralized sgd. arXiv preprint arXiv:2205.06689, 2022.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
2016a.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision. Springer, 2016b.

Sepp Hochreiter and J¨urgen Schmidhuber. Flat minima. Neural computation, 9(1):1–42, 1997.


-----

Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. Advances in neural information processing
_systems, 2017._

Hikaru Ibayashi and Masaaki Imaizumi. Exponential escape efficiency of sgd from sharp minima in
non-stationary regime. arXiv preprint arXiv:2111.04004, 2021.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, 2015.

Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407,
2018.

Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.

Yiding Jiang, Behnam Neyshabur*, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning
_Representations, 2020._

Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
_International Conference on Learning Representations, 2017._

Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified
theory of decentralized SGD with changing topology and local updates. In International Conference
_on Machine Learning, 2020._

Lingjing Kong, Tao Lin, Anastasia Koloskova, Martin Jaggi, and Sebastian Stich. Consensus control
for decentralized deep learning. In International Conference on Machine Learning. PMLR, 2021.

Konrad K¨onigsberger. Analysis 2. Springer-Verlag, 2013.

Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint
_arXiv:1404.5997, 2014._

Alex Krizhevsky, G Hinton, et al. Learning multiple layers of features from tiny images (tech. rep.).
_University of Toronto, 2009._

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.

Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape
of neural nets. Advances in neural information processing systems, 2018.

Mu Li, David G Andersen, Alexander J Smola, and Kai Yu. Communication efficient distributed
machine learning with the parameter server. Advances in Neural Information Processing Systems,
2014.

Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochastic
differential equations (sdes). Advances in Neural Information Processing Systems, 2021.

Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. In Advances in Neural Information Processing Systems, 2017.

Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic
gradient descent. In International Conference on Machine Learning, 2018.

Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite learning rate stochastic
gradient descent. In International Conference on Machine Learning. PMLR, 2021.


-----

Cassio G Lopes and Ali H Sayed. Diffusion least-mean squares over adaptive networks: Formulation
and performance analysis. IEEE Transactions on Signal Processing, 2008.

Jie Lu, Choon Yik Tang, Paul R Regier, and Travis D Bow. Gossip algorithms for convex consensus
optimization over networks. IEEE Transactions on Automatic Control, 2011.

Songtao Lu and Chai Wah Wu. Decentralized stochastic non-convex optimization over weakly
connected time-varying digraphs. In ICASSP 2020-2020 IEEE International Conference on
_Acoustics, Speech and Signal Processing (ICASSP), 2020._

Stephan M, t, Matthew D. Hoffman, and David M. Blei. Stochastic gradient descent as approximate
bayesian inference. Journal of Machine Learning Research, 18(134):1–35, 2017.

Chao Ma, Lei Wu, and Lexing Ying. The multiscale structure of neural network loss functions: The
effect on optimization and origin. arXiv preprint arXiv:2204.11326, 2022.

David JC MacKay. A practical bayesian framework for backpropagation networks. Neural computa_tion, 4(3):448–472, 1992._

Matias Mendieta, Taojiannan Yang, Pu Wang, Minwoo Lee, Zhengming Ding, and Chen Chen. Local
learning matters: Rethinking data heterogeneity in federated learning. In Proceedings of the
_IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8397–8406, 2022._

Jorma Kaarlo Merikoski. On the trace and the sum of elements of a matrix. Linear algebra and its
_applications, 60:177–185, 1984._

Giorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, Shigang Li, and Dan Alistarh. Asynchronous
decentralized sgd with quantized and local updates. Advances in Neural Information Processing
_Systems, 2021._

Angelia Nedic. Distributed gradient methods for convex machine learning problems in networks:
Distributed optimization. IEEE Signal Processing Magazine, 2020.

Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization.
_IEEE Transactions on Automatic Control, 54(1):48–61, 2009._

Bernt Øksendal. Stochastic differential equations. In Stochastic differential equations, pp. 65–84.
Springer, 2003.

Antonio Orvieto, Hans Kersting, Frank Proske, Francis Bach, and Aurelien Lucchi. Anticorrelated
noise injection for improved generalization. In International Conference on Machine Learning.
PMLR, 2022.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 2019.

Jorma Rissanen. A universal prior for integers and estimation by minimum description length. The
_Annals of statistics, 11(2):416–431, 1983._

Walter Rudin et al. Principles of mathematical analysis. McGraw-hill New York, 1976.

Eugene Seneta. Non-negative matrices and Markov chains. Springer Science & Business Media,
2006.

Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E. Dahl. Measuring the effects of data parallelism on neural network training. Journal of
_Machine Learning Research, 20(112):1–49, 2019._

Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd
_Annual Allerton Conference on Communication, Control, and Computing (Allerton), 2014._

Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentralized
consensus optimization. SIAM Journal on Optimization, 2015.


-----

Samuel Smith, Erich Elsen, and Soham De. On the generalization benefit of noise in stochastic
gradient descent. In International Conference on Machine Learning. PMLR, 2020.

Tao Sun, Dongsheng Li, and Bao Wang. Stability and generalization of decentralized stochastic
gradient descent. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp.
9756–9764, 2021.

Hossein Taheri, Aryan Mokhtari, Hamed Hassani, and Ramtin Pedarsani. Quantized decentralized
stochastic learning over directed graphs. In International Conference on Machine Learning, 2020.

Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2: Decentralized training over
decentralized data. In International Conference on Machine Learning, 2018.

John Tsitsiklis, Dimitri Bertsekas, and Michael Athans. Distributed asynchronous deterministic
and stochastic gradient optimization algorithms. IEEE transactions on automatic control, 31(9):
803–812, 1986.

John Nikolas Tsitsiklis. Problems in decentralized decision making and computation. Technical
report, Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems, 1984.

Thijs Vogels, Lie He, Anastasiia Koloskova, Sai Praneeth Karimireddy, Tao Lin, Sebastian U Stich,
and Martin Jaggi. Relaysum for decentralized deep learning on heterogeneous data. Advances in
_Neural Information Processing Systems, 34:28004–28015, 2021._

John Von Neumann. Some matrix-inequalities and metrization of matric space. 1937.

Stefanie Warnat-Herresthal, Hartmut Schultze, Krishnaprasad Lingadahalli Shastry, Sathyanarayanan
Manamohan, Saikat Mukherjee, Vishesh Garg, Ravi Sarveswara, Kristian Handler, Peter Pickkers,¨
N Ahmad Aziz, et al. Swarm learning for decentralized and confidential clinical machine learning.
_Nature, 2021._

Yeming Wen, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan, and Jimmy Ba. An
empirical study of stochastic gradient descent with structured covariance noise. In Proceedings of
_the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108, pp._
3621–3631. PMLR, 2020.

Lei Wu, Mingze Wang, and Weijie Su. When does sgd favor flat minima? a quantitative characterization via linear stability. arXiv preprint arXiv:2207.02628, 2022.

Lin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control
_Letters, 2004._

Jie Xu, Wei Zhang, and Fei Wang. A(dp)[2]sgd: Asynchronous decentralized parallel stochastic
gradient descent with differential privacy. IEEE Transactions on Pattern Analysis and Machine
_Intelligence, 2021._

Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz, and Pavlo Molchanov. See
through gradients: Image batch recovery via gradinversion. In Proceedings of the IEEE/CVF
_Conference on Computer Vision and Pattern Recognition (CVPR), 2021._

Bicheng Ying, Kun Yuan, Yiming Chen, Hanbin Hu, Pan Pan, and Wotao Yin. Exponential graph is
provably efficient for decentralized deep training. In Advances in Neural Information Processing
_Systems, 2021a._

Bicheng Ying, Kun Yuan, Hanbin Hu, Yiming Chen, and Wotao Yin. Bluefog: Make decentralized
algorithms practical for optimization and deep learning. arXiv preprint arXiv:2111.04287, 2021b.

Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. Imagenet training in
minutes. In Proceedings of the 47th International Conference on Parallel Processing. Association
for Computing Machinery, 2018.

Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM
_Journal on Optimization, 26(3):1835–1854, 2016._


-----

Kun Yuan, Yiming Chen, Xinmeng Huang, Yingya Zhang, Pan Pan, Yinghui Xu, and Wotao Yin.
Decentlam: Decentralized momentum sgd for large-batch deep training. In Proceedings of the
_IEEE/CVF International Conference on Computer Vision, pp. 3029–3039, 2021._

Wei Zhang, Mingrui Liu, Yu Feng, Xiaodong Cui, Brian Kingsbury, and Yuhai Tu. Loss landscape
dependent self-adjusting learning rates in decentralized stochastic gradient descent. arXiv preprint
_arXiv:2112.01433, 2021._

Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural
_Information Processing Systems, 2019a._

Tongtian Zhu, Fengxiang He, Lan Zhang, Zhengyang Niu, Mingli Song, and Dacheng Tao. Topologyaware generalization of decentralized sgd. In International Conference on Machine Learning.
PMLR, 2022.

Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. In
_International Conference on Machine Learning. PMLR, 2019b._

Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha C Dvornek, sekhar
tatikonda, James s Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware
training. In International Conference on Learning Representations, 2022.

Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. Strength of minibatch noise in SGD. In
_International Conference on Learning Representations, 2022._


-----

### A ADDITIONAL BACKGROUND

A.1 DECENTRALIZED LEARNING

To handle an increasing amount of data and model parameters, distributed learning across multiple
computing workers emerges. A traditional distributed learning system usually follows a centralized
setup. However, such a central server-based learning scheme suffers from two main issues: (1) A
centralized communication protocol significantly slows down training since central servers are easily
overloaded, especially in low-bandwidth or high-latency cases (Lian et al., 2017); (2) There exists a
potential information leakage through privacy attacks on model parameters despite decentralizing
data using Federated Learning (Zhu et al., 2019a; Geiping et al., 2020; Yin et al., 2021). As an
alternative, decentralized training allows workers to balance the load on the central server through
the gossip technique (Lian et al., 2017), as well as maintain confidentiality (Warnat-Herresthal et al.,
2021).

We then summarize some commonly used notions regarding decentralized learning.

**Definition A.1 (Doubly Stochastic Matrix). Let G = (V, E) stand for the decentralized communica-**
_tion topology where_ _denotes the set of m computational nodes and_ _represents the edge set. For_
_V_ _E_
_any given topology G = (V, E), the doubly stochastic gossip matrix P = [Pj,k] ∈_ R[m][×][m] _is defined_
_on the edge set_ _that satisfies_
_E_

- P = P[T] _(symmetric);_

- If j ̸= k and (j, k) /∈E, then Pj,k = 0 (disconnected) and otherwise, Pj,k > 0 (connected);



- Pj,k ∈ [0, 1] ∀k, l and [�]


_k_ **[P][j,k][ =][ �]**


_l_ **[P][j,k][ = 1][ (standard weight matrix for undirected graph).]**


In the following we illustrate some commonly-used communication topologies.

Figure A.1: An illustration of some commonly used topologies.

The intensity of gossip communications is measured by the spectral gap (Seneta, 2006) of P.

**Definition A.2 (Spectral Gap). Denote λ = max {|λ2|, |λm|} where λi (i = 2, . . ., m) is the i-th**
_largest eigenvalue of gossip matrix P ∈_ R[m][×][m]. The spectral gap of a gossip matrix P can be defined
_as follows:_

_spectral gap := 1_ _λ._
_−_

_According to the definition of doubly stochastic matrix (Definition A.1), we have 0_ _λ < 1. The_
_≤_
_spectral gap measures the connectivity of the communication topology, which is close to 0 for sparse_
_topologies and will approach 1 for well-connected topologies._

**Assumption A.1. We assume that the sum of the off-diagonal entries of Ξ(t) is smaller than d −** 1
_times of the sum of the diagonal entries of Ξ(t) in expectation:_


_m_
�

_k=1_


_m_
�

(wj (t)−wa(t))k(wj (t)−wa(t))l) ≤ Eµj (τ )∼D((d − 1)
_j=1_ _jτ=1=1,...,m,···,t_


1

_m_


_m_
�

(wj (t)−wa(t))[2]k[)][,]
_j=1_


�
E µj (τ )∼D (
_τj=1=1,···,...,m,t−1_ _k≠_ _l_


1

_m_


_where d stands for the dimensionality of wj_ (t)−wa(t).


-----

A.2 EXPLANATION OF TENSOR PRODUCT

The tensor product between a third-order tensor T ∈ R[d][×][d][×][d] and a second-order tensor (matrix)
**_M ∈_** R[d][×][d] in this paper is defined as


(T ⊗ **_M_** )i
� �� �
the i-th entry


= grandsum(Ti ⊙ **_M_** ),


where Ti ∈ R[d][×][d] is a second-order tensor (matrix), ⊙ denotes the Hadamard product (Davis,
1962), and the grandsum( ) (Merikoski, 1984) of a second-order tensor (matrix) **_M˜_** satisfies

_·_
grandsum( M[˜] ) = [�]i,j **_M[˜]_** _ij._


-----

### B ADDITIONAL MINIMA VISUALIZATION

We plot the minima learned by C-SGD and D-SGD as follows using the 2D loss landscape visualization tool in Li et al. (2018).


(a) C-SGD, 128 total batch size

0.4 0.2 0.0 0.2 0.4

(d) D-SGD, 128 total batch size


(b) C-SGD, 1024 total batch size

0.4

0.2

0.0

0.2

0.4

0.4 0.2 0.0 0.2 0.4

(e) D-SGD, 1024 total batch size


(c) C-SGD, 8196 total batch size

0.4

0.2

0.0

0.2

0.4

0.4 0.2 0.0 0.2 0.4

(f) D-SGD, 8196 total batch size


Figure B.1: Minima 2D visualization of C-SGD and D-SGD with ResNet-18 on CIFAR-10.

From Figure B.1, we observe that (1) the minima of D-SGD is flatter than those of C-SGD; and (2)
the gap in flatness becomes larger as the total batch size increases. Similar results are obtained with
AlexNet and ResNet-34 on CIFAR-10.


-----

### C PROOF

**Corollary C.1. The gradient diversity in Equation (4) equals to zero in the following cases: (1) the**
_loss L = w[T]_ **_Hw is quadratic; and (2) the optimization algorithm is distributed centralized SGD_**
_(Equation (1)),_

_Proof of Corollary C.1._

On quadratic loss, we have


_m_
�

[Hwj (t)−Hwa(t)] = H [1]

_m_

_j=1_


_m_
�

[wj (t)−wa(t)] _._
_j=1_


1

_m_


_m_
�

[∇L[µ][j][ (][t][)] (wj (t)) −∇L[µ][j][ (][t][)] (wa(t))] = [1]

_m_

_j=1_


� ��0 �


In distributed centralized SGD, the gradient diversity statisfies


1

_m_


_m_
�

[∇L[µ][j][ (][t][)] (wj (t)) _−∇L[µ][j][ (][t][)]_ (wa(t))] = 0.
_j=1_ ����
=wa(t)


**Lemma C.2. We denote Ξ(t) =** _m1_ �mj=1[(][w][j] [(][t][)][−][w][a][(][t][)][)(][w][j] [(][t][)][−][w][a][(][t][)][)][T][ the weight diversity ma-]

_trix and Ξ[∗](t) =_ [�]i[d]=1[⟨][e][i][e]i[T] _[,][ Ξ][(][t][)][⟩][F][ e][i][e]i[T]_ _[. We assume that][ d][1][ (][d][1][ < d][)][, the marginal contribu-]_
_tion of λH(wa(t)),1 on the full spectrum of H(wa(t)), is non-negative and satisfies λH(wa(t)),1 =_
_d1_

_d_ [Tr(][H][(][w][a][(][t][)][)][. Then the product of][ Tr(][Ξ][(][t][)][)][ and the maximum eigenvalue of][ H][(][w][a][(][t][)][)][ is upper]
_and lower bounded as_


0 ≤ Tr(H(wa(t))Ξ[∗](t)) ≤ _λH(wa(t)),1_
� �� �
_sharpness_

_Proof of Lemma C.2._


Tr(Ξ(t))

_·_
� �� �
_consensus distance_


_≤_ _d1 Tr(H(wa(t))Ξ[∗](t))._


On the one hand, von Neumann’s trace inequality (Von Neumann, 1937) guarantees


Tr(H(wa(t))Ξ[∗](t)) ≤


_d_
�

_λH(wa(t)),r_ _λΞ∗(t),r_ _λH(wa(t)),1_ Tr(Ξ(t)), (C.1)
_·_ _≤_ _·_
_r=1_


where λH(wa(t)),r and λΞ(t),r represent the r-th largest eigenvalue of H(wa(t)) and Ξ(t), respectively. On the other hand, we will prove that λH(wa(t)),1 ·Tr(Ξ(t)) ≤O(Tr(H(wa(t))Ξ[∗](t))). Since
**Ξ[∗](t) =** [�]i[d]=1[⟨][e][i][e]i[T] _[,][ Ξ][(][t][)][⟩][F][ e][i][e]i[T]_ [is a diagonal matrix,][ Tr(][H][(][w][a][(][t][)][)][Ξ][∗][(][t][)][)][ can be lower bounded as]

Tr(H(wa(t))Ξ[∗](t)) ≥ _ξ[2]_ Tr(H(wa(t))),

where ξ[2] is the lower bound of _m[1]_ �mj=1 [(][w][j] [(][t][)][−][w][a][(][t][)][)]k[2] [(][k][ = 1][, . . ., d][)][.]

Knowing that Tr(Ξ[∗](t)) = Tr(Ξ(t)), we can bound the right hand side of Equation (C.1) as follows:

0 ≤ _λH(wa(t)),1 · Tr(Ξ(t)) ≤_ _dξ[d][1][2][ Tr(][H][(][w][a][(][t][)][)][Ξ][∗][(][t][)][)][ ·][ Tr(][Ξ][(][t][)][)][ ≤]_ _[d][1][ Tr(][H][(][w][a][(][t][)][)][Ξ][∗][(][t][)][)][.]_

Note than we can also obtain

0 ≤ Tr(H(wa(t))Ξ[∗](t)) ≤ Tr(H(wa(t))) · Tr(Ξ(t)) ≤ _d1 Tr(H(wa(t))Ξ[∗](t)),_

which shows that D-SGD also implicitly regularizes Tr(H(wa(t))).


**Lemma C.3 ((Kong et al., 2021)). Suppose that the averaged gradient norm satisfies**
1 �m �m

_m_ _j=1_ _[∥∇][L][ (][w][j]_ [(][t][)][)][∥][2][ ≤] [(1 +][ 1][−]4 _[λ]_ [)][ 1]m _j=1_ _[∥∇][L][ (][w][j]_ [(][t][+1)][)][∥][2][, then the the consensus distance]

_of D-SGD satisfies_


Tr(Ξ(t)) = [1]

_m_


_m_
�

_∥wj_ (t) − **wa(t)∥2[2]**
_j=1_


-----



_,_



= λη[2]

_· O_


 1 �m

 _m_ _j=1_ _[∥∇][L][ (][w][j]_ [(][t][)][)][∥][2] +

(1 _λ)[2]_
_−_


_m1_ �mj=1 [E][µ]j [(][t][)][∼D] ��∇Lµj (t) (wj (t)) −∇L (wj (t))��22

1 _λ_
_−_


_where λ equals to 1_ _spectral gap (see Definition A.2)._
_−_
**Lemma C.4. D-SGD is approximated by the following SDE**

dwa(t) = −�∇L (wa(t)) + [1] �dt + �ηΣD(t)dW (t),

2 **_[T][ (][w][a][(][t][)][)][ ⊗]_** **[Ξ][∗][(][t][)]**

_where ⊗_ _denotes the tensor product (see Appendix A.2), ΣD(t) denotes the covariance matrix of the_
_unbiased noise ϵ(t), and W (t) is a standard Brownian motion (Feynman, 1964) in R[d]._

_Proof of Lemma C.4._

If we omit the residual terms, the iterate of D-SGD becomes

**wa(t+1) = wa(t)−η∇�L (wa(t)) + Tr(H(wa(t))Ξ[∗](t))�** + ηϵD(t)

= wa(t)−[∇L (wa(t)) + [1] �ηΣD(t)[√]ηϵ[∗],

2 **_[T][ (][w][a][(][t][)][)][ ⊗]_** **[Ξ][∗][(][t][)][]][η][+]**

where ϵD(t) ∼N (0, ΣD(t)) (Gaussian approximation) and ϵ[∗] is a standard Gaussian random variable.

For small enough constant learning rate η, we arrive at

dwa(t) = −�∇L (wa(t)) + [1] �dt + �ηΣD(t)dW (t).

2 **_[T][ (][w][a][(][t][)][)][ ⊗]_** **[Ξ][∗][(][t][)]**

The stochastic processes give a way to model D-SGD as a continuous-time evolution (i.e., SDE)
without ignoring the role of mini-batch noise if the learning rate is infinitesimal.


**Theorem 1 (Implicit regularization of D-SGD). Given the loss L is continuous and**
_has fourth-order partial derivatives._ _Denote the weight diversity matrix as Ξ(t)_ =
1 �m

_m_ _j=1[(][w][j]_ [(][t][)][−][w][a][(][t][)][)(][w][j] [(][t][)][−][w][a][(][t][)][)][T][, its diagonal matrix as][ Ξ][∗][(][t][)][, and the][ d][-dimensional all-ones]
_vector as 1. With probability greater than 1_ (η), the mean iterate of D-SGD becomes
_−O_

Eµj (t)∼D
_j=1,...,m[[][w][a][(][t][+1)][]]_


= wa(t)−η∇ �L (wa(t)) + [1] �

2 [Tr(][H][(][w][a][(][t][)][)][Ξ][∗][(][t][)][)]

� �� �
_the regularized loss_


1
+O(η 2 1)+O (η∥wj (t)−wa(t)∥2[3] **[1][)][,]**


_Under the mild assumptions in Lemma C.2, D-SGD implicitly regularizes_


_reg( wj_ (t) ) = _λH(wa(t)),1_
_j=1,...,m_ � �� �
_maximum Hessian eigenvalue_


Tr(Ξ(t))

_·_
� �� �
_consensus distance_


_._


_Proof of Theorem 1._

We start by rewriting the update of the global averaged model wa(t) of D-SGD as follows,

**wa(t+1) =wa(t)−η�** _∇L (wa(t))_ + ∇L (wa(t)) −∇L[µ][(][t][)] (wa(t))
unbiased gradient� �� � �gradient noise over the superbatch�� _µ(t)�_


+ [1]

_m_


_m_
�

[∇L[µ][j][ (][t][)] (wj (t)) −∇L[µ][j][ (][t][)] (wa(t))]
_j=1_


�.


� �� �
gradient diversity among workers

Analyzing the effect of the gradient diversity on the training dynamics of D-SGD on the general
non-convex losses is highly non-trivial. Technically, we perform a second-order Taylor expansion
(see Appendix A.2) on the gradient diversity around wa(t), omitting the high-order residuals R:


1

_m_


_m_
�

[∇L[µ][j][ (][t][)] (wj (t)) −∇L[µ][j][ (][t][)] (wa(t))]
_j=1_


-----

_m_
� **_T_** _[µ][j][ (][t][)](wa(t)) ⊗_ [(wj (t)−wa(t))(wj (t)−wa(t))[T] ].

_j=1_


= [1]

_m_


_m_
�

**_H_** _[µ][j][ (][t][)](wa(t))(wj_ (t)−wa(t)) + [1]

2m

_j=1_


Here H _[µ][j][ (][t][)](wa(t)) ≜_ _|µj1(t)|_ �|ζµ(tj)=1(t)| **_[H][(][w][a][(][t][)][;][ z][j,ζ][(][t][)][)][ stands for the empirical Hessian at][ w][a][(][t][)]_**

and T _[µ][j][ (][t][)](wa(t)) ≜_ _|µj1(t)|_ �|ζµ(tj)=1(t)| **_[T][ (][w][a][(][t][)][;][ z][j,ζ][(][t][)][)][ denotes the tensor containing all empirical]_**

third-order partial derivatives at wa(t), where µj (t) and zj,ζ(t) follows the notation in Equation (1).

Analogous to the works investigating the SGD dynamics (M et al., 2017; Zhu et al., 2019b; Ziyin et al.,
2022; Wu et al., 2022), we will calculate the expectation and covariance of the gradient diversity. The
expectation of gradient diversity is first calculated as follows. We defer the analysis of its covariance
to Subsection 4.3. Taking expectation over all local mini-batches µj (t) (j = 1, . . ., m) provides


_m_
� [∇L[µ][j][ (][t][)] (wj (t)) −∇L[µ][j][ (][t][)] (wa(t))] �

_j=1_


Eµj (t)∼D
_j=1,...,m_


1
�

_m_


=H(wa(t)) [1]

_m_


_m_
�

(wj (t)−wa(t))
_j=1_


+ [1]

2 **_[T][ (][w][a][(][t][)][)][ ⊗]_** [[ 1]m


_m_
�

(wj (t)−wa(t))(wj (t)−wa(t))[T] ]+R.
_j=1_


� =0�� �

The i-th entry of the above equation will be


_m_
� [∂iL[µ][j][ (][t][)] (wj (t)) −∂iL[µ][j][ (][t][)] (wa(t))] �

_j=1_


Eµj (t)∼D
_j=1,...,m_


1
�

_m_


= [1]

2


_m_

� �

_∂ikl[3]_ **_[L][(][w][a][(][t][)][) 1]m_** (wj (t)−wa(t))k(wj (t)−wa(t))l
_k,l_ _j=1_


+O (∥wj (t)−wa(t)∥2[3][)][,] (C.2)


� =∂i �kl _[∂]kl[2]_ **_[L][(][z][n][)][ 1]m_** �mj=1 [(]��[w][j] [(][t][)][−][w][a][(][t][))]k[(][w][j] [(][t][)][−][w][a][(][t][))]l �

where (wj (t)−wa(t))k denotes the k-th entry of the vector wj (t)−wa(t). The equality in the brace is
due to Clairaut’s theorem (Rudin et al., 1976).

According to Markov’s inequality and Assumption A.1, we obtain


1

_m_


_m_
�(wj (t)−wa(t))k(wj (t)−wa(t))l > η 21 }

_j=1_


Pr{� 1 �(wj (t)−wa(t))k(wj (t)−wa(t))l > η 21 }

_m_

_k≠_ _l_ _j=1_

E µj=1j (τ,...,m)∼D ([�]k≠ _l_ _m1_ �mj=1[(][w][j] [(][t][)][−][w][a][(][t][)][)][k][(][w][j] [(][t][)][−][w][a][(][t][)][)][l][)]
_τ_ =1,···,t−1

_≤_ _η_ 12

E µj=1j (τ,...,m)∼D ((d − 1) [�]k[m]=1 _m1_ �mj=1[(][w][j] [(][t][)][−][w][a][(][t][)][)]k[2][)]
_τ_ =1,···,t−1

_≤_ _η_ 12

E µj (τ )∼D ((d − 1) Tr(Ξ(t))
_j=1,...,m_
_τ_ =1,···,t−1

_≤_

_η_


= _[O][(]η[dη]12_ [2][)]

3
= (dη 2 ),
_O_

where d stands for the dimensionality of wj (t)−wa(t) and the penultimate equality is due to
Lemma C.3.


For sufficiently small η = o(d[−][2]), [1]2 _[∂][i]_ �

Equation (C.2) is of the order (η).
_O_


�m

_kl_ _[∂]kl[2]_ **_[L][ (][z][n][)][ 1]m_** _j=1_ [(][w][j] [(][t][)][−][w][a][(][t][)][)]k[(][w][j] [(][t][)][−][w][a][(][t][)][)]l [in]


-----

Then we derive that with probability greater than 1−O(η), the iterate of D-SGD can be written as

Eµj (t)∼D
_j=1,...,m[[][w][a][(][t][+1)][]]_


=wa(t)−η∇ �L (wa(t)) + [1] �

2 [Tr(][H][(][w][a][(][t][)][)][Ξ][∗][(][t][)][)]

� �� �
the regularized loss


+O(η[2]1)+O (η∥wj (t)−wa(t)∥2[3] **[1][)][,]**


where Ξ[∗](t) = [�]i[d]=1[⟨][e][i][e]i[T] _[,][ Ξ][(][t][)][⟩][F][ e][i][e]i[T]_ [is the diagonal of][ Ξ][(][t][)][.]

According to Lemma C.2, λH(wa(t)),1 Tr(Ξ(t)) scales positively with Tr(H(wa(t))Ξ[∗](t)):
_·_


0 ≤ Tr(H(wa(t))Ξ[∗](t)) ≤ _λH(wa(t)),1_
� �� �
sharpness


Tr(Ξ(t))

_·_
� �� �
consensus distance


_≤_ _d1 Tr(H(wa(t))Ξ[∗](t)),_


where λH(wa(t)),1 denotes the largest eigenvalue of H(wa(t)) and d1 stands for the marginal contribution of λH(wa(t)),1 on the full spectrum of H(wa(t)) (i.e., λH(wa(t)),1 = _dd1_ [Tr(][H][(][w][a][(][t][)][))][).]

Therefore, combined with Equation (3), we conclude that D-SGD also implicitly regularizes
_λH(wa(t)),1_ Tr(Ξ(t)). The proof is complete.
_·_


**Theorem 2. Suppose that the averaged gradient norm satisfies** _m1_ �mj=1 _[∥∇][L][ (][w][j]_ [(][t][)][)][∥][2][ ≤] [(1 +]

1−λ �m

4 [)][ 1]m _j=1_ _[∥∇][L][ (][w][j]_ [(][t][+1)][)][∥][2][, where][ 1][ −] _[λ][ denotes the spectral gap (see][ Definition A.2][). The]_

_sharpness regularization coefficient of D-SGD at t-th iteration is O(|µ(t)|[2](1 +_ _m[1]_ �mj=1 _|µj1(t)|_ [))][,]

_which increases with the total batch size_ _µ(t)_ _if we apply the linear scaling rule._
_|_ _|_

_Proof of Theorem 2._

Theorem 1 states that the regularization coefficient of λH(wa(t)),1 is η Tr(Ξ(t)). According to
Lemma C.3, Tr(Ξ(t)) satisfies

 


� �� �
independent of total batch size |µ(t)|


_m_
�

_∥∇L (wj_ (t))∥[2]
_j=1_


+ [1]

_m_


�m _µj_ (t) 2

Eµj (t)∼D ���∇L (wj (t)) −∇L (wj (t))���2.
_j=1_ 

� noise covariance, O( _|��µj1 (t)|_ [)][ (][M et al.][,][ 2017][)] �

(C.3)


Tr(Ξ(t)) = η[2]

_· O_










1

_m_


Given that we apply the linear scaling rule (see Subsection 4.2), we have η = ( _µ(t)_ ), which
_O_ _|_ _|_
completes the proof.

**Theorem 3 (Escaping efficiency of D-SGD). If the loss L is continuous and has fourth-order partial**
_derivatives, the escaping efficiency of D-SGD from minimum w[∗]_ _satisfies_

Ewa(t)[L(wa(t)) − **_L(w[∗])]_**

� _t_
= − Ewa(t)[∇L(wa(t))[T] _∇L(wa(t)) −_ [1]

0 2 _[grandsum][((][T][ (][w][a][(][t][)][)][∇][L][(][w][a][(][t][)][))][ ⊙]_ **[Ξ][∗][(][t][)][)]d][t]**

� _t_ _η_
+

0 2 [Tr (][H][(][w][a][(][t][)][)][Σ][D][(][t][)][) d][t,]

_where_ _denotes the Hadamard product (Davis, 1962), and the grandsum(_ ) (Merikoski, 1984) of a
_⊙_ _·_
_matrix_ **_M[˜]_** _satisfies grandsum( M[˜]_ ) = [�]i,j **_M[˜]_** _ij._

_Proof of Theorem 3._

Since L is continuous and has second-order partial derivatives, we can write


dL(wa(t)) = − (∇L(wa(t))[T] _∇L(wa(t)) −_ [1]

2

_[∇]�_ **_[L][(][w][a][(][t][)][)][T][ (][T]��[ (][w][a][(][t][)][)][ ⊗]_** **[Ξ][∗][(][t][)]�[)]**
grandsum((T (wa(t))∇L(wa(t)))⊙Ξ[∗](t))


)dt


-----

+ _[η]_ �ΣD(t) 12 H(wa(t))ΣD(t) 12 � dt + ∇L(wa(t))[T] **ΣD(t)dW (t),**

2 [Tr]

according to the Ito’s lemma (Øksendal, 2003). The term ∇L(wa(t))[T] **ΣD(t)dW (t) will be averaged**
if we take the expectation with respect to the distribution of wa(t). Finally, integrating over t will
provide

Ewa(t)[L(wa(t))]

� _t_ _η_
=L(w[∗]) +

0 2 [Tr (][H][(][w][a][(][t][)][)][Σ][D][(][t][)][) d][t]

� _t_
_−_ Ewa(t)[∇L(wa(t))[T] _∇L(wa(t)) −_ [1]

0 2 [grandsum][((][T][ (][w][a][(][t][)][)][∇][L][(][w][a][(][t][)][))][ ⊙] **[Ξ][∗][(][t][)][)]d][t,]**


which completes the proof.

**Proposition C.5 (Escaping efficiency of C-SGD). If the loss L has second-order partial derivatives,**
_the escaping efficiency of C-SGD from minimum w[∗]_ _satisfies_

Eµj (t)∼D
_j=1,...,m[[][L][(][w][a][(][t][+1)][)][ −]_ **_[L][(][w][∗][)]]_**


� _t_ � _t_
= − _∇L(wa(t))[T]_ _∇L(wa(t)) +_

0 0


_η_
2 [Tr (][H][(][w][a][(][t][)][)][Σ][C][(][t][)][) d][t,]


_where ΣC(t) denotes the covariance matrix of the gradient noise of C-SGD (Equation (1))._

The proof is analogous to Theorem 3.

**Definition C.1 (Sub-quadratic minimum). Given that the loss L is continuous and has second-order**
_partial derivatives, we call the mimimum w[∗]_ _of L δ-locally sub-quadratic if for any w in the open_
_punctured neighbourhood of w[∗]_ _(i.e., w ∈_ _U[˚](w[∗])), the following condition holds: (1) H(w[∗]) ≽_
**_H(w); and (2) ∃_** _α(w) ∈_ R[+], β(w) ∈ R[−] _s.t. H(w)(w_ _−w[∗]) = α(w)(∥w_ _−w[∗]∥2[β][(][w][)](w_ _−w[∗]))._

**Proposition 4. grandsum((T (wa(t))∇L(wa(t))) ⊙** **Ξ[∗](t)) is (1) zero on quadratic minima, (2)**
_positive on super-quadratic minima, and (3) negative on sub-quadratic minima._

_Proof of Proposition 4._

(1) quadratic minima.

It is obvious that on quadratic loss, grandsum((T (wa(t))∇L(wa(t))) ⊙ **Ξ[∗](t)) = 0 due to zero**
gradient diversity (see Corollary C.1).

(2) super-quadratic minima.

Performing the Taylor expansion of H(w) − **_H(w[∗]) around w[∗]_** provides

**_H(w) −_** **_H(w[∗]) ≈_** **_T (w)(w −_** **w[∗]) ≽** 0.

According to the definition of super-quadratic minima, we know that ∃ _α(w) ∈_ R[+], β(w) ∈ R[+] s.t.

**_T (w)(H(w)(w −_** **w[∗])) = α(w)∥w −** **w[∗]∥2[β][(][w][)]T (w)(w −** **w[∗]) ≽** 0.

Another Taylor expansion of ∇L(w) −∇L(w[∗]) around w[∗] will give


**_T (w)(_** **_L(w)_** **_L(w[∗])_**
_∇_ _−∇_
� ��0 �


) ≈ **_T (w)(H(w)(w −_** **w[∗])) = α(w)∥w −** **w[∗]∥2[β][(][w][)]T (w)(w −** **w[∗]) ≽** 0.


Then we arrive at grandsum((T (wa(t))∇L(wa(t))) ⊙ **Ξ[∗](t)) > 0 since Ξ[∗](t) is a diagonal matrix**
with all positive entries.

(3) sub-quadratic minima.

By the same token, we can prove that grandsum((T (wa(t))∇L(wa(t))) ⊙ **Ξ[∗](t)) < 0 on sub-**
quadratic minima.


-----


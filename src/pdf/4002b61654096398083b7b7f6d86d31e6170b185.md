# SYNC: SAFETY-AWARE NEURAL CONTROL FOR STA## BILIZING STOCHASTIC DELAY-DIFFERENTIAL EQUA- TIONS

**Jingdong Zhang[1,2], Qunxi Zhu[2][,*], Wei Yang[2,3][,*], Wei Lin[1,2,3,4][âˆ—]**

1 School of Mathematical Sciences, SCMS, SCAM, and CCSB, Fudan University,
Shanghai 200433, China
2 Research Institute of Intelligent Complex Systems, Fudan University, Shanghai 200433, China
3 Shanghai Artificial Intelligence Laboratory, China
4 MOE Frontiers Center for Brain Science and State Key Laboratory of Medical Neurobiology,
Fudan University, Shanghai 200032, China

{zhangjd20,qxzhu16,yangwei,wlin}@fudan.edu.cn

### ABSTRACT

Stabilization of the systems described by stochastic delay-differential equations
(SDDEs) under preset conditions is a challenging task in the control community.
Here, to achieve this task, we leverage neural networks to learn control policies
using the information of the controlled systems in some prescribed regions. Specifically, two learned control policies, i.e., the neural deterministic controller (NDC)
and the neural stochastic controller (NSC), work effectively in the learning procedures that rely on, respectively, the well-known LaSalle-type theorem and the
newly-established theorem for guaranteeing the stochastic stability in SDDEs. We
theoretically investigate the performance of the proposed controllers in terms of
convergence time and energy cost. More practically and significantly, we improve
our learned control policies through considering the situation where the controlled
trajectories only evolve in some specific safety set. The practical validity of such
control policies restricted in safety set is attributed to the theory that we further
develop for safety and stability guarantees in SDDEs using the stochastic control barrier function and the spatial discretization. We call this control as SYNC
(SafetY-aware Neural Control). The efficacy of all the articulated control policies,
including the SYNC, is demonstrated systematically by using representative control
problems.

### 1 INTRODUCTION

Stochastic delay-differential equations (SDDEs) (Mao, 1996; Lin & He, 2005; Sun & Cao, 2007;
Guo et al., 2016) have been widely applied to characterize the complex dynamical behavior emergent
in real-world systems with dependence on the current state, the past state, and the noise. Efficiently
controlling these systems is a long-standing and crucial problem, with the consequent emphasis
being placed on the design of control policies and analysis of stability in SDDEs. Traditional control
methods in stochastic settings have been fully developed in the convex optimization frameworks
using the control Lyapunov stability theory, e.g. the quadratic programming (QP) (Fan et al., 2020;
Sarkar et al., 2020). These methods cannot provide the analytical form of feedback controllers and
own a high computational cost, requiring solving QP problems at each iteration step. To overcome
these difficulties, utilizing neural networks (NNs) to automatically design controllers becomes one
of the mainstream approaches in recent years (Zhang et al., 2022; Chang et al., 2019). However,
existing machine-learning-based methods either focus on controlling systems without time-delay
or aim at learning the control Lyapunov function instead of the control policy (Khansari-Zadeh &

_âˆ—To whom correspondence should be addressed: Q.Z., W.Y. and. W.L.,_
[https://faculty.fudan.edu.cn/wlin/zh_CN/index.htm.](https://faculty.fudan.edu.cn/wlin/zh_CN/index.htm)


-----

Billard, 2014). All these, therefore, motivate us to design neural controllers for general nonlinear
SDDEs.


The safety verification of controlled systems
plays an important role in many branches of cybernetics and industry. For example, with the
safety verification, one can reduce a significant
economic burden and loss of life (Ames et al.,
2016; Wang et al., 2016). In particular, the dominant framework for safety control in stochastic
settings is the use of stochastic control barrier
function (SCBF) (Clark, 2019; 2021; Santoyo
et al., 2021). The core idea of designing a candidate SCBF is that its value tends to explode
as the systemâ€™s state leaves the safe region, implying a safety guarantee as long as one could
design a controller such that the SCBF is always
finite within the controlled time duration. Unfortunately, the existing theories of SCBF either
require a lot of inequality constraints or are limited in handling systems without any time delay.

In this paper, we utilize neural networks (NNs)
to learn control policies for SDDEs based on
the corresponding stability theories. Additionally, we develop a simplified SCBF theory for
SDDEs and then use it to construct the neural controller with a safety guarantee, named
SYNC. All these control policies are intuitively
depicted in Figure 1. The major contributions
of this paper include:


Initial position Target position

âˆ—

Safe region
0 Uncontrolled orbit

d ( ) = ( ( ), ( âˆ’), )d + ( ( ), ( âˆ’), )d

+ d + d

Trained

Section 3: NDC Section 4: NSC
(Theorem 2.2, (Theorem 4.1,
LaSalle-type s t o c h a s t i c
stabilization) (, âˆ’, ) stabilization) (, âˆ’, )

Section 5: SYNC
(Theorem 5.3, safety guarantee)

Section 6: Upper bounds Untrained
of SYNCâ€™s convergence NDC
time and energy cost

NSC

(Theorem 6.2, 6.1)

Figure 1: Overall work flow. Sketches of SYNC. Both
the NDC and NSC can stabilize the SDDEs to the target
unstable equilibrium x[âˆ—]. The safety-aware controlled
state trajectories are restricted in the safe region.

|+|+ d d|
|---|---|
|+|d + d|
|Trained Section 3: NDC Section 4: NSC (Theorem 2.2, (Theorem 4.1, LaSalle-type s t o c h a s t i c stabilization) (, âˆ’, ) stabilization) (, âˆ’, ) Section 5: SYNC (Theorem 5.3, safety guarantee)||
|Section 6: Upper bounds Untrained of SYNCâ€™s convergence NDC time and energy cost NSC (Theorem 6.2, 6.1)||



    - designing a novel and practical framework of neural deterministic control based on the
existing LaSalle-Type stability theory,

    - proposing a simplified stability theorem and designing the second novel neural stochastic
control framework that can benefit from noise according to this theorem,

    - establishing an SCBF theory for SDDEs as well as a theory of safety guarantee and stability
guarantee using neural network settings,

    - providing theoretical estimation for the proposed neural controller in terms of convergence
time and energy cost based on the developed theory of safety and stability guarantees, and

   - demonstrating the efficacy of the proposed neural control methods through numerical
comparisons with the typical existing control methods on several representative physical
systems.

### 2 PRELIMINARIES

To begin with, we consider the SDDE in a general form of

dx(t) = F (x(t), x(t âˆ’ _Ï„_ ), t)dt + G(x(t), x(t âˆ’ _Ï„_ ), t)dBt, t â‰¥ 0, Ï„ > 0, x(t) âˆˆ R[d], (1)

where x(t) = Î¾(t) âˆˆ _CF0_ ([âˆ’Ï„, 0]; R[d]) is the initial function, the drift term F : R[d] _Ã—_ R[d] _Ã—_ R+ â†’ R[d]
and the diffusion term G : R[d] _Ã— R[d]_ _Ã— R+ â†’_ R[d][Ã—][r] are Borel-measurable functions, and Bt is a
standard r-dimensional (r-D) Brownian motion defined on probability space (â„¦, F, {Ft}tâ‰¥0, P) with
a filtration {Ft}tâ‰¥0 satisfying the regular conditions. Without loss of generality, we assume that
_F_ (0, 0, t) = 0 and G(0, 0, t) = 0. This assumption guarantees that the zero solution x(t) **0 with**
_â‰¡_
_t_ 0 is an equilibrium of Eq. (1). Additionally, the following notations and assumptions are used
_â‰¥_
throughout the paper.


-----

**Assumption 2.1 Assume that Eq. (1) has a unique solution x(t, Î¾) on t â‰¥** 0 for any Î¾ âˆˆ
_CF0_ ([âˆ’Ï„, 0]; R[d]) and that, for every integer n â‰¥ 1, there is a number Kn > 0 such that

_âˆ¥F_ (x, y, t)âˆ¥âˆ¨âˆ¥G(x, y, t)âˆ¥F â‰¤ _Kn,_

_for any (x, y, t) âˆˆ_ R[d] _Ã— R[d]_ _Ã— R+ with âˆ¥xâˆ¥âˆ¨âˆ¥yâˆ¥â‰¤_ _n, where âˆ¥Â· âˆ¥_ _denotes the L[2]-norm and âˆ¥Â· âˆ¥F_
_denotes the Frobenius norm, i.e. âˆ¥G(x, y, t)âˆ¥F[2]_ [=][ ï¿½]i[d]=1 ï¿½rj=1 _[G][ij][(][x][,][ y][, t][)][2][.]_

**Definition 2.1 (Derivative Operator) Define the differential operator L associated with Eq. (1) by**


_d_
ï¿½ _âˆ‚[2]_

[G(x, y, t)G[âŠ¤](x, y, t)]ij _._

_âˆ‚xiâˆ‚xj_

_i,j=1_


_L â‰œ_ _[âˆ‚]_

_âˆ‚t_ [+]


_d_
ï¿½

_Fi(x, y, t)_ _[âˆ‚]_ + [1]

_âˆ‚xi_ 2

_i=1_


According to the above definition of the derivative operator, an operation of on the function
_L_
_V âˆˆ_ _C_ [2][,][1](R[d] _Ã— R+; R) yields:_

_LV (x, y, t) = Vt(x, t) + âˆ‡V (x, t)[âŠ¤]F_ (x, y, t) + [1] ï¿½G[âŠ¤](x, y, t)HV (x, t)G(x, y, t)ï¿½ _._ (2)

2 [Tr]

Here, Vt, âˆ‡V and HV represent, respectively, the time derivative, the gradient, and the Hessian
matrix of V . Notably, the following LaSalle-type stability theorem will be crucial to the establishment
of our partial results.
**Theorem 2.2 (Mao, 2002) Suppose that Assumptions 2.1 holds. Assumes there are functions V âˆˆ**
_C_ [2][,][1](X Ã— R+; R+), Î³ âˆˆ _L[1](R+; R+), and w1, w2 âˆˆ_ _C(X_ ; R+) such that LV (x, y, t) â‰¤ _Î³(t) âˆ’_
_w1(x) + w2(y), w1(x) â‰¥_ _w2(x), and limâˆ¥xâˆ¥â†’âˆ_ inf 0â‰¤tâ‰¤âˆ _V (x, t) = âˆ. Here, X âŠ‚_ R[d] _is the_
_state space. Then, Ker(w1 âˆ’_ _w2) Ì¸= âˆ…_ _and limtâ†’âˆ_ dist(x(t, Î¾), Ker(w1 âˆ’ _w2)) = 0 a.s., where_
Ker(w1 âˆ’ _w2) â‰œ_ _{x : w1(x) âˆ’_ _w2(x) = 0}, dist(x, K) â‰œ_ inf _yâˆˆK âˆ¥x âˆ’_ _yâˆ¥_ _for a set K âŠ†_ R[d], and
_a.s. stands for the abbreviation of almost surely._

**Problem Statement** We assume that the zero solution of the following SDDE:

dx(t) = f (x, x(t âˆ’ _Ï„_ ), t)dt + g(x, x(t âˆ’ _Ï„_ ), t)dBt (3)

is unstable, i.e. limtâ†’âˆ **_x(t; Î¾) Ì¸= 0 on some set of positive measures. We aim to stabilize the zero_**
solution using control based on neural networks (NNs). In other words, our goal is to leverage the
NNs to design an appropriate controller u = (uf _, ug) with uf_ (0, 0, t) = ug(0, 0, t) = 0 such that
the controlled system

dx = [f + uf (x(t), x(t âˆ’ _Ï„_ ), t)]dt + [g + ug(x(t), x(t âˆ’ _Ï„_ ), t)]dBt (4)

is steered to the zero solution. We call uf : R[d] _Ã— R[d]_ _Ã— R+ â†’_ R[d] as deterministic control while
we call ug : R[d] _Ã— R[d]_ _Ã— R+ â†’_ R[d][Ã—][r] as stochastic control, since they are integrated with dt and
dBt, respectively. The major difficulty of this problem comes from the non-Markovian property
of SDDEs. As such, we cannot apply the Markov decision process (MDP)-based methods, such
as the reinforcement learning, to control SDDEs. The majority of existing works prefer to learn
deterministic control and often regard the noise as a negative ingredient that may destroy the natural
dynamics of f . In what follows, we not only show that the deterministic control can achieve
stabilization in a probability sense, but also that elaborately-designed stochastic control can make
the same stabilization. This, therefore, yields two frameworks, viz., the neural deterministic control
(Section 3) and the neural stochastic control (Section 4). We make all our code and data available at
[https://github.com/jingddong-zhang/SYNC.](https://github.com/jingddong-zhang/SYNC)

### 3 NEURAL DETERMINISTIC CONTROL

In this section, we propose the neural deterministic controller (NDC) based on the Theorem 2.2 to
stabilize system (3). Heuristically, we construct the neural network form auxiliary functions and
control functions, and integrate the sufficient conditions in the theorem into the loss function to find
the neural controller that satisfies the expected conditions. However, the NDC can neither be used to
find stochastic controllers nor rigorously satisfy the expected stability conditions. These problems
will be addressed in Section 4 and 5.


-----

3.1 METHOD: LEARNING CONTROL AND AUXILIARY FUNCTIONS

The core idea of our method is base on using Theorem 2.2, that is, once we construct the auxiliary
functions V, Î³, w1, w2 and the neural controller u to meet all the conditions assumed in Theorem 2.2
for the controlled system (4), the solution x(t; Î¾) converges to the Ker(w1 âˆ’ _w2). In particular, if we_
set Ker(w1 âˆ’ _w2) = {0}, the unstable zero solution of the control-free system (3) can be stabilized._
To this end, we first provide appropriate constructions of NNs to learn these candidate functions.
Thus, we design the explicit form of the loss function in the learning step.

**Auxiliary Function** We employ a multi-layer feedforward neural network, denoted by NN(Â·; Î¸), to
design all the functions. Precisely, Î¸1 is the parameter vector of the positive function V (x, t; Î¸1), and
the L2 term âˆ¥xâˆ¥[2] is added to guarantee limâˆ¥xâˆ¥â†’âˆ inf 0â‰¤t<âˆ _V (x, t; Î¸1) = âˆ, that is_

_V (x, t; Î¸V ) = NN(x, t; Î¸V )[2]_ + Îµâˆ¥xâˆ¥[2], Îµ > 0. (5)

In our framework, it requires V âˆˆ _C_ [2][,][1](R[d] _Ã— R+). We therefore use a C_ [2] activation function for
an NN, such as the hyperbolic tangent function, Tanh(Â·). We further discuss the impact of the L2
term in Appendix A.1.3. In order to design an integrable positive function Î³(t) with the NN, we use
an activation function with at most linear growth such as ReLU and multiply an exponential decay
factor to the output of the NN, that is

_Î³(t; Î¸Î³) = exp(âˆ’ct) Â· NN(t; Î¸Î³)[2], c > 0._ (6)

For simplicity, we design w(x, Î¸w) = NN(x; Î¸w)[2] as a positive function. Additionally, we set

_w2 = w, w1 = w + p(x), p â‰¥_ 0, Ker(p) = {0}. (7)

**Deterministic Control Function** We first consider the deterministic control, i.e. u = (uf _, 0). To_
guarantee the same zero solution of the control-free system (3) and the controlled system (4), the NDC
**_uf : R[d]_** _Ã—_ R[d] _Ã—_ R[+] _â†’_ R[d] should satisfy uf (0, 0, t) = 0. One feasible way to meet such a condition
is to set uf (x, y, t) = NN(x, y, t; Î¸f ) âˆ’ **NN(0, 0, t; Î¸f** ) or uf (x, y, t) = diag(x)NN(x, y, t; Î¸f ).
Here, diag(x) is a diagonal matrix with xi as its i-th diagonal element.

**Loss Function** Once the learned functions V, Î³, w1, w2 and u with the coefficient functions,
_fu â‰œ_ _f + u and g, in the controlled system (4), meet all the conditions assumed in Theorem 2.2, the_
stability of zero solution is naturally assured. To achieve this, we demand a suitable loss function to
evaluate the likelihood that those conditions are satisfied. It can be seen from our construction that
the only condition needed to be satisfied is LV (x, y, t) â‰¤ _Î³(t) âˆ’_ _w1(x) + w2(y). Hence, we define_
LaSalleâ€™s loss function for the controlled system (4) as follows.

**Definition 3.1 (LaSalleâ€™s Loss) Consider the above parameterized candidate functions V, Î³, w1, w2**
and a controller uf for the controlled system (4). Then, LaSalleâ€™s loss is defined as


_LN,Îµ,c,p(Î¸V, Î¸Î³, Î¸w, Î¸f_ ) = [1]

_N_


_N_
ï¿½

max (0, LV (xi, yi, ti) âˆ’ _Î³(ti) + w1(xi) âˆ’_ _w2(yi)),_ (8)
_i=1_


where {xi, yi, ti}i[N]=1 [are sampled from some distribution][ Âµ][ on][ R][d][ Ã—][ R][d][ Ã—][ R][+][.]

In summary, the developed NDC framework is shown in Algorithm 1 in Appendix A.3.1.

**Remark 3.1 The proposed NDC framework can be easily applied to the autonomous SDDE: dx(t) =**
_f_ (x, x(t âˆ’ _Ï„_ ))dt + g(x, x(t âˆ’ _Ï„_ ))dBt. In particular, one can simply consider the autonomous
_auxiliary function V and the control function, and set Î³(t) = 0. For sample distribution Âµ(â„¦), here_
_we select the uniform distribution on a sufficiently large and closed region â„¦_ _as used in (Han et al.,_
_2016; Chang et al., 2019), and we include further analyses for the impact of Âµ in Appendix A.2.1._

3.2 NUMERICAL AND ANALYTICAL INVESTIGATIONS

**Comparison Studies** Recent works on controlling time-delayed systems mainly focus on elaborately designing the analytical form of control to satisfy the conditions in the LaSalle-Type Theorem
2.2 (Lin & He, 2005; Xu et al., 2014), or simultaneously designing control and the Lyapunov function


-----

1


1

3


5


y3 y3


3

y2 drive y2

5


y2 y2 7 7

(a) y1 (b) y1 (c) -0.1 0 t 0.25 0.5 (d)-0.1 0 t 0.25 0.5

Figure 3: (a) The original driving-response model, (b) the controlled orbits under LC and NDC, (c)
the time trajectory of y2 with autonomous noise, and (d) the nonautonomous noise. The solid lines
are obtained through averaging the 10 sampled trajectories, while the shaded areas stand for the
standard errors.
to satisfy the conditions based on the Lyapunov theory (Yu & Cao, 2007). It should be noted that all
these methods require a delicate design of functions for specific dynamics, and thus are limited in
practical application for controlling general time-delayed systems. However, our neural method leverages NNs to automatically learn the control policies, and can be applied in any kind of time-delayed
systems with stochastic settings. In Figure 3, we numerically compare the NDC and a baseline, the
linear control (LC) proposed in (Lin & He, 2005), on a noised driving-response Chuaâ€™s circuit. Here,
Chuaâ€™s circuit is a three-dimensional autonomous dynamical system with a unique nonlinear element,
producing typical chaotic dynamics (Matsumoto, 1984). In the simulation, we show that the NDC can
find the neural control for the response system y = (y1, y2, y3) with the autonomous and even the
nonautonomous time-delay noise. Actually, the nonautonomous time-delay noise was not considered
in (Lin & He, 2005). The simulation configurations are described in Appendix A.3.4.


**Failure in Finding Stochastic Control** As we can see that 0.10
the NDC performs well, a natural idea is to utilize the noise
part to achieve the stabilization of the SDDE (3). To ex- 0.05
plore this idea, we adopt the same NN of uf, design ug = 0.00 0 500 1000
**NN(x, y, t; Î¸g), and train its parameters Î¸g with LaSalleâ€™s** Training Epoch
loss (8). However, in Figure 2, we show that the loss can- Figure 2: Training loss for 1-D SDDE.
not converge to zero in controlling a simple 1-D toy system via the stochastic controller ug:
dx(t) = [x(t) + x(t âˆ’ _Ï„_ )]dt + [x(t âˆ’ _Ï„_ ) + ug(x(t), x(t âˆ’ _Ï„_ ); Î¸g)]dBt. Actually, this phenomenon can be analytically explained. Notice that Î¸g arises in loss function as a quadratic term
_l(Î¸g) =_ 12 [Tr[][u]g[âŠ¤][H][V][ u][g][]][ according to Eq. (2), the sign of this term depends on the convexity]
of V, i.e. the maximum eigenvalueâ€™s sign of _V . Nevertheless, the positive function V with_
_H_
limâˆ¥xâˆ¥â†’âˆ _V (x, t) = âˆ_ implies l(Î¸g) â‰¥ 0 for most of time. Hence, when we minimize l(Î¸g) â‰¥ 0
in the training procedure, the ideal case l(Î¸g) = 0 is equivalent to ug = 0. This indicates that we
are unable to learn a stochastic controller under LaSalleâ€™s loss (8) satisfying the sufficient conditions
assumed in Theorem 2.2.

### 4 NEURAL STOCHASTIC CONTROL


To find the neural stochastic controller (NSC), we provide the following theoretical result on stabilization of general stochastic functional differential equations (SFDEs) with the proof provided in
Appendix A.1.4. Since the failure of NDC in the stochastic control case comes from the positive
number contributed by the diffusion term, we aim at constructing stability condition such that the part
related to the diffusion term can be negative. We further explain the Theorem 4.1 in Appendix A.1.4.
**Theorem 4.1 (Stochastic Stabilization) Consider the SFDE dx(t) = F** (xt, t)dt + G(xt, t)dB(t),
_with F, G being locally Lipschitzian functions, F_ (0, t) = 0, and G(0, t) = 0. For every M > 0,
_assume that minâˆ¥xt(0)âˆ¥=M âˆ¥xt(0)[âŠ¤]G(xt, t)âˆ¥_ _> 0. If there exists a number Î± âˆˆ_ (0, 1) such that

_âˆ¥xt(0)âˆ¥[2](2âŸ¨xt(0), F_ (xt, t)âŸ© + âˆ¥G(xt, t)âˆ¥F[2] [)][ âˆ’] [(2][ âˆ’] _[Î±][)][âˆ¥][x][t][(0)][âŠ¤][G][(][x][t][, t][)][âˆ¥][2][ â‰¤]_ [0][,] (9)


_for xt âˆˆ_ _C([âˆ’Ï„, 0], X_ ), where xt(s) = x(t + s) for s âˆˆ [âˆ’Ï„, 0] and X is the state space. Then, the
_solution of the SFDE satisfies limtâ†’âˆ_ **_x(t; Î¾) = 0 a.s. for any Î¾ âˆˆ_** _CF0_ ([âˆ’Ï„, 0]; R[d]).

**Remark 4.2 The SFDE in Theorem 4.1 is formulated in a very general type, including the SDDE**
dx(t) = F (x(t), x(t âˆ’ _Ï„1), Â· Â· Â·, x(t âˆ’_ _Ï„q), t)dt + G(x(t), x(t âˆ’_ _Ï„1), Â· Â· Â·, x(t âˆ’_ _Ï„q), t)dBt with_


-----

_Ï„1 < Ï„2 < Â· Â· Â· < Ï„q âˆˆ_ [0, Ï„ ]. This indicates that our framework can be generalized to stabilize the
_SDDEs with multiple delays and even more general SFDEs as well._

In light of Theorem 4.1, we establish a more general framework for learning a neural controller
of system (4) with the form u = (uf _, ug) designed in the same NN architecture as the one used_
in the NDC framework. We focus on stochastic control with uf = 0 and provide more control
combinations in Appendix A.3.3, whereas the loss function is differently designed as follows.

**Definition 4.1 (Asymptotic Loss) Utilize the notations set in Definition 3.1 and gu = g + ug. The**
loss function for the controlled system (4) with the controller u is defined as:


ï¿½ max ï¿½0, (Î± âˆ’ 2)âˆ¥xiâŠ¤gu(xi, yi, ti)âˆ¥2 + âˆ¥xiâˆ¥2(2âŸ¨xi, f (xi, yi, ti)âŸ© + âˆ¥gu(xi, yi, ti)âˆ¥2F[)]ï¿½ï¿½,


_LÂµ,Î±(Î¸) = N[1]_


_N_
ï¿½


_i=1_

(10)
where Î¸ = (Î¸f _, Î¸g). Akin to Definition 3.1, we use the empirical loss function for training._

Here, Î± is an adjustable parameter, which is related to the convergence rate and the control energy.
We further discuss the design of the asymptotic loss in Appendix A.2.2 and numerically investigate
the role of Î± in Appendix A.4.1. We summarize the framework in Algorithm 2 in Appendix A.3.1.
And we further compare the computational complexity in Appendix A.3.2.


4.1 EXPERIMENTS OF THE COMBINATION METHODS

Table 1: Results on kinematic bicycle model.

We compare our neural control methods on a noiseperturbed kinematic bicycle model for car-like vehicles (Rajamani, 2011) in terms of the convergence Tt _E0.001_ Nd E[Ï„0.001]
time and the energy cost, which are two important NDC 1028.81s 102.17 6.3e-4 1.81
indexes to measure the quality of a controller (Yan NSC **59.80s** **62.10** 4.0e-7 0.29

QP                        -                        - 0.016 _> 5_

et al., 2012; Li et al., 2017; Sun et al., 2017). To
quantify the energy cost in the control process, we first denote by Ï„Ïµ â‰œ inf{t > 0 : âˆ¥x(t)âˆ¥ = Ïµ}
the stopping time and then by EÏµ â‰œ E ï¿½ï¿½0Ï„Ïµ ï¿½âˆ¥uf _âˆ¥[2]_ + âˆ¥ugâˆ¥[2][ï¿½] dtï¿½ the energy cost. We approximate

this expectation value by the empirical value as _N[1]_ ï¿½Ni=1 ï¿½0Ï„Ïµ[i] ï¿½âˆ¥u[i]f _[âˆ¥][2][ +][ âˆ¥][u]g[i]_ _[âˆ¥][2][ï¿½]dt through the Monte_

Carlo sampling. We show the results in Figure 4 and in Table 1 as well. Table 1 includes the training
time (Tt), empirical energy cost E0.001, nearest distance (Nd) between the bicycle and target position,
and empirical expectation E[Ï„0.001] for different methods. We include more experimental details in
Appendix A.3.5. We can see that the ranking of the comprehensive performance is NSC > NDC >
QP. This means that we can really benefit from introducing noise in the control protocol. This is
reasonable because, when we regard the energy cost as an objective function for minimization, the
randomness is more likely to lead this functional to the shortest path, akin to the common case where
the stochastic gradient descent outperforms the full-batch gradient descent. We show the NSC can
enlarge the region of attraction of the 100-D gene regulatory networks in Appendix A.4.2.


**Uncontrollable Fluctuation** The neural stochastic method we propose outperforms the control
methods including the deterministic control. However, the method can cause uncontrollable fluctuation due to the stochasticity. In practice, we always want to bound this perturbation owing to physical
and engineering restrictions in the real world. We tackle this safety guarantee problem in Section 5.


Without contorl


-0.1 0 0.25 0.5


4

2

0

-2


2

0


NDC

-0.1 1 2 3


2

1


0


2

1


QP

-0.1 1 2 3


0


x y


Target posiï¿½on (0, 0)


-0.1 0 0.25 0.5

t


Figure 4: (Left) A schematic diagram of the kinematic bicycle model. (Right) Time trajectories of the
state variables x, y of the kinematic bicycle under different control cases. The solid lines are obtained
through averaging the 10 sampled trajectories, while the shaded areas stand for the standard errors.


-----

### 5 SAFETY GUARANTEE FOR SDDES

In this section, we study the safety and stability guarantees

â„’â„â‰¥âˆ’ğœ†â„= 0

for the SYNC framework. Based on the stochastic control

ğ‘Ÿ

barrier functions, we establish an analytical result on the

ğ‘¥(ğ‘¡)

safety guarantee problem for SDDEs, which guarantees ğ‘¥(ğ‘¡+ 1)
that the process x(t; Î¾) satisfies the safety constraint, i.e., Safety region â„ğ‘¥â‰¥0
**_x(t; Î¾) âˆˆ_** int(C) for all t with the initial value Î¾(0) âˆˆ â„ğ‘¥< 0
int( ). Here, = **_x : h(x)_** 0 is a compact set
_C_ _C_ _{_ _â‰¥_ _}_

Figure 5: Diagram of the safety guarantee.

and the local Lipschitz function h: R[d] _â†’_ R is called We check the safety condition on discretizaa stochastic control barrier function (SCBF). Inspired by

tion points with mesh r.

(Lechner et al., 2022), we prove that the safety and stability conditions for NN form functions can be
guaranteed through a stronger condition on finite samples. We include the analytical proofs for all
the results in Appendix A.1.

**Definition 5.1 A continuous function Î± : (âˆ’b, +âˆ) â†’** (âˆ’âˆ, +âˆ) is said to be of an extended
_class-_ _function for some b > 0 if it is strictly increasing and Î±(0) = 0._
_K_

**Baseline** We extend the recent results on stochastic control barrier functions in SDEs (Clark, 2019)
to the SDDEs and summarize the results in Proposition 5.1. With this proposition and Theorem 2.2,
the traditional deterministic control methods based on the Quadratic Program (QP) in (Fan et al.,
2020; Sarkar et al., 2020) can be applied to test on the SDDEs. We use this QP method as the baseline
and the specific algorithm is shown in Appendix A.3.1. We also take the classic MPC method as the
baseline.

**Proposition 5.1 Let the function B: R[d]** _â†’_ R be locally Lipschitz and twice-differentiable on
int(C). If there exist three extended class-K functions Î±1,2,3(x) such that [Î±1(h(x))][âˆ’][1] _â‰¤B(x) â‰¤_

[Î±2(h(x))][âˆ’][1], and LB(x, y, t) â‰¤ _Î±3(h(x)) for the SDDE in (1). Then, Pï¿½x(t) âˆˆ_ int(C)ï¿½ = 1 for
_all t, provided with x(0)_ int( ).
_âˆˆ_ _C_

A natural idea is to integrate Proposition 5.1 into our proposed neural control framework, but the
main drawback in the usage of this proposition is that (x) is unbounded on, lacking Lipschitz
_B_ _C_
continuity. This drawback makes it impossible to fulfill the expected conditions only through
numerical verification on finite samples. To conquer the difficulty, we propose the following theorem
for safety guarantee, which, we believe, is a significant promotion of the existing barrier function
theory.

**Theorem 5.2 For the SDDE specified in (1), where F and G satisfy locally Lipschitz condition**
_and locally linear growth condition, if there exists an extended class-_ _function Î»(x) such that_
_K_
_h_ _Î»_ _h for x_ _, where_ _represents the function composition,_ _is compact and_ _._
_L_ _â‰¥âˆ’_ _â—¦_ _âˆˆD_ _â—¦_ _D_ _C âŠ‚D_
_Then, the solution satisfies P(x(t; Î¾) âˆˆ_ int(C)) = 1 for any Î¾ âˆˆ _CF0_ ([âˆ’Ï„, 0]; R[d]) with Î¾(0) âˆˆ int(C).

**Discretization and Safety Guarantee.** Based on the Theorem 5.2, we can construct a neural
candidate class- function Î» and combine it with the NDC and NSC to learn a safe controller, where
_K_
the candidate Î» is required to satisfy the condition assumed in Theorem 5.2. However, the main
difficulty is to guarantee the condition for every point x, since, in practice, we can basically
_âˆˆD_
guarantee this condition on a finite number of training data [Ëœ] with [Ëœ] being a discretization of .
_D_ _D_ _D_
Surprisingly, the following theorem suggests that we only need to check a slightly stronger condition
on a finite number of states in [Ëœ] in order to establish the safety guarantee on the whole .
_D_ _D_

**Theorem 5.3 Let M = M(F, G, h, Î», D) be the maximum of the Lipschitz constants of Lh and**
_Î»_ _h on_ _. Also, let r be the mesh size of_ [Ëœ]. Thus, for each x _, there exists Ëœx_ _such that_
_â—¦_ _D_ _D_ _âˆˆD_ _âˆˆ_ _D[Ëœ]_
_âˆ¥x âˆ’_ **_xËœâˆ¥2 < r. Suppose there exists a non-negative constant Î´ â‰¤_** _Mr such that_

_h_ _Î»_ _h + 4Mr_ _Î´,_ **_x_** _._ (11)
_âˆ’L_ _âˆ’_ _â—¦_ _â‰¤_ _âˆ€_ _âˆˆ_ _D[Ëœ]_

_Then, Î» satisfies the safety condition specified in Theorem 5.2._

**Remark 5.4 Here, the non-negative Î´ is regarded as the tolerance error in the training stage. So,**
_practically, we terminate the training until the safety loss is smaller than Mr._

|â„’â„â‰¥âˆ’ğœ†|Col2|â„ = 0|Col4|Col5|Col6|ğ‘Ÿ|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
|S|||ğ‘¥(ğ‘¡)||ğ‘¥(ğ‘¡+ 1)||
||S|afety region|||â„ğ‘¥ â‰¥0||
||||||||
|â„ğ‘¥ < 0|||||||


â„’â„â‰¥âˆ’ğœ†â„= 0

ğ‘Ÿ

ğ‘¥(ğ‘¡)
ğ‘¥(ğ‘¡+ 1)

Safety region â„ğ‘¥â‰¥0

â„ğ‘¥< 0


-----

(a) (b) Without control (c) Baseline (d) NSC (e)

3 2


NSC+Safe

0 0.2 0.4
t


Î¸ > Ï€


0


2

1


1

0


0

|Col1|Q M|P PC|
|---|---|---|
||||
||||


Î¸

Î¸ > Ï€


0 0.2 0.4
t


0 0.2 0.4
t


0 0.2 0.4
t


Figure 6: Schematic diagram of inverted pendulum task (a). The Î¸ component of the original system
(b), under baseline control (c), under NSC (d), and under our proposed safe control (e). The solid
lines are obtained through averaging the 5 sampled trajectories, while the shaded areas stand for the
standard errors.

**Construct Neural Networks with Bounded Lipschitz Constant.** We can define the loss function
for safety in the manner of the left-hand side in (11). However, M depends on the Lipschitz constants
of the NN functions Î» and u, which probably makes it complex and difficult to train the loss function.
To simplify the loss function, we construct the NNs with bounded Lipschitz constants for Î» and u.
Specifically, we add the spectral normalization for the neural control function to constrain its Lipschitz
constant lower than 1 (Miyato et al., 2018; Yoshida & Miyato, 2017). We apply the monotonic NNs
to construct the candidate extended class-K function as Î»Î¸Î» (x) = ï¿½0x _[q][Î¸][Î»]_ [(][s][)d][s][, where][ q][Î¸][Î»] [(][Â·][)][, the]
output of the NNs, is definitely positive (Wehenkel & Louppe, 2019). To constrain the Lipschitz
constant of Î»Î¸Î», we modify the integral formula as Î»Î¸Î» (x) = ï¿½0x [min][{][q][Î¸][Î»] [(][s][)][, M][Î»][}][d][s][, where][ M][Î»][ is]
a predefined hyperparameter. Thus, the Lipschitz constant of Î»Î¸Î» is smaller than MÎ». Therefore, we
can calculate M from the considered functions and MÎ». Other Lipschitz regularization methods can
be applied in our framework (Gouk et al., 2021; Liu et al., 2022) as well.

**SYNC Algorithm:** We define the loss function for the safety guarantee of the controlled system (4)
as follows, (the specific algorithms are summarized in Algorithm 1 and 2)


1 ï¿½
_L ËœD,MÎ»_ [(][Î¸][,][ Î¸][Î»][) =] [Ëœ] max {0, âˆ’Lh(x, y) âˆ’ _Î»Î¸Î»_ (h(x)) + 4Mr} . (12)

_|D|[2]_ (x,y)âˆˆDÃ—[Ëœ] _D[Ëœ]_

We add this loss to equation 8 and equation 10, respectively, to separately train the NDC and NSC. To
obtain the safety guarantee, we terminate the training process once L ËœD,MÎ» [(][Î¸][,][ Î¸][Î»][)][ is less than][ Mr][.]

**From Safety Guarantee to Stability Guarantee.** Akin to the safety guarantee, we provide the
stability guarantee for the candidate neural control functions satisfying the condition in Theorems 2.2
and 4.1. However, both theorems require their conditions to be valid for every point x âˆˆX âŠ‚ R[d],
while, in practice, it is impossible to obtain a finite discretization or a bounded Lipschitz constant on
the unbounded . Ingeniously, this difficulty can be conquered with the help of the safety guarantee
_X_
since the safety condition restricts where is compact. As such, we can establish theoretical
_X âŠ‚D_ _D_
results on stability guarantee for NDC and NSC in a similar manner as that in Theorem 5.3. We thus
summarize all these results in Appendix A.1.8.

We test the proposed safe control method to suppress the fluctuations emergent in the control process
on the task of controlling noise-perturbed inverted pendulum with time-delay. This control task is a
standard nonlinear control problem for testing different control methods (Anderson, 1989; Huang &
Huang, 2000). We apply the safe control method to steer the system to the upright position without
rotating a semi-circle, i.e. _Î¸_ _Ï€. The results are shown in Figure 6 and the experimental details are_
_|_ _| â‰¤_
provided in Appendix A.3.6. It is observed that the safe control method significantly outperforms the
baseline and the stochastic control method in terms of stabilization and safety guarantee.


### 6 THEORETICAL RESULTS FOR NDC AND NSC
We have mentioned the stopping time and the energy cost in section 4.1 and numerically compare
the proposed neural controllers with these indexes. These two indexes are the classic factors to
measure the performance of the controller (Sun et al., 2017). From the construction in Section 5,
we circumscribe the Lipschitz constant ku of the control function. Based on the safety and stability
guarantee, the neural controller thus satisfies the conditions assumed in Theorems 2.2 and 4.1.


-----

Then, we have the following theoretical results and include their proofs in Appendix A.1.9.
**Theorem 6.1 (Estimation for NDC) Consider the SDDE with NDC controller as**

dx(t) = (f (x, x(t âˆ’ _Ï„_ )) + uf (x(t), x(t âˆ’ _Ï„_ ))dt + g(x(t), x(t âˆ’ _Ï„_ ))dBt, x(0) = x0 âˆˆ R[d],

where âˆ¥f (x, y) âˆ’ _f_ (Â¯x, Â¯y)âˆ¥âˆ¨âˆ¥uf (x, y) âˆ’ **_uf_** (Â¯x, Â¯y)âˆ¥â‰¤ _L(âˆ¥x âˆ’_ **_xÂ¯âˆ¥_** + âˆ¥y âˆ’ **_yÂ¯âˆ¥). Assume that the_**
controlled system satisfies the conditions assumed in Theorem 2.2 and Remark 3.1 with Ker(w1 âˆ’
_w2) = 0. Denote by Î·Îµ = inf{t > 0 : âˆ¥x(t)âˆ¥_ = Îµ} the stopping time and by E(Î·Îµ, T ) =
E[ï¿½0Î·Îµâˆ§T _âˆ¥u(x(s), x(s âˆ’_ _Ï„_ ))âˆ¥[2]ds] the corresponding energy cost in the control process with Ïµ <
_âˆ¥x0âˆ¥. Thus, using the same notations in Theorem 2.2, we have_


ï£±
ï£´ï£´ï£´ï£²

ï£´ï£´ï£´ï£³


E[Î·Ïµ] â‰¤ _TÏµ =_ _V (x0) âˆ’_ minâˆ¥xâˆ¥=Îµ V (x) + ï¿½âˆ’0Ï„ _[w][2][(][Î¾][(][s][))d][s]_ _,_

minâˆ¥xâˆ¥â‰¥Îµ(w1(x) âˆ’ _w2(x))_

_E(Î·Ïµ, TÏµ) â‰¤_ _ku[2]_ _[C][0]_ ï¿½exp ï¿½4(L[2] + L + ku)TÎµï¿½ _âˆ’_ 1ï¿½ + ï¿½ 0 _ku[2]_ _[Î¾][2][(][s][)d][s.]_

2(L[2] + L + ku) _âˆ’Ï„_


where C0 = âˆ¥x0âˆ¥[2] + (2L[2] + L + ku) ï¿½âˆ’0Ï„ _[Î¾][(][s][)][2][d][s][ and][ Î¾][ âˆˆ]_ _[C][[][âˆ’][Ï„,][ 0]][ is the initial data.]_

We provide the similar theoretical results for NSC in Appendix A.1.10.

### 7 RELATED WORKS

**Stability Theory of SDDEs.** The early endeavors to develop the stability theory for SDDEs were
attributed to (Mao, 1999; 2002) inspired by LaSalleâ€™s theory (LaSalle, 1968). The subsequent
developments have been systematically and fruitfully achieved in the last twenty years in the control
community Appleby (2003); Song et al. (2014); Liu et al. (2016); Zhu (2018); Peng et al. (2021).
These works reveal the positive effect of multiplicative noise to the stochastic dynamics with delays,
and motivate us to develop only neural stochastic control to stabilize dynamical systems.

**Finding Stabilization Controller.** Traditional control methods focus on transforming control
criteria, such as the control Lyapunov functions (CLFs), into the QP (Fan et al., 2020; Sarkar et al.,
2020) or the semi-definite planning (SDP) problems (Henrion & Garulli, 2005; Jarvis-Wloszek et al.,
2003; Parrilo, 2000) to find optimal control iteratively. These methods have high computational
complexity since they cannot give the closed form of the control. Hence, machine-learning-based
control methods have been introduced to improve the generalization and efficiency of the original
convex optimal problems (Khansari-Zadeh & Billard, 2014; Ravanbakhsh & Sankaranarayanan,
2019; Gurriet et al., 2018). However, all the existing learning methods consider dynamics without
time-delay (Wagener et al., 2019; Williams et al., 2018; Chang et al., 2019; Zhang et al., 2022).

**Theory and Application of Control Barrier Function** The barrier function method has been
extensively researched in the problem of safety verification of controlled dynamics (Prajna & Jadbabaie, 2004; Jankovic, 2018; Prajna et al., 2004; Clark, 2019; 2021). Existing works for constructing
barrier functions in applications typically based on quadratic programming (Ames et al., 2014; 2016;
Khojasteh et al., 2020; Fan et al., 2020). Machine learning methods have also been introduced in safe
control fields in (Robey et al., 2020; Dean et al., 2020; Taylor et al., 2020).

### 8 DISCUSSION

We heuristically design two kinds of neural controllers for SDDEs based on the classic LaSalle-type
stabilization theory and the newly proposed stochastic stabilization theorem. To assure the controlled
trajectories can stay in the safety region, we cultivate the safety guarantee theorem through the SCBF
and the discretization techniques. Since the state space of the controlled SDDEs with safety guarantee
is bounded by the compact safety region, we can similarly deduce the stability guarantee theorem
for neural controllers through spatial discretization. Furthermore, we theoretically and numerically
investigate the neural controllersâ€™ performance in terms of convergence time and energy cost. The
proposed neural controllers with safety and stability guarantee are summarized as SYNC, which
significantly simplifies the process of control design and has extensive potential in different control
fields, such as financial engineering (Zhou & Li, 2000).


-----

### 9 ACKNOWLEDGMENTS

We thank the anonymous reviewers for their valuable and constructive comments that helped
us to improve the work. Q.Z is supported by the China Postdoctoral Science Foundation (No.
2022M720817), by the Shanghai Postdoctoral Excellence Program (No. 2021091), and by the STCSM
(Nos. 21511100200 and 22ZR1407300). W.Y. is supported by the STCSM (Nos. 21511100200,
22ZR1407300 and 22dz1200502). W.L. is supported by the National Natural Science Foundation of China (No. 11925103) and by the STCSM (Nos. 22JC1402500, 22JC1401402, and
2021SHZDZX0103).

### REFERENCES

Uri Alon. An introduction to systems biology: design principles of biological circuits. Chapman and
Hall/CRC, 2006.

Aaron D Ames, Jessy W Grizzle, and Paulo Tabuada. Control barrier function based quadratic
programs with application to adaptive cruise control. In 53rd IEEE Conference on Decision and
_Control, pp. 6271â€“6278. IEEE, 2014._

Aaron D Ames, Xiangru Xu, Jessy W Grizzle, and Paulo Tabuada. Control barrier function based
quadratic programs for safety critical systems. IEEE Transactions on Automatic Control, 62(8):
3861â€“3876, 2016.

Charles W Anderson. Learning to control an inverted pendulum using neural networks. IEEE Control
_Systems Magazine, 9(3):31â€“37, 1989._

John AD Appleby. Stabilisation of functional differential equations by noise. Systems & Control
_Letters, 2003._

Eduardo F Camacho and Carlos Bordons Alba. Model predictive control. Springer science & business
media, 2013.

Ya-Chien Chang, Nima Roohi, and Sicun Gao. Neural lyapunov control. In Proceedings of the 33rd
_International Conference on Neural Information Processing Systems, pp. 3245â€“3254, 2019._

Andrew Clark. Control barrier functions for complete and incomplete information stochastic systems.
In 2019 American Control Conference (ACC), pp. 2928â€“2935. IEEE, 2019.

Andrew Clark. Control barrier functions for stochastic systems. Automatica, 130:109688, 2021.

Sarah Dean, Andrew J Taylor, Ryan K Cosner, Benjamin Recht, and Aaron D Ames. Guaranteeing
safety of learned perception modules via measurement-robust control barrier functions. arXiv
_preprint arXiv:2010.16001, 2020._

David D Fan, Jennifer Nguyen, Rohan Thakker, Nikhilesh Alatur, Ali-akbar Agha-mohammadi, and
Evangelos A Theodorou. Bayesian learning-based adaptive control for safety critical systems. In
_2020 IEEE international conference on robotics and automation (ICRA), pp. 4093â€“4099. IEEE,_
2020.

Marco Gallieri, Seyed Sina Mirrazavi Salehian, Nihat Engin Toklu, Alessio Quaglino, Jonathan
Masci, Jan KoutnÃ­k, and Faustino Gomez. Safe interactive model-based learning. arXiv preprint
_arXiv:1911.06556, 2019._

Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural networks
by enforcing lipschitz continuity. Machine Learning, 110(2):393â€“416, 2021.

Qian Guo, Xuerong Mao, and Rongxian Yue. Almost sure exponential stability of stochastic
differential delay equations. SIAM Journal on Control and Optimization, 54(4):1919â€“1933, 2016.

Thomas Gurriet, Andrew Singletary, Jacob Reher, Laurent Ciarletta, Eric Feron, and Aaron Ames.
Towards a framework for realizable safety critical control through active set invariance. In 2018
_ACM/IEEE 9th International Conference on Cyber-Physical Systems (ICCPS), pp. 98â€“106. IEEE,_
2018.


-----

Jiequn Han et al. Deep learning approximation for stochastic control problems. arXiv preprint
_arXiv:1611.07422, 2016._

Didier Henrion and Andrea Garulli. Positive Polynomials in Control, volume 312. Springer Science
& Business Media, 2005.

Shiuh-Jer Huang and Chien-Lo Huang. Control of an inverted pendulum using grey prediction model.
_IEEE Transactions on Industry Applications, 36(2):452â€“458, 2000._

Mrdjan Jankovic. Control barrier functions for constrained control of linear systems with input delay.
In 2018 annual American control conference (ACC), pp. 3316â€“3321. IEEE, 2018.

Zachary Jarvis-Wloszek, Ryan Feeley, Weehong Tan, Kunpeng Sun, and Andrew Packard. Some
controls applications of sum of squares programming. In 42nd IEEE International Conference on
_Decision and Control (IEEE Cat. No. 03CH37475), volume 5, pp. 4676â€“4681. IEEE, 2003._

Ioannis Karatzas and Steven Shreve. Brownian motion and stochastic calculus, volume 113. Springer
Science & Business Media, 2012.

S Mohammad Khansari-Zadeh and Aude Billard. Learning control lyapunov function to ensure
stability of dynamical system-based robot reaching motions. Robotics and Autonomous Systems,
62(6):752â€“765, 2014.

Mohammad Javad Khojasteh, Vikas Dhiman, Massimo Franceschetti, and Nikolay Atanasov. Probabilistic safety constraints for learned high relative degree system dynamics. In Learning for
_Dynamics and Control, pp. 781â€“792. PMLR, 2020._

J Zico Kolter and Gaurav Manek. Learning stable deep dynamics models. Advances in Neural
_Information Processing Systems, 32:11128â€“11136, 2019._

Joseph P LaSalle. Stability theory for ordinary differential equations. Journal of Differential
_equations, 4(1):57â€“65, 1968._

Mathias Lechner, Ãor Â¯de Å½ikeliÂ´c, Krishnendu Chatterjee, and Thomas A Henzinger. Stability verification in stochastic control systems via neural network supermartingales. In Proceedings of the
_AAAI Conference on Artificial Intelligence, volume 36, pp. 7326â€“7336, 2022._

Aming Li, Sean P Cornelius, Y-Y Liu, Long Wang, and A-L BarabÃ¡si. The fundamental advantages
of temporal networks. Science, 358(6366):1042â€“1046, 2017.

Wei Lin and Yangbo He. Complete synchronization of the noise-perturbed chuaâ€™s circuits. Chaos:
_An Interdisciplinary Journal of Nonlinear Science, 15(2):023705, 2005._

Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, and Or Litany. Learning smooth
neural functions via lipschitz regularization. arXiv preprint arXiv:2202.08345, 2022.

Liang Liu, Shen Yin, Lixian Zhang, Xunyuan Yin, and Huaicheng Yan. Improved results on
asymptotic stabilization for stochastic nonlinear time-delay systems with application to a chemical
reactor system. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 47(1):195â€“204,
2016.

X Mao. Robustness of exponential stability of stochastic differential delay equations. IEEE Transac_tions on Automatic Control, 41(3):442â€“447, 1996._

Xuerong Mao. Lasalle-type theorems for stochastic differential delay equations. Journal of mathe_matical analysis and applications, 236(2):350â€“369, 1999._

Xuerong Mao. A note on the lasalle-type theorems for stochastic differential delay equations. Journal
_of mathematical analysis and applications, 268(1):125â€“142, 2002._

Xuerong Mao. Stochastic differential equations and applications. Elsevier, 2007.

Takashi Matsumoto. A chaotic attractor from chuaâ€™s circuit. IEEE Transactions on Circuits and
_Systems, 31(12):1055â€“1058, 1984._


-----

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.

Pablo A Parrilo. Structured Semidefinite Programs and Semialgebraic Geometry Methods in Robust_ness and Optimization. California Institute of Technology, 2000._

Dongxue Peng, Xiaodi Li, R Rakkiyappan, and Yanhui Ding. Stabilization of stochastic delayed
systems: Event-triggered impulsive control. Applied Mathematics and Computation, 401:126054,
2021.

Stephen Prajna and Ali Jadbabaie. Safety verification of hybrid systems using barrier certificates.
In International Workshop on Hybrid Systems: Computation and Control, pp. 477â€“492. Springer,
2004.

Stephen Prajna, Ali Jadbabaie, and George J Pappas. Stochastic safety verification using barrier certificates. In 2004 43rd IEEE conference on decision and control (CDC)(IEEE Cat. No. 04CH37601),
volume 1, pp. 929â€“934. IEEE, 2004.

Rajesh Rajamani. Vehicle dynamics and control. Springer Science & Business Media, 2011.

Hadi Ravanbakhsh and Sriram Sankaranarayanan. Learning control lyapunov functions from counterexamples and demonstrations. Autonomous Robots, 43(2):275â€“307, 2019.

Alexander Robey, Haimin Hu, Lars Lindemann, Hanwen Zhang, Dimos V Dimarogonas, Stephen Tu,
and Nikolai Matni. Learning control barrier functions from expert demonstrations. In 2020 59th
_IEEE Conference on Decision and Control (CDC), pp. 3717â€“3724. IEEE, 2020._

Cesar Santoyo, Maxence Dutreix, and Samuel Coogan. A barrier function approach to finite-time
stochastic system verification and control. Automatica, 125:109439, 2021.

Meenakshi Sarkar, Debasish Ghose, and Evangelos A Theodorou. High-relative degree stochastic
control lyapunov and barrier functions. arXiv preprint arXiv:2004.03856, 2020.

AN Shiryaev. Theory of martingales, 1989.

Bo Song, Ju H Park, Zheng-Guang Wu, and Xuchao Li. New results on delay-dependent stability
analysis and stabilization for stochastic time-delay systems. International Journal of Robust and
_Nonlinear Control, 24(16):2546â€“2559, 2014._

Yong-Zheng Sun, Si-Yang Leng, Ying-Cheng Lai, Celso Grebogi, and Wei Lin. Closed-loop control
of complex networks: A trade-off between time and energy. Physical Review Letters, 119(19):
198301, 2017.

Yonghui Sun and Jinde Cao. Adaptive synchronization between two different noise-perturbed chaotic
systems with fully unknown parameters. Physica A: statistical mechanics and its applications,
376:253â€“265, 2007.

Andrew Taylor, Andrew Singletary, Yisong Yue, and Aaron Ames. Learning for safety-critical control
with control barrier functions. In Learning for Dynamics and Control, pp. 708â€“717. PMLR, 2020.

Nolan Wagener, Ching-An Cheng, Jacob Sacks, and Byron Boots. An online learning approach to
model predictive control. arXiv preprint arXiv:1902.08967, 2019.

Li Wang, Aaron D Ames, and Magnus Egerstedt. Multi-objective compositions for collision-free
connectivity maintenance in teams of mobile robots. In 2016 IEEE 55th Conference on Decision
_and Control (CDC), pp. 2659â€“2664. IEEE, 2016._

Duncan J Watts and Steven H Strogatz. Collective dynamics of â€˜small-worldâ€™networks. nature, 393
(6684):440â€“442, 1998.

Antoine Wehenkel and Gilles Louppe. Unconstrained monotonic neural networks. Advances in
_neural information processing systems, 32, 2019._


-----

Grady Williams, Paul Drews, Brian Goldfain, James M Rehg, and Evangelos A Theodorou.
Information-theoretic model predictive control: Theory and applications to autonomous driving. IEEE Transactions on Robotics, 34(6):1603â€“1622, 2018.

Yuhua Xu, Yuling Wang, Wuneng Zhou, and Jianâ€™an Fang. Stochastic complex networks synchronize
to the limit set with adaptive controller and adaptive delay. Mathematical Methods in the Applied
_Sciences, 37(15):2290â€“2296, 2014._

Gang Yan, Jie Ren, Ying-Cheng Lai, Choy-Heng Lai, and Baowen Li. Controlling complex networks:
How much energy is needed? Physical Review Letters, 108(21):218703, 2012.

Xue Ying. An overview of overfitting and its solutions. In Journal of Physics: Conference Series,
volume 1168, pp. 022022. IOP Publishing, 2019.

Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability
of deep learning. arXiv preprint arXiv:1705.10941, 2017.

Wenwu Yu and Jinde Cao. Adaptive synchronization and lag synchronization of uncertain dynamical
system with time delay based on parameter identification. Physica A: Statistical Mechanics and its
_Applications, 375(2):467â€“482, 2007._

Jingdong Zhang, Qunxi Zhu, and Wei Lin. Neural stochastic control. _arXiv preprint_
_arXiv:2209.07240, 2022._

Xun Yu Zhou and Duan Li. Continuous-time mean-variance portfolio selection: A stochastic lq
framework. Applied Mathematics and Optimization, 42(1):19â€“33, 2000.

Quanxin Zhu. Stabilization of stochastic nonlinear delay systems with exogenous disturbances and
the event-triggered feedback control. IEEE Transactions on Automatic Control, 64(9):3764â€“3771,
2018.


-----


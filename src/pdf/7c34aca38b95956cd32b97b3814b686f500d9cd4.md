# WEAKLY-SUPERVISED DOMAIN ADAPTATION IN FED## ERATED LEARNING FOR HEALTHCARE

**Anonymous authors**
Paper under double-blind review

### ABSTRACT

Federated domain adaptation (FDA) describes the setting where a set of source
clients seek to optimize the performance of a target client. To be effective, FDA
must address some of the distributional challenges of Federated learning (FL).
For instance, FL systems exhibit distribution shifts across clients. Further, labeled data are not always available among the clients. To this end, we propose
and compare novel approaches for FDA, combining the few labeled target samples with the source data when auxiliary labels are available to the clients. The
in-distribution auxiliary information is included during local training to boost outof-domain accuracy. Also, during fine-tuning, we devise a simple yet efficient
gradient projection method to detect the valuable components from each source
client model towards the target direction. The extensive experiments on healthcare datasets show that our proposed framework outperforms the state-of-the-art
unsupervised FDA methods with limited additional time and space complexity.

### 1 INTRODUCTION

Federated learning (FL) is a distributed learning paradigm, where an aggregated model is learned
using local decentralized data on edge devices (McMahan et al., 2017). FL systems usually share
the model weights or gradient updates of clients to the server, which prevents direct exposure of
the sensitive client data. As a result, data heterogeneity remains an important challenge in FL, and
much of the research focuses on mitigating the negative impacts of the distribution shifts between
clients’ data (Wang et al., 2019; Karimireddy et al., 2020; Xie et al., 2020b). Further, much of the
FL literature has focused on settings where all datasets are fully-labeled. However, in the real world,
one often encounters settings where the labels are scarce on some of the clients. To this end, multisource domain adaptation (MSDA) (Ben-David et al., 2010; Zhao et al., 2020; Guan & Liu, 2021)
is a common solution to this problem, where models trained on several labeled, separate source
domains are transferred to the unlabeled or sparsely labeled target domain. Here, we consider the
more specialized setting of Federated domain adaptation (FDA) – where a set of source clients seek
to optimize the performance of a target client. As an analogue to MSDA, one may consider clients
data as different domains. Thus, goal is to learn a good model for the few-labeled target client data
samples by transfer the useful knowledge from multiple source clients. In this work, we consider
the FDA problem under weak supervision, where auxiliary labels are available to the clients. In
brief, we propose novel approaches to deal with weakly-supervised FDA, focusing on techniques
that adapt both the local training and fine-tuning stages.

**Motivating Application. Our work is inspired and applied to applications in predictive modeling**
for healthcare where there can be significant differences across hospitals, causing transfer errors
across sites (Li et al., 2020; Guan et al., 2021; Wolleb et al., 2022). Unlike many other industries,
healthcare in the US is highly heterogeneous (e.g., HCA, the largest consortium of hospitals covers
_< 2% of the market (Statista, 2020; Wikipedia contributors, 2022)), thus many variables are not_
standardized (Adnan et al., 2020; Osarogiagbon et al., 2021). Hence, we consider experiments that
simulate differences across institutions as a large shift. Further, we consider an FL application
across several hospitals located at different states in the US. In this setting, FDA is employed to
improve performance at a target hospital by leveraging information from all of the source hospitals.
The human cost of labeling the images is expensive, thus the data are sparsely labeled. Further,
in addition to the medical images, the data also include demographic information such as age, sex,
race, among others. While such auxiliary data is ubiquitous, it is often ignored when working to


-----

(b)

(a)

Figure 1: (a) Proposed framework of weakly-supervised FDA: we set up a MTL framework
leveraging auxiliary labels during the source clients’ local training. (b) Intuition of GP: we project
the valuable components of source gradients towards the target direction to boost FDA performance.

improve centralized or federated models. Here, we show how this data can be used to significantly
improve out-of-domain (OOD) performance when properly utilized in FL. Also, in FL, local models
are gradually trained, so the importance of source domains may change dynamically during each
iteration. Our work seeks to extract the valuable components of model updates instead of the whole
models towards the target direction during each round. We discover that the few labeled target
samples can provide sufficient guidance towards adapting quickly to the target domain.

Our contributions are twofold, in local training and fine-tuning. First, we leverage auxiliary information to reduce the task risk on the target client during local training. Auxiliary information is often
cheap and easy to find along with the image inputs, which may offer some useful signals for the unlabeled samples, because of the underlying correlations between auxiliary tasks and main task. Xie
et al. (2021) and Wang et al. (2022) both found that in centralized setting, using auxiliary information can improve OOD performance, resulting in a smaller OOD risk compared with the baseline. In
our work, we set up a cheap and efficient multi-task learning (MLT) framework with auxiliary tasks
during local training of source clients, as shown in Figure 1a. By optimizing the main and auxiliary
task losses together, we show empirically that one can boost target domain accuracy after fine-tuning
on labeled target samples (Section 5.2). The gains are more evident when the distribution shifts are
large, yet auxiliary tasks may introduce unexpected noise when domains are too close to each other.

Secondly, we observe that including auxiliary information alone does not fully account for the importance of source domains. Thus, during fine-tuning, we propose a simple yet efficient gradient
_projection (GP) method. This method utilizes the useful source domain components and projects_
them towards the target direction. As shown in Figure 1b, during each communication round, we
compute the model updates of source clients, and then project them on the target model update obtained by fine-tuning on a small set of labeled target samples. In this way, we greedily approach
to the optimum of target domain: the positive cosine similarity between a pair of (source, target)
updates serves as the importance of that domain. Our experiment results indicate that this gradient
projection method achieves a better and more stable FDA performance on the target client, through
combining projected gradients with the fine-tuning gradient. We show the superiority of the gradient
projection method through the comprehensive experiments on both medical and general-purposed
datasets in Section 5.2 and Appendix A.6.

Combining two techniques together, our proposed framework outperforms the state-of-the-arts unsupervised FDA methods with limited additional computational cost. Also, we show empirically
that our framework is resistant to data imbalances on the real-world MIDRC dataset (Section 5.2).

### 2 PROBLEM SETUP

In this section, we introduce the framework of weakly-supervised FDA. We first provide the background knowledge of weakly-supervised MSDA. Following this, we extend the framework to federated learning setting.


-----

**Weakly-supervised MSDA. Let DS and DT denote source and target domains. In each domain,**
we have sample pairs of x ∈ R[d] as the input, y ∈ R as the output label. In MSDA, we have N
source domains DS = {DSi _}i[N]=1_ [with][ n][i][ labeled samples as well as a target domain with][ n][T][ total]
samples, which consists of nl labeled samples and nT _nl (a large number of) unlabeled samples._
_−_
During pretraining, we train a model hSi on each source domain using the corresponding labeled
samples. The goal of fine-tuning is to learn a target model hT, which minimizes the target risk
_ϵT (h) := Pr(x,y)∼DT [h(x) ̸= y]. A common approach is to first aggregates the model parameters_
using [�]i[N]=1 _[α][i][h][S]i_ [with][ α][i][ controlling the importance of each source domain such that][ �]i[N]=1 _[α][i][ = 1][.]_
Then, we fine-tune the aggregated parameters using the set of labeled samples from DT .

**Federated problem setting. We now extend weakly-supervised MSDA to the FL setting. As shown**
in Figure 1a and Algorithm 1, we assume there are N + 1 clients in the system, where N clients
_{CSi_ _}i[N]=1_ [have labeled data][ {D][S]i _[}][N]i=1_ [and the remaining client][ C][T][ has][ D][T][ with][ n][l][ labeled data.]
Different from the centralized setting, weakly-supervised FDA requires the target client CT to have
no direct access to the source data {DSi _}i[N]=1[. The aggregation of the source domain models is]_
performed by the server using any federated aggregation rules such as FedAvg (McMahan et al.,
_S_
2017) or SCAFFOLD (Karimireddy et al., 2020). After finishes the aggregation, it sends the
_S_
model parameters to CT and performs a fine-tuning – just as with weakly-supervised MSDA. The
final model is sent to all the source clients {CSi _}i[N]=1[, which ends one communication round.]_

**Algorithm 1 Weakly-Supervised Federated Domain Adaptation**
**Input: N source domains DS = {DSi** _}i[N]=1[; target domain][ D][T][ ;][ N][ source clients][ {C][S]i_ _[}][N]i=1[; target]_
client CT ; target model h[(]T[r][−][1)] from the previous round r − 1; aggregation rule aggr; server S.
**Output: Target model h[(]T[r][)]** at round r.

1: Step 1: Local training on {CSi _}i[N]=1_

2: for DSi in DS do

3: Initialize h[(]S[r]i[)] _T_ .

_[←]_ _[h][(][r][−][1)]_

4: Optimize h[(]S[r]i[)] [on][ D][S]i [with classification task loss (Eq. 1)]

5: Send h[(]S[r]i[)] [to][ S]

6: end for
7: Step 2: S received all {h[(]S[r]i[)][}]i[N]=1

8: h[(]global[r][)] _Si_ _i=1[)]_

_[←]_ _[aggr][(][{][h][(][r][)][}][N]_

9: Send h[(]global[r][)] [to][ C][T]

10: Step 3: CT received h[(]global[r][)]

11: Optimize h[(]global[r][)] [on][ D][T][ using labeled samples.]

12: h[(]T[r][)] _←_ _h[(]global[r][)]_

13: Send h[(]T[r][)] to S

14: Step 4: S received h[(]T[r][)]

15: for CSi in CS do

16: Send h[(]T[r][)] to CSi

17: end for

### 3 LEVERAGING AUXILIARY INFORMATION DURING LOCAL TRAINING

After defining the framework for weakly-supervised FDA, we now look into the ways to boost the
target client performance. In this section, we explain the idea of leveraging auxiliary information:
optimizing the main and auxiliary tasks together during source clients’ local training. Here, we only
derive the loss objective for one source client CSi (since it is the same for all source clients). Let n
be the total local training sample size, and ℓ as the loss function for the main task. Eq. 1 is the loss
objective of the main task (image classification).


-----

_Lmain =_


_n_
�

_ℓ(h(xj), yj)_ (1)
_j=1_


Let K be the number of auxiliary tasks, and zk R[T], bk be kth auxiliary input and output labels,
_∈_
respectively. Thus, zkj and bkj denote the jth sample input and output for kth auxiliary task. Each
auxiliary task shares the same parameters with h except for the last layer (denoted as h[l][−][1]) and
we define gk to be the feature mapping from the feature representation to the kth auxiliary output.
Lastly, let ℓaux as the loss function for auxiliary tasks. Then, we can construct the loss objective for
auxiliary tasks as follows:


_n_
�

_Laux =_

_j=1_


_K_
�

_ℓaux(gk(h[l][−][1](zkj)), bkj)_ (2)

_k=1_


In the end, we write the total loss objective for leveraging auxiliary information as Eq. 3, where α
controls the weight of auxiliary task losses:


�


�


_Laux−info = Lmain + α · Laux =_


_n_
�

_j=1_


_ℓ(h(xj), yj) + α ·_


_K_
�

_ℓaux(gk(h[l][−][1](zkj)), bkj)_

_k=1_


(3)


Including auxiliary information only changes the local training process (Step 1): CSi optimizes its
model parameters using a summed loss together with auxiliary task losses. As shown in Figure 1a,
we optimize the losses of all tasks together, which consist of negative/positive diagnosis as the main
task, and race, sex as well as age information as auxiliary tasks. When fine-tuning on the target
domain DT, we only optimize the model parameters using the main task loss without any auxiliary
tasks (same as Xie et al. (2021)), which makes other steps identical to Algorithm 1. Algorithm 2
displays the local training procedure with auxiliary tasks on one of the source clients CSi .


**Algorithm 2 Domain Adaptation with**
Auxiliary Information (Local training on
_CSi_ )
**Input:** One source domain DSi ; one
source client CSi ; target model h[(]T[r][−][1)]
from the previous round r 1; input
_−_
images x, auxiliary input z, main task
label y, auxiliary output label b; main
task loss function ℓ and auxiliary task
loss function ℓaux; loss weight control
hyper-parameter α.
**Output: source model h[(]S[r]i[)]** [at round][ r][.]

1: Initialize h[(]S[r]i[)] _T_ .

_[←]_ _[h][(][r][−][1)]_

2: Optimize _h[(]S[r]i[)]_ on _DSi_ with

_Laux−info(h[(]S[r]i[)][, x, y, z, b, ℓ, ℓ][aux][, α][)][.]_

3: Send h[(]S[r]i[)] [to][ S]


**Algorithm 3 Gradient Projection on CT**
**Input: N source domain models hS = {hSi** _}i[N]=1[; tar-]_
get model h[(]T[r][−][1)] from the previous round r−1; global
model h[(]global[r][−][1)] [from the previous round][ r][−][1][; target do-]
main DT ; target client CT ; server S; GP weight control variable β; number of samples of source domains
_{ni}i[N]=1[.]_
**Output: Target model h[(]T[r][)]** at round r.

1: Step 2: S received all {h[(]S[r]i[)][}]i[N]=1

2: h[(]global[r][)] _[←]_ _[h][(]global[r][−][1)]_

3: for hSi in {h[(]S[r]i[)][}]i[N]=1 **[do]**

4: _GSi ←_ _h[(]S[r]i[)]_ _[−]_ _[h]global[(][r][)]_

5: end for
6: Send h[(]global[r][)] [and][ {][G]S[(][r]i[)] _i=1_ [to][ C][T]

_[}][N]_

7: Step 3: CT received {G[(]S[r]i[)][}]i[N]=1

8: h[(]T[r][)] _←_ _h[(]T[r][−][1)]_

9: Optimize h[(]T[r][)] on DT using labeled samples

10: GT ← _h[(]T[r][)]_ _−_ _h[(]T[r][−][1)]_

11: Update h[(]T[r][)] using Eq. 7 with {G[(]S[r]i[)][}]i[N]=1[,][ G][T][,][ β][,]
_{ni}i[N]=1[.]_

12: Send h[(]T[r][)] to S


-----

### 4 UTILIZING SOURCE DOMAIN KNOWLEDGE VIA GRADIENT PROJECTION

Leveraging auxiliary information does not consider the importance of each source client contributing
to the target client, because we only use a simple aggregation rule. In weakly-supervised FDA, how
can one better utilize the knowledge from both labeled target samples and source client models?
Here, we suggest a novel Gradient Projection (GP) method.

**Algorithm intuition. The small set of labeled target samples provides a useful signal on the di-**
rection of target domain optimum. Thus, during each communication round, server does not
_S_
aggregate the weights (Algorithm 1), but instead computes the model updates denoted by {G[(]S[r]i[)][}]i[N]=1
where G[(]S[r]i[)] _Si_ _global_ [at round][ r][ from all source clients][ {C][S][i] _[}]i[N]=1[. On the target client][ C][T][,]_

_[≃]_ _[h][(][r][)]_ _[−]_ _[h][(][r][−][1)]_

it will perform gradient projection using cosine similarity on each G[(]S[r]i[)] [towards the target direction]

_G[(]T[r][)]_ _≃_ _h[(]T[r][)]_ _−_ _h[(]T[r][−][1)], which could be computed after fine-tuning on the small set of the labeled_
target samples. In this way, we greedily maximize knowledge transfer to the target domain in each
round; the projection of G[(]S[r]i[)] [on][ G]T[(][r][)] could be regarded as the weight of DSi at round r. By com
bining G[(]T[r][)] with projected gradients ({G[(]S[r]i[)][}]i[N]=1 [on][ G]T[(][r][)][) controlled by a hyper-parameter][ β][, we]
observe a more steady convergence towards the target direction, as noted in the experiment results
outlined in Section 5.2. Figure 1b and Algorithm 3 illustrate the procedure of gradient projection.

**Details of the Gradient Projection. We compute the cosine similarity (Eq. 4) between one source**
client model update G[(]S[r]i[)] [and target client update][ G]T[(][r][)] for each layer of the model (for a finer projection). In addition, we align the magnitude of the model updates according to the number of target/source samples, batch sizes, local updates, and learning rates (more details are in Appendix A.2).
To prevent negative projection, we set the threshold for function GP to be 0. For a certain layer l
of G[(]S[r]i[)] [and][ G][(]T[r][)] (for simplicity, we denote them as G[l]Si [and][ G]T[l] [), the cosine similarity and corre-]
sponding gradient projection result is (Eq. 5):

_G[l]Si_ _[G]T[l]_
cos(G[l]Si _[, G]T[l]_ [) =] (4)

_∥G[l]Si_ _[∥∥][G]T[l]_ _[∥]_

�cos(GlSi _[, G]T[l]_ [)][,] if cos(G[l]Si _[, G]T[l]_ [)][ >][ 0]
**GP(G[l]Si** _[, G]T[l]_ [) =] (5)
0, if cos(G[l]Si _[, G]T[l]_ [)][ ≤] [0]

The total gradient projection PGP from all source clients {G[(]S[r]i[)][}]i[N]=1 [projected on the target direction]
_GT could be computed as Eq. 6. We use L to denote all layers of current model updates. ni denotes_
the number of samples trained on source client CSi, which is adapted from FedAvg (McMahan et al.,
2017) to redeem data imbalance issue. Hence, we normalize the gradient projections according to
their number of samples. Also, ++[L]l∈L [concatenates the projected gradients of all layers.]


_ni_
**GP(G[l]Si** _[, G]T[l]_ [)][ ·] _N_ _· G[l]Si_
�
_i_ _[n][i]_


�


�


_PGP = ++[L]l∈L_


_N_
�

_i=0_


(6)


Lastly, a hyper-parameter β is used to incorporate target update GT into PGP to have a more stable
performance. The final target model weight h[(]T[r][)] at round r is thus expressed as:

_h[(]T[r][)]_ = h[(]T[r][−][1)] + (1 − _β) · PGP + β · GT_ (7)

### 5 EXPERIMENTS

We evaluate our proposed framework on three medical datasets: CheXpert (Irvin et al., 2019),
MIMIC (Johnson et al., 2019), and a real-world imbalanced dataset from MIDRC tasked with
COVID-19 detection from X-ray images. The data are split to represent three scenarios: (a) CheX**pert: balanced data across clients and a large distribution shift among domains. (b) MIMIC:**


-----

Figure 2: Summary of the medical datasets in our experiments.


States Number of Samples

IL **16203**
NC 3717
CA 560
IN 557
TX 501

Table 1: Statistics of MIDRC
dataset: an extremely imbalanced scenario (IL consists
most samples).


_balanced data across clients and and a small distribution shift among domains. (c) MIDRC_ [1]:
highly imbalanced data across clients and a large distribution shift across domains. Note that this
is a real-world application with data from hospitals in different locations across U.S. The first two
experiments are designed to highlight the kinds of extreme data shift often noted in healthcare, e.g.,
where the same features are measured differently across hospitals (Wiens et al., 2014), or where the
same variables’ names refer to very different measurements/ diagnoses across health systems. Unfortunately, there are little public data illustrating these kinds of important and understudied shifts.
Thus, we designed representative semi-synthetic shifts to illustrate the extent and impact of the issue.

5.1 EXPERIMENTAL SETUP

We provide basic information and experiment setup of three medical imaging datasets in Figure 2.
More details on data splitting and implementation details are discussed at Appendix A.3.

**CheXpert is a widely used medical imaging dataset for multi-label classification consisting of**
224,316 chest radiographs from 65,240 patients (Irvin et al., 2019). We use sex, age, frontal/lateral
information from patients to construct auxiliary labels and split domains by labelled condition i.e.,
selecting “Lung Lesion”, “Edema”, “Consolidation”, “Pneumonia”, “Atelectasis”– all of which are
lung conditions, as source and target domains. Thus, the task is to predict a new lung condition
based on labels of existing lung conditions.

**MIMIC is a large dataset of 227,835 imaging studies for 65,379 patients between 2011–2016 (John-**
son et al., 2019). We set domains using the race information provided and merge the result into four
main categories: White, Black, Asian, Hispanic/Latino. Thus, the task is to predict conditions for
a previously unobserved racial group condition based on labels collected from other groups. The
distribution shifts between race domains are considered small, as we can get a high accuracy simply
using FedAvg, as shown in Table 2.

**MIDRC includes Computed Radiography (CR) images as the primary input. We evaluate the pro-**
posed framework using NC, CA, IN, TX states as source clients, and try to adapt to the target client
IL which has a large number of unlabeled samples. The statistics of these five states are shown in
Table 1. We collect race, sex, and age data as auxiliary information.

5.2 MAIN RESULTS

Table 2 and Figure 3 compare the target domain accuracy and convergence speed of following methods on three datasets: a) FedAvg only aggregates the source clients using FedAvg (McMahan et al.,
2017) without fine-tuning on any target samples; b) FedAvg Finetune performs fine-tuning step
after source client aggregations (Algorithm 1); c) FedAvg Finetune AuxInfo includes auxiliary
information during local training (Algorithm 2); d) FedAvg Finetune GP performs gradient projection during fine-tuning (Algorithm 3); e) FedAvg Finetune AuxInfo GP combines c) and d)

[1MIDRC data is semi-public, and is available by request https://www.midrc.org/](https://www.midrc.org/)


-----

**CheXpert** **MIMIC** **MIDRC**
ACC AUC ACC AUC ACC AUC

FedAvg 53.28±2.44 54.13±3.77 69.95±1.57 76.34±3.02 53.56±2.33 52.45±4.23
FedAvg Finetune 58.90±0.77 63.81±2.92 70.21±1.38 77.18±0.87 62.33±2.34 66.66±2.17
FedAvg Finetune AuxInfo 63.15±2.11 69.10±4.08 69.41±1.40 76.38±1.27 68.22±3.68 73.41±1.68
FedAvg Finetune GP 72.63±2.27 82.40±2.17 **70.69±0.91** **77.24±1.87** 69.36±2.93 72.10±2.48
FedAvg Finetune AuxInfo GP **75.61±1.17** **83.09±2.28** 68.68±2.16 75.54±2.44 **70.28±1.68** **74.61±3.16**

FADA (Peng et al., 2020) 61.83±3.33 64.83±5.44 65.92±1.43 70.11±2.04 51.98±1.59 53.19±2.39

KD3A (Feng et al., 2021) 62.61±0.50 67.72±2.50 70.08±0.64 75.50±0.94 51.27±2.69 55.76±0.55

Oracle 75.72±1.27 82.51±1.15 71.28±0.79 76.29±0.94 84.28±1.01 87.62±1.54

Table 2: Target domain accuracy and AUC scores (%) on three medical datasets with comparisons
with SOTA methods. Results are reported averaged across 3 trials and 95% confidence intervals.

(a) CheXpert (b) MIMIC (c) MIDRC

Figure 3: Target Domain Accuracy v.s Rounds on (a) CheXpert (b) MIMIC (c) MIDRC datasets.
For the large shift scenarios (a) and (c), AuxInfo and GP both significantly improve target domain
performance. For the small shift scenario (b), GP still manages to achieve 1% boost while AuxInfo
may introduce extra noise into the training procedure.

together; f) and g) FADA (Peng et al., 2020) and KD3A (Feng et al., 2021) are SOTA methods focusing on the unsupervised FDA setting; h) Oracle refers to a fully-supervised training on the target
domain and serves as the upper bound. For all datasets, our framework generally outperforms the
state-of-the-art FDA methods with a large margin and is close to the oracle performance.

**CheXpert. For balanced and large distribution shifts, AuxInfo and GP improve around 6% and 18%**
of accuracy and AUC individually. Combining the two together, we can get a further 2 3% gain
_∼_
with close-to-oracle performance. However, we notice that AuxInfo slows down the convergence
speed of the training procedure.

**MIMIC. For balanced and small distribution shifts, the boost compared with baseline (FedAvg)**
becomes small. Yet GP still manages to achieve a 1% boost with a close-to-oracle performance. We
think when shifts are small, including auxiliary information may hinder the fine-tuning of the model,
introducing extra noise during the training procedure. The variance is quite large for AuxInfo during
the first several epochs, which leads to a slow convergence when combining two techniques. When
the local signals are not helpful, doing GP on top enlarges the negative effect. We hypothesize that
this is why it results in a slightly worsened performance.

**MIDRC. For imbalanced and large distribution shifts, both AuxInfo and GP achieve a significant**
increase of 6% and 7% on the target client accuracy/AUC. 1 2% extra increase is obtained when
_∼_
combining AuxInfo with GP. It is interesting to see when client data is imbalanced, AuxInfo actually
achieves a faster convergence. We think data imbalance may require more signals coming from
auxiliary information to converge. In general, data imbalance has little impact on the performance
of our proposed framework. We hypothesize that is because we have normalized the client sample
size when updating the model weights. In contrast, SOTA methods are not resistant to this issue.

**Computational complexity analysis. As shown in Table 3, our proposed framework requires a**
small amount of additional time and space complexity compared with SOTA methods. GP has a
memory cost of O(1) and a time cost of O(N _m[2]/l) (Details in Appendix A.4). The average time_
_·_
for calling the GP function is 0.068 seconds (pretty fast) using the ResNet-18 with N = 4.


-----

FADA 5.6369E+07 Train all unlabeled samples in DT 166.32 (+135.62)

Table 3: Comparison of computational efficiency. We calculate the average time for running 1
global epoch on CheXpert with N = 4 source clients using the same Quadro RTX 6000 GPU. We do
not consider communication and testing cost and assume the clients’ training happens sequentially.

(a) MIDRC: target sample size. (b) MIDRC: comparisons of auxiliary tasks.

(c) CheXpert: β. (d) MIMIC: β.

Figure 4: (a) Target AUC with different sample sizes on FedAvg v.s Finetune v.s. AuxInfo v.s. GP
for MIDRC. (b) Performance gain compared with FedAvg Finetune using different auxiliary inputs.
(c) and (d) display the value of β v.s. target domain ACC/AUC for CheXpert and MIMIC.

|Methods|Number of parameters|Operations with extra time per epoch|Wallclock time (in seconds)|
|---|---|---|---|
|FedAvg|5.6212E+07|/|30.70|
|FedAvg Finetune|5.6212E+07|Fine-tune n l labeled samples in DT|31.53 (+0.83)|
|Ours|5.6221E+07|Fine-tune n l labeled samples in DT + optimize auxiliary tasks (Eq. 3)+ GP aggregation|34.38 (+3.68)|
|KD3A|5.6212E+07|Train all unlabeled samples in DT|41.67 (+10.97)|
|FADA|5.6369E+07|Train all unlabeled samples in DT|166.32 (+135.62)|


**Effects of labeled target sample sizes. We perform the**
experiment on MIDRC dataset with various target sample sizes of (50, 100, 500, 1000, 2000), testing its impact
on AuxInfo and GP individually, as shown in Table 4 and
Figure 4a. We report AUC scores for a more accurate
comparison. Generally, as we increase the number of target samples, the target domain accuracy boosts as well.
Apart from that, the improvements coming from AuxInfo
and GP compared with the baselines also increase when
we have more labeled target samples.


50 100 500 1000 2000

FedAvg 52.45 52.45 52.45 52.45 52.45
Finetune 51.92 54.56 58.52 60.48 65.57
AuxInfo **55.13** **57.38** 61.30 64.88 **72.51**
GP 53.27 55.38 **63.45** **67.44** 71.98

Table 4: Target domain AUC (%) on
MIDRC dataset with different number of
target labeled samples.


**Effects of number of auxiliary tasks. To compare the contribution of each auxiliary task, we train**
FedAvg Finetune AuxInfo with a single auxiliary task branch of race/sex/age. In Figure 4b, we
report the performance gain of each auxiliary information compared with FedAvg Finetune. The
auxiliary tasks seem to have a summed gain effect (race gain + sex gain + age gain all gain).
_≈_

**Choice of controlled hyper-parameter β between gradient projections PGP and fine-tuning gradi-**
ent update GT . We set β = 0, 0.2, 0.4, 0.6, 0.8 and use 50, 20 labeled target samples on CheXpert
and MIMIC datasets. We use a small number of labeled samples to better evaluate the effectiveness
of GP. Figure 4c and Figure 4d present the target domain accuracies with different β choices. When
_β = 0, it relies solely on the set of labeled samples to optimize the parameters. In other words, it_
could be regarded as not transferring any knowledge from source clients. For both large and small
shift cases, We observe around β = 0.4, both the accuracy and AUC reach the highest values.
Whereas, when β is large, the performance drops severely for CheXpert but has little influence on
MIMIC, for which a close-to-oracle performance is more easily attained. For the large shift case,


-----

we find that the total gradient updates become too greedy, taking an overly-large step projecting the
gradients from source clients, leading to a worsened performance, while it results in little harm for
the small shift case. Therefore, we choose β = 0.5 for our experiments, though the tuning of β may
change slightly as the number of labeled target samples varies.

### 6 RELATED WORK

**Data heterogeneity and label deficiency in federated learning. Distribution shifts between clients**
remains a crucial challenge in FL. Current work often focus on improving the aggregation rules:
Karimireddy et al. (2020) use control variates and Xie et al. (2020b) cluster the client weights via EM
algorithm to correct the drifts among clients. In medical scene, Jiang et al. (2022) and Dinsdale et al.
(2022) try to mitigate local and global drifts via harmonisation. However, people usually assume
the local training is fully-supervised for all clients at present. The truth is, label deficiency problem
could happen in any of the clients. There recent works try to tackle label deficiency problem with
self-supervision or semi-supervision for better personalized models (Jeong et al., 2020; He et al.,
2021; Yang et al., 2021). Compared to them, we explore a new setting with fully-labeled source
clients and one few-labeled target client, improving FDA performance under weak supervision.

**Federated domain adaptation. There are a considerable amount of recent work on multi-source**
domain adaptation with unsupervised setting, with recent highlights on adversarial training (Saito
et al., 2018; Zhao et al., 2018), knowledge distillation (Nguyen et al., 2021), and source-free methods (Liang et al., 2020). Peng et al. (2020); Li et al. (2020) are the first to extend MSDA into FL
setting; they apply adversarial adaptation techniques to align the representations of nodes. More
recently, in KD3A (Feng et al., 2021) and COPA (Wu & Gong, 2021), the server with unlabeled
target samples aggregates the local models by learning the importance of each source domain, via
knowledge distillation and collaborative optimization. Our work is in contrast to them that primarily focus on the unsupervised setting. Here, these methods rely heavily on both source and target
data with complicated training procedure on the server. Our framework is computationally efficient,
exploring FDA problem in a new manner with auxiliary labels available to clients.

**Auxiliary information in domain generalization. In-N-Out (Xie et al., 2021) investigate both**
out-of-distribution (OOD) and in-distribution performance of using auxiliary information as inputs
and outputs with self-training. In medical scene, Wang et al. (2022) find by pre-training and finetuning on the auxiliary tasks, one could improve the transfer performance between datasets on the
primary task. They consider single source-target scenario with no labeled target data while our
work focuses on federated MSDA setting with few labeled target data. Their frameworks cannot be
directly adapted to our setting, since they require training on the source samples again after training
on out-of-domain (target) samples. Our MLT framework properly leverages auxiliary labels in the
new setting, is cheap and efficient to compute, with good improvement for large shift cases.

**Using additional gradient information in FL. Model updates in each communication round could**
provide valuable insights of client convergence directions, which is mostly explored for byzantine
robustness in FL. Zeno++ (Xie et al., 2020a) and FlTrust (Cao et al., 2021) leverage the additional
gradient computed from a small clean training dataset on the server, which helps compute the scores
of candidate gradients for detecting the malicious adversaries. In our work, we utilize the additional
gradient provided by the labeled target samples for FDA problem. In a simple yet effective way,
we project source gradients towards the target direction. Our results support that we transfer the
knowledge from source clients to the target client with a better and more stable performance.

### 7 CONCLUSION AND FUTURE WORK

We show that including auxiliary information during local training and gradient projection during
fine-tuning, can help address significant distribution shifts and label deficiency issues existing in current federated learning systems, particularly for real medical applications. Our results on healthcare
datasets show our proposed framework improves FDA performance with small additional computational cost. Future work includes evaluating on the fly/ offline finetuning scheme, exploring how
to select the set of labeled target samples in the real-world case to better align with the distribution of the unlabeled part, analyzing the impact of more factors related to domain discrepancy, and
extending current framework to more general transfer learning setting.


-----

### 8 REPRODUCIBILITY STATEMENT

We have provided the details of our dataset preprocessing, hyper-parameters, training scheme, and
model architecture in Section 5 and in the Appendix. Also, we have uploaded the source code of our
proposed framework as part of the the supplementary materials. Because access to the MIDRC data
is restricted to approved users, we are unable to include the original data. We will release the code
upon acceptance.

### REFERENCES

Kiran Adnan, Rehan Akbar, Siak Wang Khor, and Adnan Bin Amanat Ali. Role and challenges of
unstructured big data in healthcare. Data Management, Analytics and Innovation, pp. 301–323,
2020.

Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Mach. Learn., 79(1–2):151–175, may
[2010. ISSN 0885-6125. URL https://doi.org/10.1007/s10994-009-5152-4.](https://doi.org/10.1007/s10994-009-5152-4)

Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Gong. Fltrust: Byzantine-robust federated learning
via trust bootstrapping. In Proceedings of NDSS, 2021.

Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE
_Signal Processing Magazine, 29(6):141–142, 2012._

Nicola K Dinsdale, Mark Jenkinson, and Ana IL Namburete. Fedharmony: Unlearning scanner bias
with distributed data. arXiv preprint arXiv:2205.15970, 2022.

Haozhe Feng, Zhaoyang You, Minghao Chen, Tianye Zhang, Minfeng Zhu, Fei Wu, Chao Wu, and
Wei Chen. Kd3a: Unsupervised multi-source decentralized domain adaptation via knowledge
distillation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con_ference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp._
3274–3283. PMLR, 18–24 Jul 2021.

Hao Guan and Mingxia Liu. Domain adaptation for medical image analysis: a survey. IEEE Trans_actions on Biomedical Engineering, 69(3):1173–1185, 2021._

Hao Guan, Yunbi Liu, Erkun Yang, Pew-Thian Yap, Dinggang Shen, and Mingxia Liu. Multi-site
mri harmonization via attention-guided deep domain adaptation for brain disorder identification.
_Medical Image Analysis, 71:102076, 2021._

Chaoyang He, Zhengyu Yang, Erum Mushtaq, Sunwoo Lee, Mahdi Soltanolkotabi, and Salman
Avestimehr. Ssfl: Tackling label deficiency in federated learning via personalized selfsupervision. arXiv preprint arXiv:2110.02470, 2021.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.

Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik
Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest
radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI
_conference on artificial intelligence, volume 33, pp. 590–597, 2019._

Wonyong Jeong, Jaehong Yoon, Eunho Yang, and Sung Ju Hwang. Federated semi-supervised
learning with inter-client consistency & disjoint learning. In International Conference on Learn_ing Representations, 2020._

Meirui Jiang, Zirui Wang, and Qi Dou. Harmofl: Harmonizing local and global drifts in federated
learning on heterogeneous medical images. In Proceedings of the AAAI Conference on Artificial
_Intelligence, volume 36, pp. 1087–1095, 2022._


-----

Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly
available database of chest radiographs with free-text reports. Scientific data, 6(1):1–8, 2019.

Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning.
In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on
_Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5132–5143._
PMLR, 13–18 Jul 2020.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos:
An experimental study. In IEEE International Conference on Data Engineering, 2022.

Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence H Staib, Pamela Ventola, and James S Duncan. Multi-site fmri analysis using privacy-preserving federated learning and domain adaptation:
Abide results. Medical Image Analysis, 65:101765, 2020.

Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source
hypothesis transfer for unsupervised domain adaptation. In Hal Daum´e III and Aarti Singh (eds.),
_Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceed-_
_ings of Machine Learning Research, pp. 6028–6039. PMLR, 13–18 Jul 2020._

Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR
_conference on research and development in information retrieval, pp. 43–52, 2015._

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti Singh
and Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence
_and Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 1273–1282. PMLR,_
20–22 Apr 2017.

Tuan Nguyen, Trung Le, He Zhao, Quan Hung Tran, Truyen Nguyen, and Dinh Phung. Most:
Multi-source domain adaptation via optimal transport for student-teacher learning. In Uncertainty
_in Artificial Intelligence, pp. 225–235. PMLR, 2021._

Raymond U Osarogiagbon, Helmneh M Sineshaw, Chun Chieh Lin, and Ahmedin Jemal.
Institutional-level differences in quality and outcomes of lung cancer resections in the united
states. Chest, 159(4):1630–1641, 2021.

Xingchao Peng, Zijun Huang, Yizhe Zhu, and Kate Saenko. Federated adversarial domain adaptation. In International Conference on Learning Representations, 2020.

Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE conference on com_puter vision and pattern recognition, pp. 3723–3732, 2018._

Statista. Top u.s. health systems based on number of hospitals as of 2019,
2020. URL [https://www.statista.com/statistics/245010/](https://www.statista.com/statistics/245010/top-us-for-profit-hospital-operators-based-on-number-of-hospitals/)
[top-us-for-profit-hospital-operators-based-on-number-of-hospitals/.](https://www.statista.com/statistics/245010/top-us-for-profit-hospital-operators-based-on-number-of-hospitals/)

Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. In International Conference on Learning Represen_tations, 2019._

Rongguang Wang, Pratik Chaudhari, and Christos Davatzikos. Embracing the disharmony in medical imaging: A simple and effective framework for domain adaptation. Medical Image Analysis,
76:102309, 2022.


-----

Jenna Wiens, John Guttag, and Eric Horvitz. A study in transfer learning: leveraging data from
multiple hospitals to enhance hospital-specific predictions. _Journal of the American Medical_
_Informatics Association, 21(4):699–706, 2014._

Wikipedia contributors. Hospital network — Wikipedia, the free encyclopedia, 2022.
[URL https://en.wikipedia.org/w/index.php?title=Hospital_network&](https://en.wikipedia.org/w/index.php?title=Hospital_network&oldid=1117892002)
[oldid=1117892002. [Online; accessed 15-November-2022].](https://en.wikipedia.org/w/index.php?title=Hospital_network&oldid=1117892002)

Julia Wolleb, Robin Sandk¨uhler, Florentin Bieder, Muhamed Barakovic, Nouchine Hadjikhani,
Athina Papadopoulou, Ozg¨ur Yaldizli, Jens Kuhle, Cristina Granziera, and Philippe C Cattin.[¨]
Learn to ignore: domain adaptation for multi-site mri analysis. In International Conference on
_Medical Image Computing and Computer-Assisted Intervention, pp. 725–735. Springer, 2022._

Guile Wu and Shaogang Gong. Collaborative optimization and aggregation for decentralized domain
generalization and adaptation. In Proceedings of the IEEE/CVF International Conference on
_Computer Vision (ICCV), pp. 6484–6493, October 2021._

Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous SGD. In
Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Ma_chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 10495–10503._
PMLR, 13–18 Jul 2020a.

Ming Xie, Guodong Long, Tao Shen, Tianyi Zhou, Xianzhi Wang, Jing Jiang, and Chengqi Zhang.
Multi-center federated learning, 2020b.

Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. Inn-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness.
In International Conference on Learning Representations, 2021.

Dong Yang, Ziyue Xu, Wenqi Li, Andriy Myronenko, Holger R Roth, Stephanie Harmon, Sheng
Xu, Baris Turkbey, Evrim Turkbey, Xiaosong Wang, et al. Federated semi-supervised learning for
covid region segmentation in chest ct using multi-national data from china, italy, japan. Medical
_image analysis, 70:101992, 2021._

Han Zhao, Shanghang Zhang, Guanhang Wu, Jos´e MF Moura, Joao P Costeira, and Geoffrey J Gordon. Adversarial multiple source domain adaptation. Advances in neural information processing
_systems, 31, 2018._

Sicheng Zhao, Bo Li, Colorado Reed, Pengfei Xu, and Kurt Keutzer. Multi-source domain adaptation in the deep learning era: A systematic survey, 2020.


-----

Figure 5: ResNet-18 model architecture for training auxiliary tasks.

### A APPENDIX

A.1 NETWORK ARCHITECTURE

Figure 5 displays the ResNet-18 model architecture (He et al., 2016) used in our experiment. When
training on the single classification task, the output layer of ResNet-18 only consists one branch with
the main task output predictions. When training with other auxiliary tasks, we add extra branches to
the model and output several classification logits, optimizing the losses of all tasks together.

A.2 IMPLEMENTATION DETAILS OF GP

In the previous Section 4, we compute the model updates from source and target clients as G[(]S[r]i[)]

_[≃]_

_h[(]S[r]i[)]_ _[−]_ _[h][(]global[r][−][1)]_ [and][ G]T[(][r][)] _≃_ _h[(]T[r][)]_ _−_ _h[(]T[r][−][1)], respectively. In our real training process, because we_
use different learning rates, training samples for source and target clients, we need to align the
**magnitude of model updates. Eq. 8 aligns the model updates from source clients to the target**
client and Eq. 9 combines the projection results with the target updates. We use lrT and lrS to
denote the target and source learning rates; batchsizeT and batchsizeS are the batch sizes for
target and source domains, respectively; nl is the labeled sample size on target client and ni is the
sample size for source client CSi ; rS is the rounds of local updates on source clients.




��
GP _h[(]Si[r][)]_ _[−]_ _[h]global[(][r][−][1)]_


_nl_

_batchsizeT_

_ni_

_batchsizeS_



�



(8)


�l[�] _ni_

_·_ �Ni _ni_ _·_


_lrT_ 1 �

_·_ _lrS_ _·_ _rS_ _·_ _h[(]Si[r][)]_ _[−]_ _[h]global[(][r][−][1)]_


_PGP = ++l∈L_


_N_
�

_i=0_


�l, �h[(]T[r][)] _−_ _h[(]T[r][−][1)]_


_h[(]T[r][)]_ = h[(]T[r][−][1)] + (1 − _β) · PGP + β · (h[(]T[r][)]_ _−_ _h[(]T[r][−][1)])_ (9)

A.3 IMPLEMENTATION AND DATA SPLITTING DETAILS

**CheXpert. We randomly sampled 4,000 source labeled samples (2,000 positive, 2,000 negative)**
from each domain for and 1,000 target labeled samples for fine-tuning. To create a larger distribution
shift, for negative samples, we randomly sample the ones with labels “0” for that condition instead
of using “No Finding” labels.

**MIMIC. We sample 5,00 (250 with findings and 250 without findings) for each source domain and**
adapt to the target domain with 100 labeled samples. This small sample size is intentionally chosen
to increase the difficulty of this federated domain adaptation task. For the auxiliary labels, we use
sex and age information from the database.


-----

Figure 6: Choice of α values on including auxiliary information.

**MIDRC. To set up a real-world case with multiple source domains, we split the CR dataset accord-**
ing to the zip code and select 5 states (IL, NC, CA, IN, TX) as our source and target domains. We
use all labeled samples in the source domains for local training, and 2,000 target labeled samples
for fine-tuning. As a real-world dataset, the number of samples are extremely imbalanced across
the clients, since the dataset collects data mostly from Chicago, which potentially would introduce
more distributional noise into clients.

**Setup. We used FedAvg as the aggregation rule for baselines, and set the hyper-parameters α = 1**
and β = 0.5, source learning rate as 10[−][3] and target learning rate as 2 10[−][4], communication
_·_
rounds r = 80 for MIMIC dataset and r = 50 for MIDRC and CheXpert datasets, as well as
the local training step size to be 1. We use cross-entropy losses for classification tasks and Adam
optimizer (Kingma & Ba, 2014). We select around 20% 25% of target domain labeled samples for
_∼_
fine-tuning under weak supervision. Further, we use pretrained ResNet-18 (He et al., 2016) model
with last layer hidden size of 128 for the training of three datasets. When training with auxiliary
tasks, we added branches to the output layer of ResNet-18 as shown in Figure 5.

A.4 DERIVATION OF GRADIENT PROJECTION METHOD’S TIME AND SPACE COMPLEXITY

**Time complexity: Assume the total parameter is m and we have l layers. To make it simpler,**
assume each layer has an average of _[m]l_ [parameters. Computing cosine similarity for all layers of]

one source client is O(( _[m]l_ [)][2][ ·][ l][) =][ O][(][m][2][/l][)][. We have][ N][ source clients so the total time cost for GP]

is O(N _m[2]/l)._
_·_

**Space complexity: The extra memory cost for GP (computing cosine similarity) is O(1) per client**
for storing the current cosine similarity value.

A.5 CHOICE OF THE LOSS WEIGHT HYPER-PARAMETER α FOR AUXILIARY TASKS

We observe that the convergence speed for AuxInfo is slow for small distribution shift case
(MIMIC dataset). Thus, we further conduct the ablation study on MIMIC dataset for hyperparameter α, which controls the loss weights between the main task and auxiliary tasks. We set
_α = 0.2, 0.4, 0.6, 0.8 and Figure 6 exhibits their target accuracies v.s. epochs. Setting α smaller_
may lead to a faster convergence while the final performances are almost the same for different α
values. Thus, we set α = 1 for all experiments for a fair comparison.

Books DVD Electronics Kitchen Average

FADA 78.10% 82.70% 77.40% 77.50% 78.90%
KD3A 79.00% 80.60% **85.60%** 86.90% 83.10%
FedAvg 77.93% 80.50% 82.19% 84.36% 81.24%
FedAvg Finetune 79.68% 81.31% 85.41% 86.76% 83.29%
FedAvg Finetune GP **79.78%** **82.35%** 84.93% **86.88%** **83.48%**

Table 5: Target domain accuracy (%) on AmazonReview dataset.


-----

Target Accuracy

FedAvg 60.11%
FedAvg Finetune 69.82%
FedAvg Finetune GP **79.74%**

Table 6: Target domain accuracy (%) on Non-IID MNIST dataset.

A.6 SUPPLEMENTARY EXPERIMENTS ON GRADIENT PROJECTION METHOD

Apart from experimenting on medical imaging datasets for GP, we also test GP on two
general-purposed datasets: AmazonReview (McAuley et al., 2015) and self-generated Non-IID
MNIST (Deng, 2012) datasets with imbalanced labels. We illustrate details of two datasets and
present the experiment results in this section.

A.6.1 DATASETS

**Amazon Review. This dataset is for a binary sentimental analysis task including four domains. By**
randomly choosing three of these domains as source domains and the rest one as the target domain,
we train a simple CNN model for the classification task. We use 2,000 training samples (the same
as KD3A (Feng et al., 2021) for a better comparison) for each source domain and 400 labeled target
samples.

**Non-IID MNIST. We adapt the Non-IID benchmark (Li et al., 2022) to construct the source and**
target domains for MNIST dataset in a Non-IID manner. To make the task harder, we choose the
data partition with quantity-based label imbalance with only 3 classes available for each source client
(though predicting for 10 classes) and we have 8 source clients in the system. For the target client,
we have 10 classes with all digits with a noise-based feature imbalance, creating a shift from the
source clients. We used a CNN architecture to do the experiment on classification between digits.
Also, we use 100 labeled target samples and test the accuracy on 10,000 unlabeled target samples.

A.6.2 RESULTS

**Amazon Review. From the result in Table 5, this dataset includes a comparatively simple task with**
small distribution shifts between clients, yet GP outperforms the state-of-art unsupervised KD3A by
0.38% in average target domain accuracy using 400 labeled target samples. Though our setting is
different, GP has a comparable performance against the unsupervised state-of-the-art method.

**Non-IID MNIST. The distribution shifts between clients are larger for this dataset and GP obtains**
a larger boost of performance compared with the previous dataset (20% for FedAvg and 10% for
FedAvg Finetune), as shown in Table 6. Hence, we can see that GP improves the target accuracy
more significantly when client shifts are bigger.


-----

